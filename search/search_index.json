{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#redes-neurais-e-deep-learning","title":"Redes Neurais e Deep Learning","text":"Informa\u00e7\u00f5es do Curso <p>Institui\u00e7\u00e3o: Insper Semestre: 2025.2 Professor: Humberto Sandmann Aluno: Lucas Fernando de Souza Lima  </p> <p>Este site re\u00fane os materiais, anota\u00e7\u00f5es e entregas da disciplina Redes Neurais e Deep Learning, abordando desde conceitos fundamentais (como perceptrons e MLPs) at\u00e9 modelos mais avan\u00e7ados, como autoencoders variacionais (VAE).</p>"},{"location":"#exercicios","title":"Exerc\u00edcios","text":"<ul> <li> Data</li> <li> Perceptron</li> <li> MLP</li> <li> VAE</li> </ul>"},{"location":"#projetos","title":"Projetos","text":"<ul> <li> Classifica\u00e7\u00e3o</li> <li> Regress\u00e3o</li> <li> Generative</li> </ul>"},{"location":"template/","title":"Template de Entrega","text":""},{"location":"template/#template-de-entrega","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"template/#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"template/#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"template/#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"template/#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"template/#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"template/#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"exercicios/data/main/","title":"Data","text":""},{"location":"exercicios/data/main/#exercicio-1-exploring-class-separability-in-2d","title":"Exerc\u00edcio 1 \u2013 Exploring Class Separability in 2D","text":""},{"location":"exercicios/data/main/#geracao-dos-dados","title":"Gera\u00e7\u00e3o dos Dados","text":"<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nparameters = [\n    ([2, 3], [0.8, 2.5]),\n    ([5, 6], [1.2, 1.9]),\n    ([8, 1], [0.9, 0.9]),\n    ([15, 4], [0.5, 2.0])\n]\n\ni = 0\nwhile i &lt; len(parameters):\n    p = parameters[i]\n    mean = p[0]\n    cov = np.diag(p[1])  \n    x, y = np.random.multivariate_normal(mean, cov, 100).T\n    plt.scatter(x, y, s=30, label=f'class {i}')\n    i += 1\n\nplt.title(\"Retas de decis\u00e3o\")\nplt.plot([0, 7], [10, -2], color='red')\nplt.plot([8, 13], [10, -2], color='red')\nplt.plot([2.7, 11.5], [-2, 10], color='red')\nplt.axis('equal')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"exercicios/data/main/#analise","title":"An\u00e1lise","text":"<p>As quatro classes s\u00e3o separadas, mas n\u00e3o por uma \u00fanica linha reta. \u00c9 necess\u00e1rio mais de uma reta de decis\u00e3o (tr\u00eas linhas no exemplo), correspondendo ao que uma rede com m\u00faltiplos neur\u00f4nios poderia aprender.</p> <p></p>"},{"location":"exercicios/data/main/#exercicio-2-non-linearity-in-higher-dimensions","title":"Exerc\u00edcio 2 \u2013 Non-Linearity in Higher Dimensions","text":""},{"location":"exercicios/data/main/#geracao-dos-dados_1","title":"Gera\u00e7\u00e3o dos Dados","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Classe A\nmu_A = [0, 0, 0, 0, 0]\nsigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\nclass_A = np.random.multivariate_normal(mu_A, sigma_A, 500)\n\n# Classe B\nmu_B = [1.5, 1.5, 1.5, 1.5, 1.5]\nsigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\nclass_B = np.random.multivariate_normal(mu_B, sigma_B, 500)\n\n# PCA para 2D\nX = np.vstack((class_A, class_B))\ny = np.array([0]*500 + [1]*500)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.scatter(X_pca[y==0, 0], X_pca[y==0, 1], label=\"Class A\", alpha=0.5)\nplt.scatter(X_pca[y==1, 0], X_pca[y==1, 1], label=\"Class B\", alpha=0.5)\nplt.legend()\nplt.title(\"Proje\u00e7\u00e3o PCA das Classes\")\nplt.show()\n</code></pre>"},{"location":"exercicios/data/main/#analise_1","title":"An\u00e1lise","text":"<p>As classes apresentam separa\u00e7\u00e3o parcial no primeiro componente principal, mas com sobreposi\u00e7\u00e3o significativa. Isso mostra que os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis. Modelos lineares simples teriam dificuldade, sendo necess\u00e1rio usar redes neurais com fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o-lineares.</p>"},{"location":"exercicios/data/main/#exercicio-3-preparing-real-world-data-for-a-neural-network","title":"Exerc\u00edcio 3 \u2013 Preparing Real-World Data for a Neural Network","text":""},{"location":"exercicios/data/main/#carregamento-e-descricao","title":"Carregamento e Descri\u00e7\u00e3o","text":"<p>O dataset Spaceship Titanic cont\u00e9m informa\u00e7\u00f5es de passageiros para prever se foram transportados ap\u00f3s a colis\u00e3o da nave.</p> <ul> <li>Target (<code>Transported</code>): indica se o passageiro foi transportado (<code>True/False</code>).</li> <li>Vari\u00e1veis Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>.</li> <li>Vari\u00e1veis Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>, <code>PassengerId</code>, <code>Name</code>.</li> </ul>"},{"location":"exercicios/data/main/#pre-processamento","title":"Pr\u00e9-Processamento","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"data/train.csv\")\n\ndf_clean = df.drop(columns=[\"PassengerId\", \"Name\", \"Cabin\"])\n\n# Tratar missing values\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\nfor col in num_cols:\n    df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n\ncat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\"]\nfor col in cat_cols:\n    df_clean[col] = df_clean[col].fillna(df_clean[col].mode()[0])\n\n# One-hot encoding\ndf_encoded = pd.get_dummies(df_clean, columns=cat_cols, drop_first=False)\n\n# Converter target para {-1, 1}\ndf_encoded[\"Transported\"] = df_encoded[\"Transported\"].map({True: 1, False: -1})\n\n# Padronizar num\u00e9ricos\nfor col in num_cols:\n    mean = df_encoded[col].mean()\n    std = df_encoded[col].std()\n    df_encoded[col] = (df_encoded[col] - mean) / std\n</code></pre>"},{"location":"exercicios/data/main/#visualizacao","title":"Visualiza\u00e7\u00e3o","text":"<pre><code>fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\naxes[0,0].hist(df[\"Age\"].dropna(), bins=30, color='skyblue', edgecolor='black')\naxes[0,0].set_title(\"Age antes do Scaling\")\naxes[0,1].hist(df[\"FoodCourt\"].dropna(), bins=30, color='salmon', edgecolor='black')\naxes[0,1].set_title(\"FoodCourt antes do Scaling\")\n\naxes[1,0].hist(df_encoded[\"Age\"], bins=30, color='skyblue', edgecolor='black')\naxes[1,0].set_title(\"Age depois da normaliza\u00e7\u00e3o\")\naxes[1,1].hist(df_encoded[\"FoodCourt\"], bins=30, color='salmon', edgecolor='black')\naxes[1,1].set_title(\"FoodCourt depois da normaliza\u00e7\u00e3o\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicios/data/main/#analise_2","title":"An\u00e1lise","text":"<ul> <li>Os valores num\u00e9ricos foram padronizados para m\u00e9dia 0 e desvio padr\u00e3o 1, ideal para fun\u00e7\u00f5es de ativa\u00e7\u00e3o <code>tanh</code>.</li> <li>Valores categ\u00f3ricos foram tratados com one-hot encoding.</li> <li>Valores ausentes foram preenchidos de forma consistente.</li> <li>As distribui\u00e7\u00f5es antes e depois do processamento mostram claramente a normaliza\u00e7\u00e3o aplicada.</li> </ul>"},{"location":"exercicios/mlp/","title":"MLP","text":"In\u00a0[6]: Copied! <pre>import numpy as np\n\n# Dados iniciais\nx = np.array([[0.5], [-0.2]])        \ny = 1.0\n\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]], dtype=float)   \nb1 = np.array([[0.1],\n               [-0.2]], dtype=float)        \n\nW2 = np.array([[0.5, -0.3]], dtype=float)   \nb2 = 0.2\n\n# taxa de aprendizado\neta = 0.1\n\n# fun\u00e7\u00f5es de ativa\u00e7\u00e3o\ndef tanh(z): \n    return np.tanh(z)\n\ndef tanh_prime(z):\n    return 1.0 - np.tanh(z)**2\n\n# Forward pass\nz1 = W1 @ x + b1                  \nh1 = tanh(z1)                     \nu2 = W2 @ h1 + b2                 \nyhat = np.tanh(u2).item()        \n\n# Loss (MSE, N=1)\nL = (y - yhat)**2\n\n# Backward pass\ndL_dyhat = 2 * (yhat - y)                     \ndyhat_du2 = 1 - np.tanh(u2)**2                \ndL_du2 = dL_dyhat * dyhat_du2                  \n\n# gradientes camada de sa\u00edda\ndL_dW2 = (dL_du2 * h1.T)                      \ndL_db2 = dL_du2                               \n\n# propaga para camada oculta\ndL_dh1 = (W2.T) * dL_du2                      \ndL_dz1 = dL_dh1 * tanh_prime(z1)              \n\n# gradientes camada oculta\ndL_dW1 = dL_dz1 @ x.T                         \ndL_db1 = dL_dz1                               \n\n# Atualiza\u00e7\u00e3o dos par\u00e2metros (eta = 0.1)\nW2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2.item()\n\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n\n# resultados\nnp.set_printoptions(precision=4, suppress=True)\n\nprint(\"=\"*50)\nprint(\"   FORWARD PASS\")\nprint(\"=\"*50)\nprint(f\"z1 =\\n{z1}\\n\")\nprint(f\"h1 =\\n{h1}\\n\")\nprint(f\"u2 = {u2.item():.4f}\")\nprint(f\"yhat = {yhat:.4f}\")\nprint(f\"\\nLoss (MSE): {L:.6f}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"  GRADIENTES\")\nprint(\"=\"*50)\nprint(f\"dL/dyhat   = {dL_dyhat:.6f}\")\nprint(f\"dyhat/du2  = {dyhat_du2.item():.6f}\")\nprint(f\"dL/du2     = {dL_du2.item():.6f}\\n\")\n\nprint(f\"dL/dW2 =\\n{dL_dW2}\\n\")\nprint(f\"dL/db2 = {dL_db2.item():.6f}\\n\")\n\nprint(f\"dL/dz1 =\\n{dL_dz1}\\n\")\nprint(f\"dL/dW1 =\\n{dL_dW1}\\n\")\nprint(f\"dL/db1 =\\n{dL_db1}\\n\")\n\nprint(\"=\"*50)\nprint(f\"   PAR\u00c2METROS ATUALIZADOS (\u03b7 = {eta:.3f})\")\nprint(\"=\"*50)\nprint(f\"W2_new =\\n{W2_new}\\n\")\nprint(f\"b2_new = {b2_new:.6f}\\n\")\nprint(f\"W1_new =\\n{W1_new}\\n\")\nprint(f\"b1_new =\\n{b1_new}\\n\")\nprint(\"=\"*50)\n</pre> import numpy as np  # Dados iniciais x = np.array([[0.5], [-0.2]])         y = 1.0  W1 = np.array([[0.3, -0.1],                [0.2,  0.4]], dtype=float)    b1 = np.array([[0.1],                [-0.2]], dtype=float)          W2 = np.array([[0.5, -0.3]], dtype=float)    b2 = 0.2  # taxa de aprendizado eta = 0.1  # fun\u00e7\u00f5es de ativa\u00e7\u00e3o def tanh(z):      return np.tanh(z)  def tanh_prime(z):     return 1.0 - np.tanh(z)**2  # Forward pass z1 = W1 @ x + b1                   h1 = tanh(z1)                      u2 = W2 @ h1 + b2                  yhat = np.tanh(u2).item()          # Loss (MSE, N=1) L = (y - yhat)**2  # Backward pass dL_dyhat = 2 * (yhat - y)                      dyhat_du2 = 1 - np.tanh(u2)**2                 dL_du2 = dL_dyhat * dyhat_du2                    # gradientes camada de sa\u00edda dL_dW2 = (dL_du2 * h1.T)                       dL_db2 = dL_du2                                 # propaga para camada oculta dL_dh1 = (W2.T) * dL_du2                       dL_dz1 = dL_dh1 * tanh_prime(z1)                # gradientes camada oculta dL_dW1 = dL_dz1 @ x.T                          dL_db1 = dL_dz1                                 # Atualiza\u00e7\u00e3o dos par\u00e2metros (eta = 0.1) W2_new = W2 - eta * dL_dW2 b2_new = b2 - eta * dL_db2.item()  W1_new = W1 - eta * dL_dW1 b1_new = b1 - eta * dL_db1  # resultados np.set_printoptions(precision=4, suppress=True)  print(\"=\"*50) print(\"   FORWARD PASS\") print(\"=\"*50) print(f\"z1 =\\n{z1}\\n\") print(f\"h1 =\\n{h1}\\n\") print(f\"u2 = {u2.item():.4f}\") print(f\"yhat = {yhat:.4f}\") print(f\"\\nLoss (MSE): {L:.6f}\")  print(\"\\n\" + \"=\"*50) print(\"  GRADIENTES\") print(\"=\"*50) print(f\"dL/dyhat   = {dL_dyhat:.6f}\") print(f\"dyhat/du2  = {dyhat_du2.item():.6f}\") print(f\"dL/du2     = {dL_du2.item():.6f}\\n\")  print(f\"dL/dW2 =\\n{dL_dW2}\\n\") print(f\"dL/db2 = {dL_db2.item():.6f}\\n\")  print(f\"dL/dz1 =\\n{dL_dz1}\\n\") print(f\"dL/dW1 =\\n{dL_dW1}\\n\") print(f\"dL/db1 =\\n{dL_db1}\\n\")  print(\"=\"*50) print(f\"   PAR\u00c2METROS ATUALIZADOS (\u03b7 = {eta:.3f})\") print(\"=\"*50) print(f\"W2_new =\\n{W2_new}\\n\") print(f\"b2_new = {b2_new:.6f}\\n\") print(f\"W1_new =\\n{W1_new}\\n\") print(f\"b1_new =\\n{b1_new}\\n\") print(\"=\"*50)   <pre>==================================================\n   FORWARD PASS\n==================================================\nz1 =\n[[ 0.27]\n [-0.18]]\n\nh1 =\n[[ 0.2636]\n [-0.1781]]\n\nu2 = 0.3852\nyhat = 0.3672\n\nLoss (MSE): 0.400377\n\n==================================================\n  GRADIENTES\n==================================================\ndL/dyhat   = -1.265507\ndyhat/du2  = 0.865130\ndL/du2     = -1.094828\n\ndL/dW2 =\n[[-0.2886  0.195 ]]\n\ndL/db2 = -1.094828\n\ndL/dz1 =\n[[-0.5094]\n [ 0.318 ]]\n\ndL/dW1 =\n[[-0.2547  0.1019]\n [ 0.159  -0.0636]]\n\ndL/db1 =\n[[-0.5094]\n [ 0.318 ]]\n\n==================================================\n   PAR\u00c2METROS ATUALIZADOS (\u03b7 = 0.100)\n==================================================\nW2_new =\n[[ 0.5289 -0.3195]]\n\nb2_new = 0.309483\n\nW1_new =\n[[ 0.3255 -0.1102]\n [ 0.1841  0.4064]]\n\nb1_new =\n[[ 0.1509]\n [-0.2318]]\n\n==================================================\n</pre> In\u00a0[11]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport itertools\n\nnp.random.seed(42)\n\n# 1 gerar dataset \ndef gen_class_subset(n_samples_large, clusters, keep_label, rs):\n    X, y = make_classification(\n        n_samples=n_samples_large,\n        n_features=2,\n        n_informative=2,\n        n_redundant=0,\n        n_clusters_per_class=clusters,\n        n_classes=2,\n        class_sep=1.2,\n        flip_y=0.02,\n        random_state=rs\n    )\n    mask = (y == keep_label)\n    return X[mask], y[mask]\n\n# gerar grandes conjuntos e filtrar\nX0_all, _ = gen_class_subset(2500, clusters=1, keep_label=0, rs=1)  # classe 0 (1 cluster)\nX1_all, _ = gen_class_subset(2500, clusters=2, keep_label=1, rs=2)  # classe 1 (2 clusters)\n\nn_per_class = 500\nidx0 = np.random.choice(len(X0_all), n_per_class, replace=False)\nidx1 = np.random.choice(len(X1_all), n_per_class, replace=False)\nX0 = X0_all[idx0]\nX1 = X1_all[idx1]\n\nX = np.vstack([X0, X1])\ny = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)])\nperm = np.random.permutation(len(X))\nX, y = X[perm], y[perm]\n\n# train/test split 80/20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(\"Train/test shapes:\", X_train.shape, X_test.shape)\n\n# plot dataset quick\nplt.figure(figsize=(5,4))\nplt.scatter(X[y==0,0], X[y==0,1], s=18, label='classe 0', alpha=0.7)\nplt.scatter(X[y==1,0], X[y==1,1], s=18, label='classe 1', alpha=0.7)\nplt.legend(); plt.title(\"Dataset (classe0:1 cluster, classe1:2 clusters)\"); plt.show()\n\n# 2 MLP do zero (2 hidden layers, tanh) -&gt; sa\u00edda sigmoid (bin\u00e1ria)\ndef sigmoid(z): return 1.0 / (1.0 + np.exp(-z))\ndef sigmoid_prime(a): return a * (1 - a)\ndef tanh(z): return np.tanh(z)\ndef tanh_prime(z): return 1.0 - np.tanh(z)**2\n\n# arquitetura\nD = X_train.shape[1]\nH1, H2 = 16, 8  # valores arbitrarios\nrng = np.random.default_rng(123)\n\nW1 = rng.normal(0, 1, (H1, D)) * np.sqrt(2.0/D)\nb1 = np.zeros((H1,))\nW2 = rng.normal(0, 1, (H2, H1)) * np.sqrt(2.0/H1)\nb2 = np.zeros((H2,))\nW3 = rng.normal(0, 1, (1, H2)) * np.sqrt(1.0/H2)\nb3 = 0.0\n\ndef forward_batch(Xb):\n    z1 = Xb @ W1.T + b1      # (B, H1)\n    h1 = tanh(z1)\n    z2 = h1 @ W2.T + b2      # (B, H2)\n    h2 = tanh(z2)\n    z3 = h2 @ W3.T + b3      # (B, 1)\n    yhat = sigmoid(z3)       # (B, 1)\n    cache = (z1, h1, z2, h2, z3, yhat)\n    return yhat, cache\n\ndef bce_loss(yhat, yb):\n    eps=1e-9\n    yb = yb.reshape(-1,1)\n    l = - (yb * np.log(yhat+eps) + (1-yb) * np.log(1-yhat+eps))\n    return np.mean(l)\n\n# 3 Treinamento\nlr = 0.05\nepochs = 300\nbatch = 32\nlambda_l2 = 1e-4\n\ntrain_losses = []\nfor ep in range(1, epochs+1):\n    perm = rng.permutation(X_train.shape[0])\n    Xs = X_train[perm]; ys = y_train[perm]\n    epoch_loss = 0.0\n    for i in range(0, Xs.shape[0], batch):\n        xb = Xs[i:i+batch]\n        yb = ys[i:i+batch].reshape(-1,1)\n        B = xb.shape[0]\n\n        # forward\n        yhat, cache = forward_batch(xb)\n        z1, h1, z2, h2, z3, _ = cache\n\n        # loss\n        loss = bce_loss(yhat, yb) + lambda_l2*(np.sum(W1*W1)+np.sum(W2*W2)+np.sum(W3*W3))\n        epoch_loss += loss * B\n\n        # backward (sigmoid + BCE -&gt; dZ3 = yhat - y)\n        d3 = (yhat - yb) / B  # (B,1)\n        gW3 = d3.T @ h2 + 2*lambda_l2*W3    # (1,H2)\n        gb3 = d3.sum(axis=0)                # (1,)\n\n        dh2 = d3 @ W3                       # (B,H2)\n        dz2 = dh2 * tanh_prime(z2)         # (B,H2)\n        gW2 = dz2.T @ h1 + 2*lambda_l2*W2   # (H2,H1)\n        gb2 = dz2.sum(axis=0)               # (H2,)\n\n        dh1 = dz2 @ W2                      # (B,H1)\n        dz1 = dh1 * tanh_prime(z1)         # (B,H1)\n        gW1 = dz1.T @ xb + 2*lambda_l2*W1   # (H1,D)\n        gb1 = dz1.sum(axis=0)               # (H1,)\n\n        # updates\n        W3 -= lr * gW3\n        b3 -= lr * gb3.item() if hasattr(gb3, 'item') else lr * float(gb3)\n        W2 -= lr * gW2\n        b2 -= lr * gb2\n        W1 -= lr * gW1\n        b1 -= lr * gb1\n\n    epoch_loss /= X_train.shape[0]\n    train_losses.append(epoch_loss)\n\n    if ep % 25 == 0 or ep == 1:\n        # avalia\u00e7\u00e3o r\u00e1pida no train\n        yhat_train, _ = forward_batch(X_train)\n        pred_train = (yhat_train.ravel() &gt;= 0.5).astype(int)\n        acc_train = accuracy_score(y_train, pred_train)\n        print(f\"\u00c9poca {ep:3d} | Loss {epoch_loss:.4f} | Train Acc {acc_train:.4f}\")\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 10))\n\n# Curva de Loss\naxs[0, 0].plot(train_losses, label=\"Train Loss (BCE)\", color=\"tab:blue\")\naxs[0, 0].set_xlabel(\"\u00c9poca\")\naxs[0, 0].set_ylabel(\"Loss\")\naxs[0, 0].set_title(\"Curva de Treinamento\")\naxs[0, 0].grid(True)\naxs[0, 0].legend()\n\n# Matriz de Confus\u00e3o\nim = axs[0, 1].imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\nfor (i, j), val in np.ndenumerate(cm):\n    axs[0, 1].text(j, i, val, ha=\"center\", va=\"center\",\n                   color=\"white\" if val &gt; cm.max()/2 else \"black\", fontsize=12)\naxs[0, 1].set_xticks([0, 1]); axs[0, 1].set_yticks([0, 1])\naxs[0, 1].set_xticklabels(['Pred 0', 'Pred 1'])\naxs[0, 1].set_yticklabels(['True 0', 'True 1'])\naxs[0, 1].set_title(\"Matriz de Confus\u00e3o (Teste)\")\nfig.colorbar(im, ax=axs[0, 1])\n\n# Pontos de teste\naxs[1, 0].scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=\"bwr\", edgecolor=\"k\", s=35)\naxs[1, 0].set_title(\"Pontos de Teste (y verdadeiro)\")\naxs[1, 0].set_xlabel(\"Feature 1\")\naxs[1, 0].set_ylabel(\"Feature 2\")\n\n# Fronteira de decis\u00e3o\ncontour = axs[1, 1].contourf(xx, yy, Z, levels=50, cmap=\"RdBu\", alpha=0.6)\naxs[1, 1].scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=\"bwr\", edgecolor=\"k\", s=30)\naxs[1, 1].set_title(f\"Fronteira de decis\u00e3o \u2014 acc {acc:.4f}\")\naxs[1, 1].set_xlabel(\"Feature 1\")\naxs[1, 1].set_ylabel(\"Feature 2\")\nfig.colorbar(contour, ax=axs[1, 1])\n\nplt.tight_layout()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, confusion_matrix import itertools  np.random.seed(42)  # 1 gerar dataset  def gen_class_subset(n_samples_large, clusters, keep_label, rs):     X, y = make_classification(         n_samples=n_samples_large,         n_features=2,         n_informative=2,         n_redundant=0,         n_clusters_per_class=clusters,         n_classes=2,         class_sep=1.2,         flip_y=0.02,         random_state=rs     )     mask = (y == keep_label)     return X[mask], y[mask]  # gerar grandes conjuntos e filtrar X0_all, _ = gen_class_subset(2500, clusters=1, keep_label=0, rs=1)  # classe 0 (1 cluster) X1_all, _ = gen_class_subset(2500, clusters=2, keep_label=1, rs=2)  # classe 1 (2 clusters)  n_per_class = 500 idx0 = np.random.choice(len(X0_all), n_per_class, replace=False) idx1 = np.random.choice(len(X1_all), n_per_class, replace=False) X0 = X0_all[idx0] X1 = X1_all[idx1]  X = np.vstack([X0, X1]) y = np.hstack([np.zeros(n_per_class), np.ones(n_per_class)]) perm = np.random.permutation(len(X)) X, y = X[perm], y[perm]  # train/test split 80/20 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) print(\"Train/test shapes:\", X_train.shape, X_test.shape)  # plot dataset quick plt.figure(figsize=(5,4)) plt.scatter(X[y==0,0], X[y==0,1], s=18, label='classe 0', alpha=0.7) plt.scatter(X[y==1,0], X[y==1,1], s=18, label='classe 1', alpha=0.7) plt.legend(); plt.title(\"Dataset (classe0:1 cluster, classe1:2 clusters)\"); plt.show()  # 2 MLP do zero (2 hidden layers, tanh) -&gt; sa\u00edda sigmoid (bin\u00e1ria) def sigmoid(z): return 1.0 / (1.0 + np.exp(-z)) def sigmoid_prime(a): return a * (1 - a) def tanh(z): return np.tanh(z) def tanh_prime(z): return 1.0 - np.tanh(z)**2  # arquitetura D = X_train.shape[1] H1, H2 = 16, 8  # valores arbitrarios rng = np.random.default_rng(123)  W1 = rng.normal(0, 1, (H1, D)) * np.sqrt(2.0/D) b1 = np.zeros((H1,)) W2 = rng.normal(0, 1, (H2, H1)) * np.sqrt(2.0/H1) b2 = np.zeros((H2,)) W3 = rng.normal(0, 1, (1, H2)) * np.sqrt(1.0/H2) b3 = 0.0  def forward_batch(Xb):     z1 = Xb @ W1.T + b1      # (B, H1)     h1 = tanh(z1)     z2 = h1 @ W2.T + b2      # (B, H2)     h2 = tanh(z2)     z3 = h2 @ W3.T + b3      # (B, 1)     yhat = sigmoid(z3)       # (B, 1)     cache = (z1, h1, z2, h2, z3, yhat)     return yhat, cache  def bce_loss(yhat, yb):     eps=1e-9     yb = yb.reshape(-1,1)     l = - (yb * np.log(yhat+eps) + (1-yb) * np.log(1-yhat+eps))     return np.mean(l)  # 3 Treinamento lr = 0.05 epochs = 300 batch = 32 lambda_l2 = 1e-4  train_losses = [] for ep in range(1, epochs+1):     perm = rng.permutation(X_train.shape[0])     Xs = X_train[perm]; ys = y_train[perm]     epoch_loss = 0.0     for i in range(0, Xs.shape[0], batch):         xb = Xs[i:i+batch]         yb = ys[i:i+batch].reshape(-1,1)         B = xb.shape[0]          # forward         yhat, cache = forward_batch(xb)         z1, h1, z2, h2, z3, _ = cache          # loss         loss = bce_loss(yhat, yb) + lambda_l2*(np.sum(W1*W1)+np.sum(W2*W2)+np.sum(W3*W3))         epoch_loss += loss * B          # backward (sigmoid + BCE -&gt; dZ3 = yhat - y)         d3 = (yhat - yb) / B  # (B,1)         gW3 = d3.T @ h2 + 2*lambda_l2*W3    # (1,H2)         gb3 = d3.sum(axis=0)                # (1,)          dh2 = d3 @ W3                       # (B,H2)         dz2 = dh2 * tanh_prime(z2)         # (B,H2)         gW2 = dz2.T @ h1 + 2*lambda_l2*W2   # (H2,H1)         gb2 = dz2.sum(axis=0)               # (H2,)          dh1 = dz2 @ W2                      # (B,H1)         dz1 = dh1 * tanh_prime(z1)         # (B,H1)         gW1 = dz1.T @ xb + 2*lambda_l2*W1   # (H1,D)         gb1 = dz1.sum(axis=0)               # (H1,)          # updates         W3 -= lr * gW3         b3 -= lr * gb3.item() if hasattr(gb3, 'item') else lr * float(gb3)         W2 -= lr * gW2         b2 -= lr * gb2         W1 -= lr * gW1         b1 -= lr * gb1      epoch_loss /= X_train.shape[0]     train_losses.append(epoch_loss)      if ep % 25 == 0 or ep == 1:         # avalia\u00e7\u00e3o r\u00e1pida no train         yhat_train, _ = forward_batch(X_train)         pred_train = (yhat_train.ravel() &gt;= 0.5).astype(int)         acc_train = accuracy_score(y_train, pred_train)         print(f\"\u00c9poca {ep:3d} | Loss {epoch_loss:.4f} | Train Acc {acc_train:.4f}\")  fig, axs = plt.subplots(2, 2, figsize=(12, 10))  # Curva de Loss axs[0, 0].plot(train_losses, label=\"Train Loss (BCE)\", color=\"tab:blue\") axs[0, 0].set_xlabel(\"\u00c9poca\") axs[0, 0].set_ylabel(\"Loss\") axs[0, 0].set_title(\"Curva de Treinamento\") axs[0, 0].grid(True) axs[0, 0].legend()  # Matriz de Confus\u00e3o im = axs[0, 1].imshow(cm, cmap=\"Blues\", interpolation=\"nearest\") for (i, j), val in np.ndenumerate(cm):     axs[0, 1].text(j, i, val, ha=\"center\", va=\"center\",                    color=\"white\" if val &gt; cm.max()/2 else \"black\", fontsize=12) axs[0, 1].set_xticks([0, 1]); axs[0, 1].set_yticks([0, 1]) axs[0, 1].set_xticklabels(['Pred 0', 'Pred 1']) axs[0, 1].set_yticklabels(['True 0', 'True 1']) axs[0, 1].set_title(\"Matriz de Confus\u00e3o (Teste)\") fig.colorbar(im, ax=axs[0, 1])  # Pontos de teste axs[1, 0].scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=\"bwr\", edgecolor=\"k\", s=35) axs[1, 0].set_title(\"Pontos de Teste (y verdadeiro)\") axs[1, 0].set_xlabel(\"Feature 1\") axs[1, 0].set_ylabel(\"Feature 2\")  # Fronteira de decis\u00e3o contour = axs[1, 1].contourf(xx, yy, Z, levels=50, cmap=\"RdBu\", alpha=0.6) axs[1, 1].scatter(X_test[:,0], X_test[:,1], c=y_test, cmap=\"bwr\", edgecolor=\"k\", s=30) axs[1, 1].set_title(f\"Fronteira de decis\u00e3o \u2014 acc {acc:.4f}\") axs[1, 1].set_xlabel(\"Feature 1\") axs[1, 1].set_ylabel(\"Feature 2\") fig.colorbar(contour, ax=axs[1, 1])  plt.tight_layout() plt.show()  <pre>Train/test shapes: (800, 2) (200, 2)\n</pre> <pre>\u00c9poca   1 | Loss 0.7184 | Train Acc 0.6825\n\u00c9poca  25 | Loss 0.5267 | Train Acc 0.7412\n\u00c9poca  50 | Loss 0.5166 | Train Acc 0.7475\n\u00c9poca  75 | Loss 0.5035 | Train Acc 0.7512\n\u00c9poca 100 | Loss 0.4619 | Train Acc 0.7963\n\u00c9poca 125 | Loss 0.4278 | Train Acc 0.8200\n\u00c9poca 150 | Loss 0.4232 | Train Acc 0.8213\n\u00c9poca 175 | Loss 0.4196 | Train Acc 0.8225\n\u00c9poca 200 | Loss 0.4183 | Train Acc 0.8237\n\u00c9poca 225 | Loss 0.4168 | Train Acc 0.8187\n\u00c9poca 250 | Loss 0.4168 | Train Acc 0.8200\n\u00c9poca 275 | Loss 0.4160 | Train Acc 0.8187\n\u00c9poca 300 | Loss 0.4151 | Train Acc 0.8225\n</pre> In\u00a0[13]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom matplotlib.patches import Patch\n\nnp.random.seed(42)\n\n# 1 Gerar dados: 3 classes com (2,3,4) clusters\ndef gen_keep_label(n_samples_large, n_features, clusters, keep_label=0, rs=0):\n    X, y = make_classification(\n        n_samples=n_samples_large,\n        n_features=n_features,\n        n_informative=n_features,\n        n_redundant=0,\n        n_classes=3,\n        n_clusters_per_class=clusters,\n        class_sep=1.2,\n        flip_y=0.02,\n        random_state=rs\n    )\n    mask = (y == keep_label)\n    return X[mask], y[mask] \n\nn_each = 500\nn_features = 4\n\n# gerar e filtrar\nX2_all, _ = gen_keep_label(8000, n_features=n_features, clusters=2, keep_label=0, rs=1)  # classe A (2 clusters)\nX3_all, _ = gen_keep_label(9000, n_features=n_features, clusters=3, keep_label=0, rs=2)  # classe B (3 clusters)\nX4_all, _ = gen_keep_label(10000, n_features=n_features, clusters=4, keep_label=0, rs=3) # classe C (4 clusters)\n\n# amostramos n_each de cada\nidx2 = np.random.choice(len(X2_all), n_each, replace=False)\nidx3 = np.random.choice(len(X3_all), n_each, replace=False)\nidx4 = np.random.choice(len(X4_all), n_each, replace=False)\n\nX2 = X2_all[idx2]\nX3 = X3_all[idx3]\nX4 = X4_all[idx4]\n\nX = np.vstack([X2, X3, X4])\ny = np.hstack([np.zeros(n_each), np.ones(n_each), np.full(n_each, 2)])\n\n# embaralhar\nperm = np.random.permutation(len(X))\nX, y = X[perm], y[perm]\n\n# split 80/20\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.111111, random_state=42, stratify=y_train)\n\nnum_classes = 3\nN, D = X_train.shape\n\ndef one_hot(y, K):\n    eye = np.eye(K, dtype=float)\n    return eye[y.astype(int)]\n\n# 2 MLP init\nrng = np.random.default_rng(123)\nH1, H2 = 64, 32\n\nW1 = rng.normal(0, 1, (H1, D)) / np.sqrt(D)\nb1 = np.zeros(H1)\n\nW2 = rng.normal(0, 1, (H2, H1)) / np.sqrt(H1)\nb2 = np.zeros(H2)\n\nW3 = rng.normal(0, 1, (num_classes, H2)) / np.sqrt(H2)\nb3 = np.zeros(num_classes)\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef tanh_prime(x):\n    return 1.0 - np.tanh(x)**2\n\ndef softmax(x):\n    x = x - x.max(axis=1, keepdims=True)\n    e = np.exp(x)\n    return e / e.sum(axis=1, keepdims=True)\n\ndef forward_batch(Xb):\n    z1 = Xb @ W1.T + b1      # (B, H1)\n    h1 = tanh(z1)\n    z2 = h1 @ W2.T + b2      # (B, H2)\n    h2 = tanh(z2)\n    z3 = h2 @ W3.T + b3      # (B, C)\n    yhat = softmax(z3)\n    cache = (z1, h1, z2, h2, z3, yhat)\n    return yhat, cache\n\ndef cross_entropy_loss(yhat, yb_onehot):\n    eps = 1e-9\n    return -np.mean(np.sum(yb_onehot * np.log(yhat + eps), axis=1))\n\n# 3 Treino mini-batch, L2, early stopping\nlr = 0.005\nepochs = 500\nbatch = 128\nlambda_l2 = 1e-4\npatience = 20\n\ntrain_losses = []\nval_losses = []\ntrain_accs = []\nval_accs = []\n\nbest_val = np.inf\nbad_epochs = 0\nbest_params = None\n\nfor ep in range(1, epochs+1):\n    perm = rng.permutation(X_train.shape[0])\n    Xs = X_train[perm]; ys = y_train[perm]\n    epoch_loss = 0.0\n    # mini-batches\n    for i in range(0, Xs.shape[0], batch):\n        xb = Xs[i:i+batch]\n        yb_idx = ys[i:i+batch].astype(int)\n        yb = one_hot(yb_idx, num_classes)\n        B = xb.shape[0]\n\n        # forward\n        z1 = xb @ W1.T + b1; h1 = tanh(z1)\n        z2 = h1 @ W2.T + b2; h2 = tanh(z2)\n        z3 = h2 @ W3.T + b3; yhat = softmax(z3)\n\n        # loss + L2\n        loss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))\n        l2_term = lambda_l2 * (np.sum(W1*W1) + np.sum(W2*W2) + np.sum(W3*W3))\n        loss = loss_ce + l2_term\n        epoch_loss += loss * B\n\n        # backward (softmax + CE)\n        d3 = (yhat - yb) / B                      # (B, C)\n        gW3 = d3.T @ h2 + 2 * lambda_l2 * W3      # (C, H2)\n        gb3 = d3.sum(axis=0)                      # (C,)\n\n        dh2 = d3 @ W3                             # (B, H2)\n        dz2 = dh2 * tanh_prime(z2)                # (B, H2)\n        gW2 = dz2.T @ h1 + 2 * lambda_l2 * W2     # (H2, H1)\n        gb2 = dz2.sum(axis=0)                     # (H2,)\n\n        dh1 = dz2 @ W2                            # (B, H1)\n        dz1 = dh1 * tanh_prime(z1)                # (B, H1)\n        gW1 = dz1.T @ xb + 2 * lambda_l2 * W1     # (H1, D)\n        gb1 = dz1.sum(axis=0)                     # (H1,)\n\n        # updates\n        W3 -= lr * gW3\n        b3 -= lr * gb3\n        W2 -= lr * gW2\n        b2 -= lr * gb2\n        W1 -= lr * gW1\n        b1 -= lr * gb1\n\n    epoch_loss /= X_train.shape[0]\n\n    # m\u00e9tricas\n    yhat_tr, _ = forward_batch(X_train)\n    train_acc = (np.argmax(yhat_tr, axis=1) == y_train).mean()\n    yhat_val, _ = forward_batch(X_val)\n    Y_val_oh = one_hot(y_val.astype(int), num_classes)\n    val_loss = cross_entropy_loss(yhat_val, Y_val_oh)\n    val_acc = (np.argmax(yhat_val, axis=1) == y_val).mean()\n\n    train_losses.append(epoch_loss)\n    val_losses.append(val_loss)\n    train_accs.append(train_acc)\n    val_accs.append(val_acc)\n\n    # early stopping\n    if val_loss &lt; best_val - 1e-6:\n        best_val = val_loss\n        bad_epochs = 0\n        best_params = (W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy())\n    else:\n        bad_epochs += 1\n        if bad_epochs &gt;= patience:\n            print(f\"Early stopping (\u00e9poca {ep}) melhor val_loss={best_val:.4f}\")\n            break\n\n    if ep % 10 == 0 or ep == 1:\n        print(f\"\u00c9poca {ep:3d} | TrainLoss {epoch_loss:.4f} | ValLoss {val_loss:.4f} | TrainAcc {train_acc:.3f} | ValAcc {val_acc:.3f}\")\n\n# restaura melhor\nif best_params is not None:\n    W1, b1, W2, b2, W3, b3 = best_params\n\n# 4 Avalia\u00e7\u00e3o final e plots em 1 figura 2x2\nyhat_test, _ = forward_batch(X_test)\ny_pred_test = np.argmax(yhat_test, axis=1)\nacc_test = accuracy_score(y_test, y_pred_test)\ncm = confusion_matrix(y_test, y_pred_test)\n\n# fronteira de decisao em 2D via PCA\npca = PCA(n_components=2, random_state=42)\nX_train_p = pca.fit_transform(X_train)\nX_test_p = pca.transform(X_test)\n# grade\nx_min, x_max = X_train_p[:,0].min()-0.3, X_train_p[:,0].max()+0.3\ny_min, y_max = X_train_p[:,1].min()-0.3, X_train_p[:,1].max()+0.3\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\ngrid_pca = np.c_[xx.ravel(), yy.ravel()]\ngrid_full = pca.inverse_transform(grid_pca)\ny_grid, _ = forward_batch(grid_full)\ny_grid_cls = np.argmax(y_grid, axis=1).reshape(xx.shape)\n\n# figura 2x2\nfig, axs = plt.subplots(2,2, figsize=(14,12))\n\n# 1 Loss (train &amp; val)\naxs[0,0].plot(train_losses, label=\"Train Loss\")\naxs[0,0].plot(val_losses, label=\"Val Loss\")\naxs[0,0].set_title(\"Curva de Loss\")\naxs[0,0].set_xlabel(\"\u00c9poca\"); axs[0,0].set_ylabel(\"Loss\")\naxs[0,0].legend(); axs[0,0].grid(True)\n\n# 2 Acur\u00e1cia (train &amp; val)\naxs[0,1].plot(train_accs, label=\"Train Acc\")\naxs[0,1].plot(val_accs, label=\"Val Acc\")\naxs[0,1].set_title(\"Curva de Acur\u00e1cia\")\naxs[0,1].set_xlabel(\"\u00c9poca\"); axs[0,1].set_ylabel(\"Acur\u00e1cia\")\naxs[0,1].legend(); axs[0,1].grid(True)\n\n# 3 Matriz de confus\u00e3o\nim = axs[1,0].imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\nfor (i,j), val in np.ndenumerate(cm):\n    axs[1,0].text(j, i, val, ha='center', va='center',\n                  color='white' if val&gt;cm.max()/2 else 'black', fontsize=12)\naxs[1,0].set_xticks(np.arange(num_classes)); axs[1,0].set_yticks(np.arange(num_classes))\naxs[1,0].set_xticklabels([f\"Pred {i}\" for i in range(num_classes)])\naxs[1,0].set_yticklabels([f\"True {i}\" for i in range(num_classes)])\naxs[1,0].set_title(f\"Matriz de Confus\u00e3o (Teste) \u2014 acc {acc_test:.4f}\")\nfig.colorbar(im, ax=axs[1,0])\n\n# 4 Decision boundary (PCA 2D) + pontos de teste (true vs pred)\ncmap = plt.cm.Set1 if num_classes &lt;= 9 else plt.cm.tab20\ncs = axs[1,1].contourf(xx, yy, y_grid_cls, levels=np.arange(num_classes+1)-0.5, alpha=0.35, cmap=cmap)\n# plot test points: corretos com marcador o, errados com x\ncorrect = (y_pred_test == y_test)\naxs[1,1].scatter(X_test_p[correct,0], X_test_p[correct,1], c=y_test[correct], cmap=cmap, edgecolors='k', s=40, marker='o', label=\"Corretos\")\naxs[1,1].scatter(X_test_p[~correct,0], X_test_p[~correct,1], c=y_test[~correct], cmap=cmap, edgecolors='k', s=60, marker='x', label=\"Errados\")\naxs[1,1].set_title(\"Decision Boundary (PCA 2D)\")\naxs[1,1].set_xlabel(\"PC1\"); axs[1,1].set_ylabel(\"PC2\")\nlegend_handles = [Patch(color=cmap(i / max(3,num_classes)), label=f\"Classe {i}\") for i in range(num_classes)]\naxs[1,1].legend(handles=legend_handles + [\n    plt.Line2D([0],[0], marker='o', color='w', markerfacecolor='gray', markeredgecolor='k', label='Corretos', markersize=8),\n    plt.Line2D([0],[0], marker='x', color='k', label='Errados', markersize=8)\n], loc='upper right')\nfig.colorbar(cs, ax=axs[1,1])\n\nplt.tight_layout()\nplt.show()\n\n# imprimir m\u00e9tricas\nprint(\"\\nM\u00e9tricas (teste)\")\nprint(f\"Acur\u00e1cia: {acc_test:.4f}\")\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(num_classes))\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay from matplotlib.patches import Patch  np.random.seed(42)  # 1 Gerar dados: 3 classes com (2,3,4) clusters def gen_keep_label(n_samples_large, n_features, clusters, keep_label=0, rs=0):     X, y = make_classification(         n_samples=n_samples_large,         n_features=n_features,         n_informative=n_features,         n_redundant=0,         n_classes=3,         n_clusters_per_class=clusters,         class_sep=1.2,         flip_y=0.02,         random_state=rs     )     mask = (y == keep_label)     return X[mask], y[mask]   n_each = 500 n_features = 4  # gerar e filtrar X2_all, _ = gen_keep_label(8000, n_features=n_features, clusters=2, keep_label=0, rs=1)  # classe A (2 clusters) X3_all, _ = gen_keep_label(9000, n_features=n_features, clusters=3, keep_label=0, rs=2)  # classe B (3 clusters) X4_all, _ = gen_keep_label(10000, n_features=n_features, clusters=4, keep_label=0, rs=3) # classe C (4 clusters)  # amostramos n_each de cada idx2 = np.random.choice(len(X2_all), n_each, replace=False) idx3 = np.random.choice(len(X3_all), n_each, replace=False) idx4 = np.random.choice(len(X4_all), n_each, replace=False)  X2 = X2_all[idx2] X3 = X3_all[idx3] X4 = X4_all[idx4]  X = np.vstack([X2, X3, X4]) y = np.hstack([np.zeros(n_each), np.ones(n_each), np.full(n_each, 2)])  # embaralhar perm = np.random.permutation(len(X)) X, y = X[perm], y[perm]  # split 80/20 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.111111, random_state=42, stratify=y_train)  num_classes = 3 N, D = X_train.shape  def one_hot(y, K):     eye = np.eye(K, dtype=float)     return eye[y.astype(int)]  # 2 MLP init rng = np.random.default_rng(123) H1, H2 = 64, 32  W1 = rng.normal(0, 1, (H1, D)) / np.sqrt(D) b1 = np.zeros(H1)  W2 = rng.normal(0, 1, (H2, H1)) / np.sqrt(H1) b2 = np.zeros(H2)  W3 = rng.normal(0, 1, (num_classes, H2)) / np.sqrt(H2) b3 = np.zeros(num_classes)  def tanh(x):     return np.tanh(x)  def tanh_prime(x):     return 1.0 - np.tanh(x)**2  def softmax(x):     x = x - x.max(axis=1, keepdims=True)     e = np.exp(x)     return e / e.sum(axis=1, keepdims=True)  def forward_batch(Xb):     z1 = Xb @ W1.T + b1      # (B, H1)     h1 = tanh(z1)     z2 = h1 @ W2.T + b2      # (B, H2)     h2 = tanh(z2)     z3 = h2 @ W3.T + b3      # (B, C)     yhat = softmax(z3)     cache = (z1, h1, z2, h2, z3, yhat)     return yhat, cache  def cross_entropy_loss(yhat, yb_onehot):     eps = 1e-9     return -np.mean(np.sum(yb_onehot * np.log(yhat + eps), axis=1))  # 3 Treino mini-batch, L2, early stopping lr = 0.005 epochs = 500 batch = 128 lambda_l2 = 1e-4 patience = 20  train_losses = [] val_losses = [] train_accs = [] val_accs = []  best_val = np.inf bad_epochs = 0 best_params = None  for ep in range(1, epochs+1):     perm = rng.permutation(X_train.shape[0])     Xs = X_train[perm]; ys = y_train[perm]     epoch_loss = 0.0     # mini-batches     for i in range(0, Xs.shape[0], batch):         xb = Xs[i:i+batch]         yb_idx = ys[i:i+batch].astype(int)         yb = one_hot(yb_idx, num_classes)         B = xb.shape[0]          # forward         z1 = xb @ W1.T + b1; h1 = tanh(z1)         z2 = h1 @ W2.T + b2; h2 = tanh(z2)         z3 = h2 @ W3.T + b3; yhat = softmax(z3)          # loss + L2         loss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))         l2_term = lambda_l2 * (np.sum(W1*W1) + np.sum(W2*W2) + np.sum(W3*W3))         loss = loss_ce + l2_term         epoch_loss += loss * B          # backward (softmax + CE)         d3 = (yhat - yb) / B                      # (B, C)         gW3 = d3.T @ h2 + 2 * lambda_l2 * W3      # (C, H2)         gb3 = d3.sum(axis=0)                      # (C,)          dh2 = d3 @ W3                             # (B, H2)         dz2 = dh2 * tanh_prime(z2)                # (B, H2)         gW2 = dz2.T @ h1 + 2 * lambda_l2 * W2     # (H2, H1)         gb2 = dz2.sum(axis=0)                     # (H2,)          dh1 = dz2 @ W2                            # (B, H1)         dz1 = dh1 * tanh_prime(z1)                # (B, H1)         gW1 = dz1.T @ xb + 2 * lambda_l2 * W1     # (H1, D)         gb1 = dz1.sum(axis=0)                     # (H1,)          # updates         W3 -= lr * gW3         b3 -= lr * gb3         W2 -= lr * gW2         b2 -= lr * gb2         W1 -= lr * gW1         b1 -= lr * gb1      epoch_loss /= X_train.shape[0]      # m\u00e9tricas     yhat_tr, _ = forward_batch(X_train)     train_acc = (np.argmax(yhat_tr, axis=1) == y_train).mean()     yhat_val, _ = forward_batch(X_val)     Y_val_oh = one_hot(y_val.astype(int), num_classes)     val_loss = cross_entropy_loss(yhat_val, Y_val_oh)     val_acc = (np.argmax(yhat_val, axis=1) == y_val).mean()      train_losses.append(epoch_loss)     val_losses.append(val_loss)     train_accs.append(train_acc)     val_accs.append(val_acc)      # early stopping     if val_loss &lt; best_val - 1e-6:         best_val = val_loss         bad_epochs = 0         best_params = (W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy())     else:         bad_epochs += 1         if bad_epochs &gt;= patience:             print(f\"Early stopping (\u00e9poca {ep}) melhor val_loss={best_val:.4f}\")             break      if ep % 10 == 0 or ep == 1:         print(f\"\u00c9poca {ep:3d} | TrainLoss {epoch_loss:.4f} | ValLoss {val_loss:.4f} | TrainAcc {train_acc:.3f} | ValAcc {val_acc:.3f}\")  # restaura melhor if best_params is not None:     W1, b1, W2, b2, W3, b3 = best_params  # 4 Avalia\u00e7\u00e3o final e plots em 1 figura 2x2 yhat_test, _ = forward_batch(X_test) y_pred_test = np.argmax(yhat_test, axis=1) acc_test = accuracy_score(y_test, y_pred_test) cm = confusion_matrix(y_test, y_pred_test)  # fronteira de decisao em 2D via PCA pca = PCA(n_components=2, random_state=42) X_train_p = pca.fit_transform(X_train) X_test_p = pca.transform(X_test) # grade x_min, x_max = X_train_p[:,0].min()-0.3, X_train_p[:,0].max()+0.3 y_min, y_max = X_train_p[:,1].min()-0.3, X_train_p[:,1].max()+0.3 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300)) grid_pca = np.c_[xx.ravel(), yy.ravel()] grid_full = pca.inverse_transform(grid_pca) y_grid, _ = forward_batch(grid_full) y_grid_cls = np.argmax(y_grid, axis=1).reshape(xx.shape)  # figura 2x2 fig, axs = plt.subplots(2,2, figsize=(14,12))  # 1 Loss (train &amp; val) axs[0,0].plot(train_losses, label=\"Train Loss\") axs[0,0].plot(val_losses, label=\"Val Loss\") axs[0,0].set_title(\"Curva de Loss\") axs[0,0].set_xlabel(\"\u00c9poca\"); axs[0,0].set_ylabel(\"Loss\") axs[0,0].legend(); axs[0,0].grid(True)  # 2 Acur\u00e1cia (train &amp; val) axs[0,1].plot(train_accs, label=\"Train Acc\") axs[0,1].plot(val_accs, label=\"Val Acc\") axs[0,1].set_title(\"Curva de Acur\u00e1cia\") axs[0,1].set_xlabel(\"\u00c9poca\"); axs[0,1].set_ylabel(\"Acur\u00e1cia\") axs[0,1].legend(); axs[0,1].grid(True)  # 3 Matriz de confus\u00e3o im = axs[1,0].imshow(cm, cmap=\"Blues\", interpolation=\"nearest\") for (i,j), val in np.ndenumerate(cm):     axs[1,0].text(j, i, val, ha='center', va='center',                   color='white' if val&gt;cm.max()/2 else 'black', fontsize=12) axs[1,0].set_xticks(np.arange(num_classes)); axs[1,0].set_yticks(np.arange(num_classes)) axs[1,0].set_xticklabels([f\"Pred {i}\" for i in range(num_classes)]) axs[1,0].set_yticklabels([f\"True {i}\" for i in range(num_classes)]) axs[1,0].set_title(f\"Matriz de Confus\u00e3o (Teste) \u2014 acc {acc_test:.4f}\") fig.colorbar(im, ax=axs[1,0])  # 4 Decision boundary (PCA 2D) + pontos de teste (true vs pred) cmap = plt.cm.Set1 if num_classes &lt;= 9 else plt.cm.tab20 cs = axs[1,1].contourf(xx, yy, y_grid_cls, levels=np.arange(num_classes+1)-0.5, alpha=0.35, cmap=cmap) # plot test points: corretos com marcador o, errados com x correct = (y_pred_test == y_test) axs[1,1].scatter(X_test_p[correct,0], X_test_p[correct,1], c=y_test[correct], cmap=cmap, edgecolors='k', s=40, marker='o', label=\"Corretos\") axs[1,1].scatter(X_test_p[~correct,0], X_test_p[~correct,1], c=y_test[~correct], cmap=cmap, edgecolors='k', s=60, marker='x', label=\"Errados\") axs[1,1].set_title(\"Decision Boundary (PCA 2D)\") axs[1,1].set_xlabel(\"PC1\"); axs[1,1].set_ylabel(\"PC2\") legend_handles = [Patch(color=cmap(i / max(3,num_classes)), label=f\"Classe {i}\") for i in range(num_classes)] axs[1,1].legend(handles=legend_handles + [     plt.Line2D([0],[0], marker='o', color='w', markerfacecolor='gray', markeredgecolor='k', label='Corretos', markersize=8),     plt.Line2D([0],[0], marker='x', color='k', label='Errados', markersize=8) ], loc='upper right') fig.colorbar(cs, ax=axs[1,1])  plt.tight_layout() plt.show()  # imprimir m\u00e9tricas print(\"\\nM\u00e9tricas (teste)\") print(f\"Acur\u00e1cia: {acc_test:.4f}\") disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(num_classes))  <pre>\u00c9poca   1 | TrainLoss 1.3030 | ValLoss 1.2682 | TrainAcc 0.224 | ValAcc 0.201\n\u00c9poca  10 | TrainLoss 0.9485 | ValLoss 0.9110 | TrainAcc 0.562 | ValAcc 0.627\n\u00c9poca  20 | TrainLoss 0.8393 | ValLoss 0.7917 | TrainAcc 0.636 | ValAcc 0.687\n\u00c9poca  30 | TrainLoss 0.7914 | ValLoss 0.7371 | TrainAcc 0.662 | ValAcc 0.754\n\u00c9poca  40 | TrainLoss 0.7620 | ValLoss 0.7043 | TrainAcc 0.680 | ValAcc 0.761\n\u00c9poca  50 | TrainLoss 0.7406 | ValLoss 0.6818 | TrainAcc 0.693 | ValAcc 0.776\n\u00c9poca  60 | TrainLoss 0.7241 | ValLoss 0.6648 | TrainAcc 0.702 | ValAcc 0.791\n\u00c9poca  70 | TrainLoss 0.7103 | ValLoss 0.6511 | TrainAcc 0.707 | ValAcc 0.776\n\u00c9poca  80 | TrainLoss 0.6986 | ValLoss 0.6398 | TrainAcc 0.712 | ValAcc 0.784\n\u00c9poca  90 | TrainLoss 0.6883 | ValLoss 0.6304 | TrainAcc 0.718 | ValAcc 0.791\n\u00c9poca 100 | TrainLoss 0.6787 | ValLoss 0.6216 | TrainAcc 0.718 | ValAcc 0.791\n\u00c9poca 110 | TrainLoss 0.6698 | ValLoss 0.6133 | TrainAcc 0.723 | ValAcc 0.791\n\u00c9poca 120 | TrainLoss 0.6616 | ValLoss 0.6059 | TrainAcc 0.721 | ValAcc 0.799\n\u00c9poca 130 | TrainLoss 0.6536 | ValLoss 0.5990 | TrainAcc 0.724 | ValAcc 0.799\n\u00c9poca 140 | TrainLoss 0.6462 | ValLoss 0.5923 | TrainAcc 0.731 | ValAcc 0.799\n\u00c9poca 150 | TrainLoss 0.6391 | ValLoss 0.5860 | TrainAcc 0.731 | ValAcc 0.799\n\u00c9poca 160 | TrainLoss 0.6322 | ValLoss 0.5803 | TrainAcc 0.737 | ValAcc 0.791\n\u00c9poca 170 | TrainLoss 0.6258 | ValLoss 0.5749 | TrainAcc 0.741 | ValAcc 0.799\n\u00c9poca 180 | TrainLoss 0.6197 | ValLoss 0.5698 | TrainAcc 0.745 | ValAcc 0.799\n\u00c9poca 190 | TrainLoss 0.6137 | ValLoss 0.5651 | TrainAcc 0.747 | ValAcc 0.799\n\u00c9poca 200 | TrainLoss 0.6079 | ValLoss 0.5605 | TrainAcc 0.747 | ValAcc 0.799\n\u00c9poca 210 | TrainLoss 0.6024 | ValLoss 0.5560 | TrainAcc 0.748 | ValAcc 0.799\n\u00c9poca 220 | TrainLoss 0.5972 | ValLoss 0.5518 | TrainAcc 0.750 | ValAcc 0.799\n\u00c9poca 230 | TrainLoss 0.5921 | ValLoss 0.5473 | TrainAcc 0.752 | ValAcc 0.791\n\u00c9poca 240 | TrainLoss 0.5871 | ValLoss 0.5440 | TrainAcc 0.756 | ValAcc 0.791\n\u00c9poca 250 | TrainLoss 0.5822 | ValLoss 0.5405 | TrainAcc 0.760 | ValAcc 0.791\n\u00c9poca 260 | TrainLoss 0.5776 | ValLoss 0.5370 | TrainAcc 0.764 | ValAcc 0.799\n\u00c9poca 270 | TrainLoss 0.5732 | ValLoss 0.5337 | TrainAcc 0.765 | ValAcc 0.799\n\u00c9poca 280 | TrainLoss 0.5689 | ValLoss 0.5309 | TrainAcc 0.765 | ValAcc 0.799\n\u00c9poca 290 | TrainLoss 0.5646 | ValLoss 0.5277 | TrainAcc 0.773 | ValAcc 0.799\n\u00c9poca 300 | TrainLoss 0.5605 | ValLoss 0.5245 | TrainAcc 0.774 | ValAcc 0.799\n\u00c9poca 310 | TrainLoss 0.5566 | ValLoss 0.5213 | TrainAcc 0.780 | ValAcc 0.813\n\u00c9poca 320 | TrainLoss 0.5527 | ValLoss 0.5187 | TrainAcc 0.783 | ValAcc 0.821\n\u00c9poca 330 | TrainLoss 0.5488 | ValLoss 0.5160 | TrainAcc 0.786 | ValAcc 0.828\n\u00c9poca 340 | TrainLoss 0.5453 | ValLoss 0.5137 | TrainAcc 0.793 | ValAcc 0.836\n\u00c9poca 350 | TrainLoss 0.5418 | ValLoss 0.5110 | TrainAcc 0.793 | ValAcc 0.828\n\u00c9poca 360 | TrainLoss 0.5382 | ValLoss 0.5085 | TrainAcc 0.799 | ValAcc 0.836\n\u00c9poca 370 | TrainLoss 0.5347 | ValLoss 0.5059 | TrainAcc 0.800 | ValAcc 0.836\n\u00c9poca 380 | TrainLoss 0.5315 | ValLoss 0.5034 | TrainAcc 0.799 | ValAcc 0.836\n\u00c9poca 390 | TrainLoss 0.5284 | ValLoss 0.5009 | TrainAcc 0.800 | ValAcc 0.836\n\u00c9poca 400 | TrainLoss 0.5252 | ValLoss 0.4986 | TrainAcc 0.801 | ValAcc 0.836\n\u00c9poca 410 | TrainLoss 0.5221 | ValLoss 0.4967 | TrainAcc 0.803 | ValAcc 0.843\n\u00c9poca 420 | TrainLoss 0.5190 | ValLoss 0.4945 | TrainAcc 0.805 | ValAcc 0.851\n\u00c9poca 430 | TrainLoss 0.5160 | ValLoss 0.4924 | TrainAcc 0.805 | ValAcc 0.851\n\u00c9poca 440 | TrainLoss 0.5133 | ValLoss 0.4908 | TrainAcc 0.806 | ValAcc 0.851\n\u00c9poca 450 | TrainLoss 0.5104 | ValLoss 0.4885 | TrainAcc 0.808 | ValAcc 0.851\n\u00c9poca 460 | TrainLoss 0.5078 | ValLoss 0.4864 | TrainAcc 0.809 | ValAcc 0.851\n\u00c9poca 470 | TrainLoss 0.5051 | ValLoss 0.4845 | TrainAcc 0.810 | ValAcc 0.851\n\u00c9poca 480 | TrainLoss 0.5025 | ValLoss 0.4821 | TrainAcc 0.812 | ValAcc 0.851\n\u00c9poca 490 | TrainLoss 0.4998 | ValLoss 0.4800 | TrainAcc 0.815 | ValAcc 0.851\n\u00c9poca 500 | TrainLoss 0.4974 | ValLoss 0.4783 | TrainAcc 0.817 | ValAcc 0.851\n</pre> <pre>/tmp/ipykernel_64618/3370443248.py:247: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  axs[1,1].scatter(X_test_p[~correct,0], X_test_p[~correct,1], c=y_test[~correct], cmap=cmap, edgecolors='k', s=60, marker='x', label=\"Errados\")\n</pre> <pre>M\u00e9tricas (teste)\nAcur\u00e1cia: 0.7667\n</pre> In\u00a0[15]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom matplotlib.patches import Patch\n\nnp.random.seed(42)\n\n# 1 Gerar dados: 3 classes com (2,3,4) clusters\ndef gen_keep_label(n_samples_large, n_features, clusters, keep_label=0, rs=0):\n    X, y = make_classification(\n        n_samples=n_samples_large,\n        n_features=n_features,\n        n_informative=n_features,\n        n_redundant=0,\n        n_classes=3,\n        n_clusters_per_class=clusters,\n        class_sep=1.2,\n        flip_y=0.02,\n        random_state=rs\n    )\n    mask = (y == keep_label)\n    return X[mask], y[mask]\n\nn_each = 500\nn_features = 4\n\nX2_all, _ = gen_keep_label(9000, n_features=n_features, clusters=2, keep_label=0, rs=1)\nX3_all, _ = gen_keep_label(10000, n_features=n_features, clusters=3, keep_label=0, rs=2)\nX4_all, _ = gen_keep_label(11000, n_features=n_features, clusters=4, keep_label=0, rs=3)\n\nidx2 = np.random.choice(len(X2_all), n_each, replace=False)\nidx3 = np.random.choice(len(X3_all), n_each, replace=False)\nidx4 = np.random.choice(len(X4_all), n_each, replace=False)\n\nX2 = X2_all[idx2]\nX3 = X3_all[idx3]\nX4 = X4_all[idx4]\n\nX = np.vstack([X2, X3, X4])\ny = np.hstack([np.zeros(n_each), np.ones(n_each), np.full(n_each, 2)])\nperm = np.random.permutation(len(X))\nX, y = X[perm], y[perm]\n\nX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\nval_rel = 0.15 / (1 - 0.15) \nX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_rel, random_state=42, stratify=y_temp)\n\nnum_classes = 3\nN, D = X_train.shape\n\ndef one_hot(y, K):\n    eye = np.eye(K, dtype=float)\n    return eye[y.astype(int)]\n\n# 2 MLP mais profundo: 3 camadas ocultas\nrng = np.random.default_rng(1234)\nH1, H2, H3 = 128, 64, 32 \n\nW1 = rng.normal(0, 1, (H1, D)) / np.sqrt(D)\nb1 = np.zeros(H1)\n\nW2 = rng.normal(0, 1, (H2, H1)) / np.sqrt(H1)\nb2 = np.zeros(H2)\n\nW3 = rng.normal(0, 1, (H3, H2)) / np.sqrt(H2)\nb3 = np.zeros(H3)\n\nW4 = rng.normal(0, 1, (num_classes, H3)) / np.sqrt(H3)\nb4 = np.zeros(num_classes)\n\ndef tanh(x): return np.tanh(x)\ndef tanh_prime(x): return 1.0 - np.tanh(x)**2\n\ndef softmax(x):\n    x = x - x.max(axis=1, keepdims=True)\n    e = np.exp(x)\n    return e / e.sum(axis=1, keepdims=True)\n\ndef forward_batch(Xb):\n    z1 = Xb @ W1.T + b1      # (B, H1)\n    h1 = tanh(z1)\n    z2 = h1 @ W2.T + b2      # (B, H2)\n    h2 = tanh(z2)\n    z3 = h2 @ W3.T + b3      # (B, H3)\n    h3 = tanh(z3)\n    z4 = h3 @ W4.T + b4      # (B, C)\n    yhat = softmax(z4)\n    cache = (z1, h1, z2, h2, z3, h3, z4, yhat)\n    return yhat, cache\n\ndef cross_entropy_loss(yhat, yb_onehot):\n    eps = 1e-9\n    return -np.mean(np.sum(yb_onehot * np.log(yhat + eps), axis=1))\n\n# 3 Treinamento (mini-batch SGD + L2 + early stopping)\nlr = 0.003\nepochs = 600\nbatch = 128\nlambda_l2 = 1e-4\npatience = 25\n\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\n\nbest_val = np.inf\nbad_epochs = 0\nbest_params = None\n\nfor ep in range(1, epochs+1):\n    perm = rng.permutation(X_train.shape[0])\n    Xs = X_train[perm]; ys = y_train[perm]\n    epoch_loss = 0.0\n\n    for i in range(0, Xs.shape[0], batch):\n        xb = Xs[i:i+batch]\n        yb_idx = ys[i:i+batch].astype(int)\n        yb = one_hot(yb_idx, num_classes)\n        B = xb.shape[0]\n\n        # forward\n        z1 = xb @ W1.T + b1; h1 = tanh(z1)\n        z2 = h1 @ W2.T + b2; h2 = tanh(z2)\n        z3 = h2 @ W3.T + b3; h3 = tanh(z3)\n        z4 = h3 @ W4.T + b4; yhat = softmax(z4)\n\n        # loss + L2\n        loss_ce = cross_entropy_loss(yhat, yb)\n        l2_term = lambda_l2 * (np.sum(W1*W1)+np.sum(W2*W2)+np.sum(W3*W3)+np.sum(W4*W4))\n        loss = loss_ce + l2_term\n        epoch_loss += loss * B\n\n        # backward (softmax + CE -&gt; dZ = yhat - y)\n        d4 = (yhat - yb) / B                    # (B, C)\n        gW4 = d4.T @ h3 + 2*lambda_l2 * W4      # (C, H3)\n        gb4 = d4.sum(axis=0)                    # (C,)\n\n        dh3 = d4 @ W4                           # (B, H3)\n        dz3 = dh3 * tanh_prime(z3)              # (B, H3)\n        gW3 = dz3.T @ h2 + 2*lambda_l2 * W3     # (H3, H2)\n        gb3 = dz3.sum(axis=0)                   # (H3,)\n\n        dh2 = dz3 @ W3                          # (B, H2)\n        dz2 = dh2 * tanh_prime(z2)              # (B, H2)\n        gW2 = dz2.T @ h1 + 2*lambda_l2 * W2     # (H2, H1)\n        gb2 = dz2.sum(axis=0)                   # (H2,)\n\n        dh1 = dz2 @ W2                          # (B, H1)\n        dz1 = dh1 * tanh_prime(z1)              # (B, H1)\n        gW1 = dz1.T @ xb + 2*lambda_l2 * W1     # (H1, D)\n        gb1 = dz1.sum(axis=0)                   # (H1,)\n\n        # ataulizacao dos parametros\n        W4 -= lr * gW4\n        b4 -= lr * gb4\n        W3 -= lr * gW3\n        b3 -= lr * gb3\n        W2 -= lr * gW2\n        b2 -= lr * gb2\n        W1 -= lr * gW1\n        b1 -= lr * gb1\n\n    epoch_loss /= X_train.shape[0]\n\n    # metricas\n    yhat_tr, _ = forward_batch(X_train)\n    train_acc = (np.argmax(yhat_tr, axis=1) == y_train).mean()\n    yhat_val, _ = forward_batch(X_val)\n    Y_val_oh = one_hot(y_val.astype(int), num_classes)\n    val_loss = cross_entropy_loss(yhat_val, Y_val_oh)\n    val_acc = (np.argmax(yhat_val, axis=1) == y_val).mean()\n\n    train_losses.append(epoch_loss)\n    val_losses.append(val_loss)\n    train_accs.append(train_acc)\n    val_accs.append(val_acc)\n\n    # early stopping\n    if val_loss &lt; best_val - 1e-6:\n        best_val = val_loss\n        bad_epochs = 0\n        best_params = (W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy(), W4.copy(), b4.copy())\n    else:\n        bad_epochs += 1\n        if bad_epochs &gt;= patience:\n            print(f\"Early stopping (\u00e9poca {ep}) melhor val_loss={best_val:.4f}\")\n            break\n\n    if ep % 20 == 0 or ep == 1:\n        print(f\"\u00c9poca {ep:3d} | TrainLoss {epoch_loss:.4f} | ValLoss {val_loss:.4f} | TrainAcc {train_acc:.3f} | ValAcc {val_acc:.3f}\")\n\n# pega o melhor\nif best_params is not None:\n    W1, b1, W2, b2, W3, b3, W4, b4 = best_params\n\n# 4 Avalia\u00e7\u00e3o final e plots em 1 figura 2x2\nyhat_test, _ = forward_batch(X_test)\ny_pred_test = np.argmax(yhat_test, axis=1)\nacc_test = accuracy_score(y_test, y_pred_test)\ncm = confusion_matrix(y_test, y_pred_test)\n\n# PCA para visuaslizacao da fronteira\npca = PCA(n_components=2, random_state=42)\nX_train_p = pca.fit_transform(X_train)\nX_test_p = pca.transform(X_test)\n\nx_min, x_max = X_train_p[:,0].min()-0.3, X_train_p[:,0].max()+0.3\ny_min, y_max = X_train_p[:,1].min()-0.3, X_train_p[:,1].max()+0.3\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\ngrid_pca = np.c_[xx.ravel(), yy.ravel()]\ngrid_full = pca.inverse_transform(grid_pca)\ny_grid, _ = forward_batch(grid_full)\ny_grid_cls = np.argmax(y_grid, axis=1).reshape(xx.shape)\n\n# plot 2x2: Loss, Acc, Confusion, Fronteira de decidaso\nfig, axs = plt.subplots(2,2, figsize=(14,12))\n\n# loss\naxs[0,0].plot(train_losses, label=\"Train Loss\")\naxs[0,0].plot(val_losses, label=\"Val Loss\")\naxs[0,0].set_title(\"Curva de Loss\")\naxs[0,0].set_xlabel(\"\u00c9poca\"); axs[0,0].set_ylabel(\"Loss\")\naxs[0,0].legend(); axs[0,0].grid(True)\n\n# acc\naxs[0,1].plot(train_accs, label=\"Train Acc\")\naxs[0,1].plot(val_accs, label=\"Val Acc\")\naxs[0,1].set_title(\"Curva de Acur\u00e1cia\")\naxs[0,1].set_xlabel(\"\u00c9poca\"); axs[0,1].set_ylabel(\"Acur\u00e1cia\")\naxs[0,1].legend(); axs[0,1].grid(True)\n\n# confusion\nim = axs[1,0].imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\nfor (i,j), val in np.ndenumerate(cm):\n    axs[1,0].text(j, i, val, ha='center', va='center', color='white' if val&gt;cm.max()/2 else 'black', fontsize=12)\naxs[1,0].set_xticks(np.arange(num_classes)); axs[1,0].set_yticks(np.arange(num_classes))\naxs[1,0].set_xticklabels([f\"Pred {i}\" for i in range(num_classes)])\naxs[1,0].set_yticklabels([f\"True {i}\" for i in range(num_classes)])\naxs[1,0].set_title(f\"Matriz de Confus\u00e3o (Teste) \u2014 acc {acc_test:.4f}\")\nfig.colorbar(im, ax=axs[1,0])\n\n# fronteira de decisao (PCA)\ncmap = plt.cm.Set1 if num_classes &lt;= 9 else plt.cm.tab20\ncs = axs[1,1].contourf(xx, yy, y_grid_cls, levels=np.arange(num_classes+1)-0.5, alpha=0.35, cmap=cmap)\ncorrect = (y_pred_test == y_test)\naxs[1,1].scatter(X_test_p[correct,0], X_test_p[correct,1], c=y_test[correct], cmap=cmap, edgecolors='k', s=40, marker='o', label='Corretos')\naxs[1,1].scatter(X_test_p[~correct,0], X_test_p[~correct,1], c=y_test[~correct], cmap=cmap, edgecolors='k', s=70, marker='x', label='Errados')\naxs[1,1].set_title(\"Decision Boundary (PCA 2D)\")\naxs[1,1].set_xlabel(\"PC1\"); axs[1,1].set_ylabel(\"PC2\")\nlegend_handles = [Patch(color=cmap(i / max(3,num_classes)), label=f\"Classe {i}\") for i in range(num_classes)]\naxs[1,1].legend(handles=legend_handles + [\n    plt.Line2D([0],[0], marker='o', color='w', markerfacecolor='gray', markeredgecolor='k', label='Corretos', markersize=8),\n    plt.Line2D([0],[0], marker='x', color='k', label='Errados', markersize=8)\n], loc='upper right')\nfig.colorbar(cs, ax=axs[1,1])\n\nplt.tight_layout()\nplt.show()\n\n# print final metrics\nprint(\"\\nM\u00e9tricas (teste)\")\nprint(f\"Acur\u00e1cia: {acc_test:.4f}\")\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(num_classes))\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay from matplotlib.patches import Patch  np.random.seed(42)  # 1 Gerar dados: 3 classes com (2,3,4) clusters def gen_keep_label(n_samples_large, n_features, clusters, keep_label=0, rs=0):     X, y = make_classification(         n_samples=n_samples_large,         n_features=n_features,         n_informative=n_features,         n_redundant=0,         n_classes=3,         n_clusters_per_class=clusters,         class_sep=1.2,         flip_y=0.02,         random_state=rs     )     mask = (y == keep_label)     return X[mask], y[mask]  n_each = 500 n_features = 4  X2_all, _ = gen_keep_label(9000, n_features=n_features, clusters=2, keep_label=0, rs=1) X3_all, _ = gen_keep_label(10000, n_features=n_features, clusters=3, keep_label=0, rs=2) X4_all, _ = gen_keep_label(11000, n_features=n_features, clusters=4, keep_label=0, rs=3)  idx2 = np.random.choice(len(X2_all), n_each, replace=False) idx3 = np.random.choice(len(X3_all), n_each, replace=False) idx4 = np.random.choice(len(X4_all), n_each, replace=False)  X2 = X2_all[idx2] X3 = X3_all[idx3] X4 = X4_all[idx4]  X = np.vstack([X2, X3, X4]) y = np.hstack([np.zeros(n_each), np.ones(n_each), np.full(n_each, 2)]) perm = np.random.permutation(len(X)) X, y = X[perm], y[perm]  X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y) val_rel = 0.15 / (1 - 0.15)  X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_rel, random_state=42, stratify=y_temp)  num_classes = 3 N, D = X_train.shape  def one_hot(y, K):     eye = np.eye(K, dtype=float)     return eye[y.astype(int)]  # 2 MLP mais profundo: 3 camadas ocultas rng = np.random.default_rng(1234) H1, H2, H3 = 128, 64, 32   W1 = rng.normal(0, 1, (H1, D)) / np.sqrt(D) b1 = np.zeros(H1)  W2 = rng.normal(0, 1, (H2, H1)) / np.sqrt(H1) b2 = np.zeros(H2)  W3 = rng.normal(0, 1, (H3, H2)) / np.sqrt(H2) b3 = np.zeros(H3)  W4 = rng.normal(0, 1, (num_classes, H3)) / np.sqrt(H3) b4 = np.zeros(num_classes)  def tanh(x): return np.tanh(x) def tanh_prime(x): return 1.0 - np.tanh(x)**2  def softmax(x):     x = x - x.max(axis=1, keepdims=True)     e = np.exp(x)     return e / e.sum(axis=1, keepdims=True)  def forward_batch(Xb):     z1 = Xb @ W1.T + b1      # (B, H1)     h1 = tanh(z1)     z2 = h1 @ W2.T + b2      # (B, H2)     h2 = tanh(z2)     z3 = h2 @ W3.T + b3      # (B, H3)     h3 = tanh(z3)     z4 = h3 @ W4.T + b4      # (B, C)     yhat = softmax(z4)     cache = (z1, h1, z2, h2, z3, h3, z4, yhat)     return yhat, cache  def cross_entropy_loss(yhat, yb_onehot):     eps = 1e-9     return -np.mean(np.sum(yb_onehot * np.log(yhat + eps), axis=1))  # 3 Treinamento (mini-batch SGD + L2 + early stopping) lr = 0.003 epochs = 600 batch = 128 lambda_l2 = 1e-4 patience = 25  train_losses, val_losses = [], [] train_accs, val_accs = [], []  best_val = np.inf bad_epochs = 0 best_params = None  for ep in range(1, epochs+1):     perm = rng.permutation(X_train.shape[0])     Xs = X_train[perm]; ys = y_train[perm]     epoch_loss = 0.0      for i in range(0, Xs.shape[0], batch):         xb = Xs[i:i+batch]         yb_idx = ys[i:i+batch].astype(int)         yb = one_hot(yb_idx, num_classes)         B = xb.shape[0]          # forward         z1 = xb @ W1.T + b1; h1 = tanh(z1)         z2 = h1 @ W2.T + b2; h2 = tanh(z2)         z3 = h2 @ W3.T + b3; h3 = tanh(z3)         z4 = h3 @ W4.T + b4; yhat = softmax(z4)          # loss + L2         loss_ce = cross_entropy_loss(yhat, yb)         l2_term = lambda_l2 * (np.sum(W1*W1)+np.sum(W2*W2)+np.sum(W3*W3)+np.sum(W4*W4))         loss = loss_ce + l2_term         epoch_loss += loss * B          # backward (softmax + CE -&gt; dZ = yhat - y)         d4 = (yhat - yb) / B                    # (B, C)         gW4 = d4.T @ h3 + 2*lambda_l2 * W4      # (C, H3)         gb4 = d4.sum(axis=0)                    # (C,)          dh3 = d4 @ W4                           # (B, H3)         dz3 = dh3 * tanh_prime(z3)              # (B, H3)         gW3 = dz3.T @ h2 + 2*lambda_l2 * W3     # (H3, H2)         gb3 = dz3.sum(axis=0)                   # (H3,)          dh2 = dz3 @ W3                          # (B, H2)         dz2 = dh2 * tanh_prime(z2)              # (B, H2)         gW2 = dz2.T @ h1 + 2*lambda_l2 * W2     # (H2, H1)         gb2 = dz2.sum(axis=0)                   # (H2,)          dh1 = dz2 @ W2                          # (B, H1)         dz1 = dh1 * tanh_prime(z1)              # (B, H1)         gW1 = dz1.T @ xb + 2*lambda_l2 * W1     # (H1, D)         gb1 = dz1.sum(axis=0)                   # (H1,)          # ataulizacao dos parametros         W4 -= lr * gW4         b4 -= lr * gb4         W3 -= lr * gW3         b3 -= lr * gb3         W2 -= lr * gW2         b2 -= lr * gb2         W1 -= lr * gW1         b1 -= lr * gb1      epoch_loss /= X_train.shape[0]      # metricas     yhat_tr, _ = forward_batch(X_train)     train_acc = (np.argmax(yhat_tr, axis=1) == y_train).mean()     yhat_val, _ = forward_batch(X_val)     Y_val_oh = one_hot(y_val.astype(int), num_classes)     val_loss = cross_entropy_loss(yhat_val, Y_val_oh)     val_acc = (np.argmax(yhat_val, axis=1) == y_val).mean()      train_losses.append(epoch_loss)     val_losses.append(val_loss)     train_accs.append(train_acc)     val_accs.append(val_acc)      # early stopping     if val_loss &lt; best_val - 1e-6:         best_val = val_loss         bad_epochs = 0         best_params = (W1.copy(), b1.copy(), W2.copy(), b2.copy(), W3.copy(), b3.copy(), W4.copy(), b4.copy())     else:         bad_epochs += 1         if bad_epochs &gt;= patience:             print(f\"Early stopping (\u00e9poca {ep}) melhor val_loss={best_val:.4f}\")             break      if ep % 20 == 0 or ep == 1:         print(f\"\u00c9poca {ep:3d} | TrainLoss {epoch_loss:.4f} | ValLoss {val_loss:.4f} | TrainAcc {train_acc:.3f} | ValAcc {val_acc:.3f}\")  # pega o melhor if best_params is not None:     W1, b1, W2, b2, W3, b3, W4, b4 = best_params  # 4 Avalia\u00e7\u00e3o final e plots em 1 figura 2x2 yhat_test, _ = forward_batch(X_test) y_pred_test = np.argmax(yhat_test, axis=1) acc_test = accuracy_score(y_test, y_pred_test) cm = confusion_matrix(y_test, y_pred_test)  # PCA para visuaslizacao da fronteira pca = PCA(n_components=2, random_state=42) X_train_p = pca.fit_transform(X_train) X_test_p = pca.transform(X_test)  x_min, x_max = X_train_p[:,0].min()-0.3, X_train_p[:,0].max()+0.3 y_min, y_max = X_train_p[:,1].min()-0.3, X_train_p[:,1].max()+0.3 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300)) grid_pca = np.c_[xx.ravel(), yy.ravel()] grid_full = pca.inverse_transform(grid_pca) y_grid, _ = forward_batch(grid_full) y_grid_cls = np.argmax(y_grid, axis=1).reshape(xx.shape)  # plot 2x2: Loss, Acc, Confusion, Fronteira de decidaso fig, axs = plt.subplots(2,2, figsize=(14,12))  # loss axs[0,0].plot(train_losses, label=\"Train Loss\") axs[0,0].plot(val_losses, label=\"Val Loss\") axs[0,0].set_title(\"Curva de Loss\") axs[0,0].set_xlabel(\"\u00c9poca\"); axs[0,0].set_ylabel(\"Loss\") axs[0,0].legend(); axs[0,0].grid(True)  # acc axs[0,1].plot(train_accs, label=\"Train Acc\") axs[0,1].plot(val_accs, label=\"Val Acc\") axs[0,1].set_title(\"Curva de Acur\u00e1cia\") axs[0,1].set_xlabel(\"\u00c9poca\"); axs[0,1].set_ylabel(\"Acur\u00e1cia\") axs[0,1].legend(); axs[0,1].grid(True)  # confusion im = axs[1,0].imshow(cm, cmap=\"Blues\", interpolation=\"nearest\") for (i,j), val in np.ndenumerate(cm):     axs[1,0].text(j, i, val, ha='center', va='center', color='white' if val&gt;cm.max()/2 else 'black', fontsize=12) axs[1,0].set_xticks(np.arange(num_classes)); axs[1,0].set_yticks(np.arange(num_classes)) axs[1,0].set_xticklabels([f\"Pred {i}\" for i in range(num_classes)]) axs[1,0].set_yticklabels([f\"True {i}\" for i in range(num_classes)]) axs[1,0].set_title(f\"Matriz de Confus\u00e3o (Teste) \u2014 acc {acc_test:.4f}\") fig.colorbar(im, ax=axs[1,0])  # fronteira de decisao (PCA) cmap = plt.cm.Set1 if num_classes &lt;= 9 else plt.cm.tab20 cs = axs[1,1].contourf(xx, yy, y_grid_cls, levels=np.arange(num_classes+1)-0.5, alpha=0.35, cmap=cmap) correct = (y_pred_test == y_test) axs[1,1].scatter(X_test_p[correct,0], X_test_p[correct,1], c=y_test[correct], cmap=cmap, edgecolors='k', s=40, marker='o', label='Corretos') axs[1,1].scatter(X_test_p[~correct,0], X_test_p[~correct,1], c=y_test[~correct], cmap=cmap, edgecolors='k', s=70, marker='x', label='Errados') axs[1,1].set_title(\"Decision Boundary (PCA 2D)\") axs[1,1].set_xlabel(\"PC1\"); axs[1,1].set_ylabel(\"PC2\") legend_handles = [Patch(color=cmap(i / max(3,num_classes)), label=f\"Classe {i}\") for i in range(num_classes)] axs[1,1].legend(handles=legend_handles + [     plt.Line2D([0],[0], marker='o', color='w', markerfacecolor='gray', markeredgecolor='k', label='Corretos', markersize=8),     plt.Line2D([0],[0], marker='x', color='k', label='Errados', markersize=8) ], loc='upper right') fig.colorbar(cs, ax=axs[1,1])  plt.tight_layout() plt.show()  # print final metrics print(\"\\nM\u00e9tricas (teste)\") print(f\"Acur\u00e1cia: {acc_test:.4f}\") disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(num_classes))  <pre>\u00c9poca   1 | TrainLoss 1.1599 | ValLoss 1.1352 | TrainAcc 0.378 | ValAcc 0.319\n\u00c9poca  20 | TrainLoss 0.8762 | ValLoss 0.8932 | TrainAcc 0.603 | ValAcc 0.549\n\u00c9poca  40 | TrainLoss 0.8103 | ValLoss 0.8158 | TrainAcc 0.653 | ValAcc 0.633\n\u00c9poca  60 | TrainLoss 0.7713 | ValLoss 0.7759 | TrainAcc 0.669 | ValAcc 0.668\n\u00c9poca  80 | TrainLoss 0.7426 | ValLoss 0.7466 | TrainAcc 0.696 | ValAcc 0.673\n\u00c9poca 100 | TrainLoss 0.7186 | ValLoss 0.7220 | TrainAcc 0.708 | ValAcc 0.681\n\u00c9poca 120 | TrainLoss 0.6977 | ValLoss 0.7065 | TrainAcc 0.715 | ValAcc 0.686\n\u00c9poca 140 | TrainLoss 0.6789 | ValLoss 0.6876 | TrainAcc 0.728 | ValAcc 0.690\n\u00c9poca 160 | TrainLoss 0.6619 | ValLoss 0.6726 | TrainAcc 0.738 | ValAcc 0.690\n\u00c9poca 180 | TrainLoss 0.6466 | ValLoss 0.6610 | TrainAcc 0.745 | ValAcc 0.704\n\u00c9poca 200 | TrainLoss 0.6327 | ValLoss 0.6491 | TrainAcc 0.751 | ValAcc 0.717\n\u00c9poca 220 | TrainLoss 0.6201 | ValLoss 0.6396 | TrainAcc 0.758 | ValAcc 0.726\n\u00c9poca 240 | TrainLoss 0.6087 | ValLoss 0.6297 | TrainAcc 0.759 | ValAcc 0.726\n\u00c9poca 260 | TrainLoss 0.5981 | ValLoss 0.6202 | TrainAcc 0.763 | ValAcc 0.730\n\u00c9poca 280 | TrainLoss 0.5886 | ValLoss 0.6165 | TrainAcc 0.765 | ValAcc 0.726\n\u00c9poca 300 | TrainLoss 0.5798 | ValLoss 0.6049 | TrainAcc 0.765 | ValAcc 0.743\n\u00c9poca 320 | TrainLoss 0.5715 | ValLoss 0.6005 | TrainAcc 0.770 | ValAcc 0.748\n\u00c9poca 340 | TrainLoss 0.5638 | ValLoss 0.5933 | TrainAcc 0.773 | ValAcc 0.739\n\u00c9poca 360 | TrainLoss 0.5567 | ValLoss 0.5889 | TrainAcc 0.776 | ValAcc 0.748\n\u00c9poca 380 | TrainLoss 0.5499 | ValLoss 0.5815 | TrainAcc 0.777 | ValAcc 0.743\n\u00c9poca 400 | TrainLoss 0.5434 | ValLoss 0.5785 | TrainAcc 0.780 | ValAcc 0.743\n\u00c9poca 420 | TrainLoss 0.5369 | ValLoss 0.5742 | TrainAcc 0.778 | ValAcc 0.748\n\u00c9poca 440 | TrainLoss 0.5312 | ValLoss 0.5709 | TrainAcc 0.781 | ValAcc 0.748\n\u00c9poca 460 | TrainLoss 0.5257 | ValLoss 0.5674 | TrainAcc 0.791 | ValAcc 0.752\n\u00c9poca 480 | TrainLoss 0.5205 | ValLoss 0.5629 | TrainAcc 0.787 | ValAcc 0.757\n\u00c9poca 500 | TrainLoss 0.5150 | ValLoss 0.5550 | TrainAcc 0.793 | ValAcc 0.765\n\u00c9poca 520 | TrainLoss 0.5100 | ValLoss 0.5560 | TrainAcc 0.792 | ValAcc 0.765\n\u00c9poca 540 | TrainLoss 0.5052 | ValLoss 0.5496 | TrainAcc 0.797 | ValAcc 0.779\n\u00c9poca 560 | TrainLoss 0.5006 | ValLoss 0.5486 | TrainAcc 0.796 | ValAcc 0.770\n\u00c9poca 580 | TrainLoss 0.4963 | ValLoss 0.5424 | TrainAcc 0.794 | ValAcc 0.783\n\u00c9poca 600 | TrainLoss 0.4917 | ValLoss 0.5419 | TrainAcc 0.798 | ValAcc 0.774\n</pre> <pre>/tmp/ipykernel_64618/267614986.py:249: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  axs[1,1].scatter(X_test_p[~correct,0], X_test_p[~correct,1], c=y_test[~correct], cmap=cmap, edgecolors='k', s=70, marker='x', label='Errados')\n</pre> <pre>M\u00e9tricas (teste)\nAcur\u00e1cia: 0.7600\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"exercicios/mlp/#mlp","title":"MLP\u00b6","text":""},{"location":"exercicios/mlp/#exercicio-1-calculo-manual-das-etapas-de-um-mlp","title":"Exerc\u00edcio 1: C\u00e1lculo Manual das Etapas de um MLP\u00b6","text":"<p>Considere um MLP simples com 2 caracter\u00edsticas de entrada, 1 camada oculta contendo 2 neur\u00f4nios e 1 neur\u00f4nio de sa\u00edda. Use a fun\u00e7\u00e3o tangente hiperb\u00f3lica (tanh) como ativa\u00e7\u00e3o tanto para a camada oculta quanto para a camada de sa\u00edda. A fun\u00e7\u00e3o de perda \u00e9 o erro quadr\u00e1tico m\u00e9dio (MSE):</p> <p>$$ L = \\frac{1}{N} (y - \\hat{y})^2, $$</p> <p>onde $\\hat{y}$ \u00e9 a sa\u00edda da rede.</p> <p>Valores fornecidos</p> <ul> <li><p>Entrada e sa\u00edda: $$ \\mathbf{x} = \\begin{bmatrix}0.5\\\\-0.2\\end{bmatrix}, \\qquad y = 1.0 $$</p> </li> <li><p>Pesos da camada oculta: $$ \\mathbf{W}^{(1)} = \\begin{bmatrix}0.3 &amp; -0.1\\\\[4pt] 0.2 &amp; 0.4\\end{bmatrix} $$ (cada linha representa um neur\u00f4nio oculto; as colunas correspondem \u00e0s entradas)</p> </li> <li><p>Biases da camada oculta: $$ \\mathbf{b}^{(1)} = \\begin{bmatrix}0.1\\\\[2pt]-0.2\\end{bmatrix} $$</p> </li> <li><p>Pesos da camada de sa\u00edda: $$ \\mathbf{W}^{(2)} = \\begin{bmatrix}0.5 &amp; -0.3\\end{bmatrix} $$ (1\u00d72)</p> </li> <li><p>Bias da camada de sa\u00edda: $$ b^{(2)} = 0.2 $$</p> </li> <li><p>Taxa de aprendizado: (o enunciado menciona $\\eta=0.3$ no in\u00edcio). Para o passo 4 usaremos $\\eta = 0.1$.</p> </li> </ul> <p>1. Passagem direta (Forward Pass):</p> <p>Calcule:</p> <p>$$ \\mathbf{z}^{(1)} = \\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)} $$</p> <p>$$ \\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)}) $$</p> <p>$$ u^{(2)} = \\mathbf{W}^{(2)}\\mathbf{h}^{(1)} + b^{(2)} $$</p> <p>$$ \\hat{y} = \\tanh(u^{(2)}) $$</p> <p>(Os c\u00e1lculos num\u00e9ricos ser\u00e3o realizados na c\u00e9lula de c\u00f3digo a seguir.)</p> <p>2. C\u00e1lculo da Perda</p> <p>Com $N=1$:</p> <p>$$ L = (y - \\hat{y})^2. $$</p> <p>3. Passagem reversa (Backpropaga\u00e7\u00e3o)</p> <p>Come\u00e7amos com:</p> <p>$$ \\frac{\\partial L}{\\partial \\hat{y}} = 2(\\hat{y}-y). $$</p> <p>Derivada da fun\u00e7\u00e3o $\\tanh$:</p> <p>$$ \\frac{d}{du}\\tanh(u) = 1 - \\tanh^2(u). $$</p> <p>Assim:</p> <p>$$ \\frac{\\partial L}{\\partial u^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{d\\hat{y}}{du^{(2)}} = 2(\\hat{y}-y)\\cdot\\big(1-\\tanh^2(u^{(2)})\\big). $$</p> <p>Gradientes da camada de sa\u00edda:</p> <p>$$ \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\cdot \\mathbf{h}^{(1)\\top} $$</p> <p>$$ \\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}}. $$</p> <p>Propaga\u00e7\u00e3o para a camada oculta:</p> <p>$$ \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = \\mathbf{W}^{(2)\\top} \\cdot \\frac{\\partial L}{\\partial u^{(2)}} $$</p> <p>$$ \\frac{\\partial L}{\\partial z^{(1)}_i} = \\frac{\\partial L}{\\partial h^{(1)}_i}\\cdot\\big(1 - \\tanh^2(z^{(1)}_i)\\big) $$</p> <p>Gradientes da camada oculta:</p> <p>$$ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\left(\\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}}\\right) \\mathbf{x}^\\top $$</p> <p>$$ \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}}. $$</p> <p>4. Atualiza\u00e7\u00e3o dos Par\u00e2metros (Gradiente Descendente)</p> <p>Usando $\\eta = 0.1$:</p> <p>$$ \\mathbf{W}^{(2)} \\leftarrow \\mathbf{W}^{(2)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} $$</p> <p>$$ b^{(2)} \\leftarrow b^{(2)} - \\eta \\frac{\\partial L}{\\partial b^{(2)}} $$</p> <p>$$ \\mathbf{W}^{(1)} \\leftarrow \\mathbf{W}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} $$</p> <p>$$ \\mathbf{b}^{(1)} \\leftarrow \\mathbf{b}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} $$</p>"},{"location":"exercicios/mlp/#resolucao","title":"Resolu\u00e7\u00e3o\u00b6","text":""},{"location":"exercicios/mlp/#exercicio-2-classificacao-binaria-com-dados-sinteticos-e-mlp-do-zero","title":"Exerc\u00edcio 2: Classifica\u00e7\u00e3o Bin\u00e1ria com Dados Sint\u00e9ticos e MLP do Zero\u00b6","text":"<p>Usando a fun\u00e7\u00e3o <code>make_classification</code> da biblioteca scikit-learn (documenta\u00e7\u00e3o{target='_blank'}), gere um conjunto de dados sint\u00e9tico com as seguintes especifica\u00e7\u00f5es:</p> <ul> <li>N\u00famero de amostras: 1000</li> <li>N\u00famero de classes: 2</li> <li>N\u00famero de clusters por classe: Use o par\u00e2metro <code>n_clusters_per_class</code> de forma criativa para obter 1 cluster para uma classe e 2 para a outra (dica: pode ser necess\u00e1rio gerar subconjuntos separadamente e combin\u00e1-los, j\u00e1 que a fun\u00e7\u00e3o aplica o mesmo n\u00famero de clusters para todas as classes por padr\u00e3o).</li> <li>Outros par\u00e2metros: Defina <code>n_features=2</code> para facilitar a visualiza\u00e7\u00e3o, <code>n_informative=2</code>, <code>n_redundant=0</code>, <code>random_state=42</code> para reprodutibilidade, e ajuste <code>class_sep</code> ou <code>flip_y</code> conforme necess\u00e1rio para obter um conjunto de dados desafiador, mas separ\u00e1vel.</li> </ul> <p>Implemente um MLP do zero (sem usar bibliotecas como TensorFlow ou PyTorch para o modelo em si; \u00e9 permitido usar NumPy para opera\u00e7\u00f5es com arrays) para classificar esses dados. Voc\u00ea tem total liberdade para escolher a arquitetura, incluindo:</p> <ul> <li>N\u00famero de camadas ocultas (pelo menos 1)</li> <li>N\u00famero de neur\u00f4nios por camada</li> <li>Fun\u00e7\u00f5es de ativa\u00e7\u00e3o (por exemplo, sigmoid, ReLU, tanh)</li> <li>Fun\u00e7\u00e3o de perda (por exemplo, entropia cruzada bin\u00e1ria)</li> <li>Otimizador (por exemplo, gradiente descendente, com taxa de aprendizado escolhida)</li> </ul> <p>Etapas a seguir:</p> <ol> <li>Gere e divida os dados em conjuntos de treinamento (80%) e teste (20%).</li> <li>Implemente a passagem direta (forward pass), o c\u00e1lculo da perda, a passagem reversa (backward pass) e as atualiza\u00e7\u00f5es de par\u00e2metros em c\u00f3digo.</li> <li>Treine o modelo por um n\u00famero razo\u00e1vel de \u00e9pocas (por exemplo, 100\u2013500), acompanhando a perda de treinamento.</li> <li>Avalie no conjunto de teste: apresente a acur\u00e1cia e, opcionalmente, plote as fronteiras de decis\u00e3o ou a matriz de confus\u00e3o.</li> <li>Envie seu c\u00f3digo e resultados, incluindo quaisquer visualiza\u00e7\u00f5es.</li> </ol>"},{"location":"exercicios/mlp/#resolucao","title":"Resolu\u00e7\u00e3o\u00b6","text":""},{"location":"exercicios/mlp/#exercicio-3-classificacao-multiclasse-com-dados-sinteticos-e-mlp-reutilizavel","title":"Exerc\u00edcio 3: Classifica\u00e7\u00e3o Multiclasse com Dados Sint\u00e9ticos e MLP Reutiliz\u00e1vel\u00b6","text":"<p>Semelhante ao Exerc\u00edcio 2, mas com complexidade aumentada.</p> <p>Use <code>make_classification</code> para gerar um conjunto de dados sint\u00e9tico com:</p> <ul> <li>N\u00famero de amostras: 1500</li> <li>N\u00famero de classes: 3</li> <li>N\u00famero de atributos: 4</li> <li>N\u00famero de clusters por classe: Obtenha 2 clusters para uma classe, 3 para outra e 4 para a \u00faltima (novamente, pode ser necess\u00e1rio gerar subconjuntos separadamente e combin\u00e1-los, j\u00e1 que a fun\u00e7\u00e3o n\u00e3o suporta diretamente um n\u00famero vari\u00e1vel de clusters por classe).</li> <li>Outros par\u00e2metros: <code>n_features=4</code>, <code>n_informative=4</code>, <code>n_redundant=0</code>, <code>random_state=42</code>.</li> </ul> <p>Implemente um MLP do zero para classificar esses dados. Voc\u00ea pode escolher livremente a arquitetura, mas para um ponto extra (elevando este exerc\u00edcio para 4 pontos), reutilize exatamente o mesmo c\u00f3digo de implementa\u00e7\u00e3o do MLP do Exerc\u00edcio 2, modificando apenas os hiperpar\u00e2metros (por exemplo, o tamanho da camada de sa\u00edda para 3 classes, a fun\u00e7\u00e3o de perda para entropia cruzada categ\u00f3rica, se necess\u00e1rio) sem alterar a estrutura principal.</p> <p>Etapas:</p> <ol> <li>Gere e divida os dados (80/20 treino/teste).</li> <li>Treine o modelo, acompanhando a perda.</li> <li>Avalie no conjunto de teste: apresente a acur\u00e1cia e, opcionalmente, visualize (por exemplo, gr\u00e1fico de dispers\u00e3o dos dados com os r\u00f3tulos previstos).</li> <li>Envie o c\u00f3digo e os resultados.</li> </ol>"},{"location":"exercicios/mlp/#resolucao","title":"Resolu\u00e7\u00e3o\u00b6","text":""},{"location":"exercicios/mlp/#exercicio-4-classificacao-multiclasse-com-mlp-mais-profundo","title":"Exerc\u00edcio 4: Classifica\u00e7\u00e3o Multiclasse com MLP Mais Profundo\u00b6","text":"<p>Repita exatamente o Exerc\u00edcio 3, mas agora garanta que seu MLP tenha pelo menos 2 camadas ocultas. Voc\u00ea pode ajustar o n\u00famero de neur\u00f4nios por camada conforme necess\u00e1rio para obter melhor desempenho. Reutilize o c\u00f3digo do Exerc\u00edcio 3 sempre que poss\u00edvel, mas o foco \u00e9 demonstrar a arquitetura mais profunda. Envie o c\u00f3digo atualizado, os resultados do treinamento e a avalia\u00e7\u00e3o no teste.</p>"},{"location":"exercicios/mlp/#resolucao","title":"Resolu\u00e7\u00e3o\u00b6","text":""},{"location":"exercicios/perceptron/main/","title":"Perceptron","text":""},{"location":"exercicios/perceptron/main/#exercicios-de-perceptron","title":"Exerc\u00edcios de Perceptron","text":""},{"location":"exercicios/perceptron/main/#exercicio-1-classificacao-de-dados-linearmente-separaveis","title":"Exerc\u00edcio 1 \u2013 Classifica\u00e7\u00e3o de Dados Linearmente Separ\u00e1veis","text":""},{"location":"exercicios/perceptron/main/#data-generation-task","title":"Data Generation Task:","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:  </p> <ul> <li> <p>Class 0:</p> <p>Mean = \\([2, 2]\\),</p> <p>Covariance matrix = \\([[0.5, 0], [0, 0.5]]\\) (i.e., variance of \\(0.5\\) along each dimension, no covariance).  </p> </li> <li> <p>Class 1:</p> <p>Mean = \\([5, 5]\\),</p> <p>Covariance matrix = \\([[0.5, 0], [0, 0.5]]\\).  </p> </li> </ul> <p>These parameters ensure the classes are mostly linearly separable, with minimal overlap due to the distance between means and low variance. Plot the data points (using libraries like matplotlib if desired) to visualize the separation, coloring points by class.</p>"},{"location":"exercicios/perceptron/main/#perceptron-implementation-task","title":"Perceptron Implementation Task:","text":"<p>Implement a single-layer perceptron from scratch to classify the generated data into the two classes. You may use NumPy only for basic linear algebra operations (e.g., matrix multiplication, vector addition/subtraction, dot products). Do not use any pre-built machine learning libraries (e.g., no scikit-learn) or NumPy functions that directly implement perceptron logic.  </p> <ul> <li>Initialize weights (w) as a 2D vector (plus a bias term b).  </li> <li>Use the perceptron learning rule: For each misclassified sample \\((x, y)\\), update \\(w = w + \u03b7 * y * x\\) and \\(b = b + \u03b7 * y\\), where \\(\u03b7\\) is the learning rate (start with \\(\u03b7=0.01\\)).  </li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point. Track accuracy after each epoch.  </li> <li>After training, evaluate accuracy on the full dataset and plot the decision boundary (line defined by \\(w\u00b7x + b = 0\\)) overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress. Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.  </li> </ul> <p>Report the final weights, bias, accuracy, and discuss why the data's separability leads to quick convergence.</p>"},{"location":"exercicios/perceptron/main/#geracao-dos-dados","title":"Gera\u00e7\u00e3o dos Dados","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nmean_0 = [2, 2]\ncov_0 = [[0.5, 0], [0, 0.5]]\n\nmean_1 = [5, 5]\ncov_1 = [[0.5, 0], [0, 0.5]]\n\nX0 = np.random.multivariate_normal(mean_0, cov_0, 1000)\nX1 = np.random.multivariate_normal(mean_1, cov_1, 1000)\n\ny0 = np.zeros(1000)\ny1 = np.ones(1000)\n\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\nprint(\"Shape dos dados:\", X.shape, y.shape)\n\nplt.figure(figsize=(6,6))\nplt.scatter(X0[:,0], X0[:,1], color=\"blue\", alpha=0.5, label=\"Classe 0\")\nplt.scatter(X1[:,0], X1[:,1], color=\"red\", alpha=0.5, label=\"Classe 1\")\nplt.legend()\nplt.title(\"Dados gerados\")\nplt.show()\n</code></pre>"},{"location":"exercicios/perceptron/main/#implementacao-do-perceptron","title":"Implementa\u00e7\u00e3o do Perceptron","text":"<pre><code>class Perceptron:\n    def __init__(self, input_dim, learning_rate=0.01):\n        self.w = np.random.randn(input_dim) * 0.01\n        self.b = 0.0\n        self.lr = learning_rate\n        self.history = []  \n\n    def activation(self, z):\n        return np.where(z &gt;= 0, 1, 0)\n\n    def predict(self, X):\n        z = np.dot(X, self.w) + self.b\n        return self.activation(z)\n\n    def fit(self, X, y, epochs=100):\n        n_samples = X.shape[0]\n        for epoch in range(epochs):\n            errors = 0\n            for xi, target in zip(X, y):\n                pred = self.predict(xi)\n                error = target - pred\n                if error != 0: \n                    self.w += self.lr * error * xi\n                    self.b += self.lr * error\n                    errors += 1\n\n            y_pred = self.predict(X)\n            acc = np.mean(y_pred == y)\n            self.history.append(acc)\n\n            print(f\"\u00c9poca {epoch+1}/{epochs} - Erros: {errors}, Acur\u00e1cia: {acc:.3f}\")\n            if errors == 0:\n                print(\"Converg\u00eancia atingida!\")\n                break\n\n# Trieno\nperceptron = Perceptron(input_dim=2, learning_rate=0.01)\nperceptron.fit(X, y, epochs=100)\n\nprint(\"Pesos finais:\", perceptron.w)\nprint(\"Bias final:\", perceptron.b)\n\n# Output: Pesos finais: [0.37118936 0.24406321]\n# Output: Bias final: -2.020000000000001\n\ndef plot_decision_boundary(X, y, model):\n    plt.figure(figsize=(6,6))\n\n    plt.scatter(X[y==0, 0], X[y==0, 1], color=\"blue\", alpha=0.5, label=\"Classe 0\")\n    plt.scatter(X[y==1, 0], X[y==1, 1], color=\"red\", alpha=0.5, label=\"Classe 1\")\n\n    x_vals = np.linspace(np.min(X[:,0])-1, np.max(X[:,0])+1, 100)\n    y_vals = -(model.w[0] * x_vals + model.b) / model.w[1]\n\n    plt.plot(x_vals, y_vals, color=\"green\", linewidth=2, label=\"Fronteira\")\n\n    y_pred = model.predict(X)\n    misclassified = X[y != y_pred]\n    if len(misclassified) &gt; 0:\n        plt.scatter(\n            misclassified[:,0], misclassified[:,1],\n            color=\"black\", marker=\"x\", s=80, label=\"Erros\", alpha=0.5\n        )\n\n\n    plt.legend()\n    plt.title(\"Fronteira de decis\u00e3o do Perceptron\")\n    plt.show()\n\nplot_decision_boundary(X, y, perceptron)\n</code></pre>"},{"location":"exercicios/perceptron/main/#resultados","title":"Resultados","text":"<ul> <li>O perceptron converge rapidamente (em poucas \u00e9pocas) para acur\u00e1cia pr\u00f3xima de 100%.</li> <li>Os pesos e bias se ajustam at\u00e9 que a fronteira de decis\u00e3o (reta em 2D) separe bem as duas classes.</li> </ul>"},{"location":"exercicios/perceptron/main/#exercicio-2-dados-parcialmente-sobrepostos","title":"Exerc\u00edcio 2 \u2013 Dados Parcialmente Sobrepostos","text":""},{"location":"exercicios/perceptron/main/#data-generation-task_1","title":"Data Generation Task:","text":"<p>Generate two classes of 2D data points (1000 samples per class) using multivariate normal distributions. Use the following parameters:</p> <ul> <li> <p>Class 0:</p> <p>Mean = \\([3, 3]\\),</p> <p>Covariance matrix = \\([[1.5, 0], [0, 1.5]]\\) (i.e., higher variance of 1.5 along each dimension).</p> </li> <li> <p>Class 1:</p> <p>Mean = \\([4, 4]\\),</p> <p>Covariance matrix = \\([[1.5, 0], [0, 1.5]]\\).  </p> </li> </ul> <p>These parameters create partial overlap between classes due to closer means and higher variance, making the data not fully linearly separable. Plot the data points to visualize the overlap, coloring points by class.</p>"},{"location":"exercicios/perceptron/main/#perceptron-implementation-task_1","title":"Perceptron Implementation Task:","text":"<p>Using the same implementation guidelines as in Exercise 1, train a perceptron on this dataset.  </p> <ul> <li>Follow the same initialization, update rule, and training process.  </li> <li>Train the model until convergence (no weight updates occur in a full pass over the dataset) or for a maximum of 100 epochs, whichever comes first. If convergence is not achieved by 100 epochs, report the accuracy at that point and note any oscillation in updates; consider reporting the best accuracy achieved over multiple runs (e.g., average over 5 random initializations). Track accuracy after each epoch.  </li> <li>Evaluate accuracy after training and plot the decision boundary overlaid on the data points. Additionally, plot the training accuracy over epochs to show convergence progress (or lack thereof). Highlight any misclassified points in a separate plot or by different markers in the decision boundary plot.  </li> </ul> <p>Report the final weights, bias, accuracy, and discuss how the overlap affects training compared to Exercise 1 (e.g., slower convergence or inability to reach 100% accuracy).</p>"},{"location":"exercicios/perceptron/main/#geracao-dos-dados_1","title":"Gera\u00e7\u00e3o dos Dados","text":"<pre><code>np.random.seed(42)\n\nmean_0 = [3, 3]\ncov_0 = [[1.5, 0], [0, 1.5]]\n\nmean_1 = [4, 4]\ncov_1 = [[1.5, 0], [0, 1.5]]\n\nX0 = np.random.multivariate_normal(mean_0, cov_0, 1000)\nX1 = np.random.multivariate_normal(mean_1, cov_1, 1000)\n\ny0 = np.zeros(1000)\ny1 = np.ones(1000)\n\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\nprint(\"Shape dos dados:\", X.shape, y.shape)\n\nplt.figure(figsize=(6,6))\nplt.scatter(X0[:,0], X0[:,1], color=\"blue\", alpha=0.5, label=\"Classe 0\")\nplt.scatter(X1[:,0], X1[:,1], color=\"red\", alpha=0.5, label=\"Classe 1\")\nplt.legend()\nplt.title(\"Dados gerados (com sobreposi\u00e7\u00e3o)\")\nplt.show()\n</code></pre>"},{"location":"exercicios/perceptron/main/#treinamento","title":"Treinamento","text":"<pre><code>perceptron = Perceptron(input_dim=2, learning_rate=0.01)\nperceptron.fit(X, y, epochs=100)\n\nprint(\"Pesos finais:\", perceptron.w) # Output: Pesos finais: [0.02156597 0.04316767]\nprint(\"Bias final:\", perceptron.b) # Output: Bias final: -0.03\n</code></pre> <pre><code>plt.figure(figsize=(6,4))\nplt.plot(perceptron.history, marker=\"o\")\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Acur\u00e1cia\")\nplt.title(\"Acur\u00e1cia durante o treino\")\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"exercicios/perceptron/main/#resultados_1","title":"Resultados","text":"<p>Os pesos finais encontrados foram aproximadamente [0.0216, 0.0432], com bias = -0.03, e a acur\u00e1cia final ficou em torno de 50%, praticamente equivalente ao acaso. Isso ocorre porque, diferentemente do Exercise 1, as distribui\u00e7\u00f5es de classe aqui t\u00eam m\u00e9dias muito pr\u00f3ximas e alta vari\u00e2ncia, gerando grande sobreposi\u00e7\u00e3o entre as amostras. Essa falta de separabilidade linear faz com que o perceptron oscile durante o treino, sem conseguir convergir para 100% de acur\u00e1cia, evidenciando sua limita\u00e7\u00e3o em problemas n\u00e3o linearmente separ\u00e1veis.</p>"},{"location":"projetos/ann-regression/","title":"ANN Regression","text":"In\u00a0[5]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n</pre> import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns  import tensorflow as tf from tensorflow.keras.utils import plot_model  from sklearn.preprocessing import MinMaxScaler from sklearn.model_selection import train_test_split, cross_val_score from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename)) In\u00a0[56]: Copied! <pre>df = pd.read_csv(\"data/Clean_Dataset.csv\")\ndf = df.drop(['Unnamed: 0'], axis = 1)\nprint(\"\\n================== HEAD DO DATASET ==================\\n\")\ndisplay(df.head())\nprint(\"\\n=====================================================\\n\")\nprint(\"\\n================== SHAPE DO DATASET ==================\\n\")\ndisplay(df.shape)\nprint(\"\\n=====================================================\\n\")\ndisplay(df.dtypes)\nprint(\"\\n================== VISUALIZANDO VALORES NULOS ==================\\n\")\nnull_summary = df.isnull().sum().to_frame().rename(columns={0: 'Null Count'})\nnull_summary['Percentage'] = (null_summary['Null Count'] / len(df)) * 100\ndisplay(null_summary)\nprint(\"\\n=====================================================\\n\")\n</pre> df = pd.read_csv(\"data/Clean_Dataset.csv\") df = df.drop(['Unnamed: 0'], axis = 1) print(\"\\n================== HEAD DO DATASET ==================\\n\") display(df.head()) print(\"\\n=====================================================\\n\") print(\"\\n================== SHAPE DO DATASET ==================\\n\") display(df.shape) print(\"\\n=====================================================\\n\") display(df.dtypes) print(\"\\n================== VISUALIZANDO VALORES NULOS ==================\\n\") null_summary = df.isnull().sum().to_frame().rename(columns={0: 'Null Count'}) null_summary['Percentage'] = (null_summary['Null Count'] / len(df)) * 100 display(null_summary) print(\"\\n=====================================================\\n\") <pre>================== HEAD DO DATASET ==================\n\n</pre> airline flight source_city departure_time stops arrival_time destination_city class duration days_left price 0 SpiceJet SG-8709 Delhi Evening zero Night Mumbai Economy 2.17 1 5953 1 SpiceJet SG-8157 Delhi Early_Morning zero Morning Mumbai Economy 2.33 1 5953 2 AirAsia I5-764 Delhi Early_Morning zero Early_Morning Mumbai Economy 2.17 1 5956 3 Vistara UK-995 Delhi Morning zero Afternoon Mumbai Economy 2.25 1 5955 4 Vistara UK-963 Delhi Morning zero Morning Mumbai Economy 2.33 1 5955 <pre>=====================================================\n\n\n================== SHAPE DO DATASET ==================\n\n</pre> <pre>(300153, 11)</pre> <pre>=====================================================\n\n</pre> <pre>airline              object\nflight               object\nsource_city          object\ndeparture_time       object\nstops                object\narrival_time         object\ndestination_city     object\nclass                object\nduration            float64\ndays_left             int64\nprice                 int64\ndtype: object</pre> <pre>================== VISUALIZANDO VALORES NULOS ==================\n\n</pre> Null Count Percentage airline 0 0.0 flight 0 0.0 source_city 0 0.0 departure_time 0 0.0 stops 0 0.0 arrival_time 0 0.0 destination_city 0 0.0 class 0 0.0 duration 0 0.0 days_left 0 0.0 price 0 0.0 <pre>=====================================================\n\n</pre> <p>Na primeira breve an\u00e1lise, o dataset n\u00e3o possui valores nulos, h\u00e1 v\u00e1rias vari\u00e1veis categ\u00f3ricas que precisam ser tratadas e o target \u00e9 num\u00e9rico cont\u00ednuo.</p> In\u00a0[19]: Copied! <pre># Todas poss\u00edveis vari\u00e1veis categ\u00f3ricas\ncat_features = [column for column in df.columns if df[column].dtype in [\"object\", \"category\", \"bool\"]]\n\n# Num\u00e9ricas mas na verdade s\u00e3o categ\u00f3ricas\nnum_but_cat_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (df[column].nunique() &lt; 9)]\n\n# Vari\u00e1veis categ\u00f3ricas\ncategoric_features = cat_features + num_but_cat_features\ncategoric_features = [column for column in categoric_features]\n\ndf[categoric_features]\n</pre> # Todas poss\u00edveis vari\u00e1veis categ\u00f3ricas cat_features = [column for column in df.columns if df[column].dtype in [\"object\", \"category\", \"bool\"]]  # Num\u00e9ricas mas na verdade s\u00e3o categ\u00f3ricas num_but_cat_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (df[column].nunique() &lt; 9)]  # Vari\u00e1veis categ\u00f3ricas categoric_features = cat_features + num_but_cat_features categoric_features = [column for column in categoric_features]  df[categoric_features] Out[19]: airline flight source_city departure_time stops arrival_time destination_city class 0 SpiceJet SG-8709 Delhi Evening zero Night Mumbai Economy 1 SpiceJet SG-8157 Delhi Early_Morning zero Morning Mumbai Economy 2 AirAsia I5-764 Delhi Early_Morning zero Early_Morning Mumbai Economy 3 Vistara UK-995 Delhi Morning zero Afternoon Mumbai Economy 4 Vistara UK-963 Delhi Morning zero Morning Mumbai Economy ... ... ... ... ... ... ... ... ... 300148 Vistara UK-822 Chennai Morning one Evening Hyderabad Business 300149 Vistara UK-826 Chennai Afternoon one Night Hyderabad Business 300150 Vistara UK-832 Chennai Early_Morning one Night Hyderabad Business 300151 Vistara UK-828 Chennai Early_Morning one Evening Hyderabad Business 300152 Vistara UK-822 Chennai Morning one Evening Hyderabad Business <p>300153 rows \u00d7 8 columns</p> In\u00a0[\u00a0]: Copied! <pre>sns.set_style(\"whitegrid\")\nsns.set_context(\"talk\") \npalette_colors = \"Set2\" \n\nplt.figure(figsize=(20,32))\n\nplt.subplot(4, 2, 1)\nsns.countplot(x=df[\"airline\"], data=df, legend=False)\nplt.title(\"Frequency of Airline\")\nsns.despine() \n\nplt.subplot(4, 2, 2)\nsns.countplot(x=df[\"source_city\"], data=df, palette=palette_colors, \n              hue=df[\"source_city\"], legend=False) \nplt.title(\"Frequency of Source City\")\nsns.despine() \n\nplt.subplot(4, 2, 3)\nsns.countplot(x=df[\"departure_time\"], data=df, palette=palette_colors, \n              hue=df[\"departure_time\"], legend=False)\nplt.title(\"Frequency of Departure Time\")\nsns.despine() \n\nplt.subplot(4, 2, 4)\nsns.countplot(x=df[\"stops\"], data=df, palette=\"deep\", \n              hue=df[\"stops\"], legend=False) \nplt.title(\"Frequency of Stops\")\nsns.despine() \n\nplt.subplot(4, 2, 5)\nsns.countplot(x=df[\"arrival_time\"], data=df, palette=palette_colors, \n              hue=df[\"arrival_time\"], legend=False)\nplt.title(\"Frequency of Arrival Time\")\nsns.despine() \n\nplt.subplot(4, 2, 6)\nsns.countplot(x=df[\"destination_city\"], data=df, palette=palette_colors, \n              hue=df[\"destination_city\"], legend=False)\nplt.title(\"Frequency of Destination City\")\nsns.despine() \n\nplt.subplot(4, 2, 7)\nsns.countplot(x=df[\"class\"], data=df, palette=\"pastel\", \n              hue=df[\"class\"], legend=False) \nplt.title(\"Class Frequency\")\nsns.despine() \n\nplt.tight_layout() \nplt.show()\n</pre> sns.set_style(\"whitegrid\") sns.set_context(\"talk\")  palette_colors = \"Set2\"   plt.figure(figsize=(20,32))  plt.subplot(4, 2, 1) sns.countplot(x=df[\"airline\"], data=df, legend=False) plt.title(\"Frequency of Airline\") sns.despine()   plt.subplot(4, 2, 2) sns.countplot(x=df[\"source_city\"], data=df, palette=palette_colors,                hue=df[\"source_city\"], legend=False)  plt.title(\"Frequency of Source City\") sns.despine()   plt.subplot(4, 2, 3) sns.countplot(x=df[\"departure_time\"], data=df, palette=palette_colors,                hue=df[\"departure_time\"], legend=False) plt.title(\"Frequency of Departure Time\") sns.despine()   plt.subplot(4, 2, 4) sns.countplot(x=df[\"stops\"], data=df, palette=\"deep\",                hue=df[\"stops\"], legend=False)  plt.title(\"Frequency of Stops\") sns.despine()   plt.subplot(4, 2, 5) sns.countplot(x=df[\"arrival_time\"], data=df, palette=palette_colors,                hue=df[\"arrival_time\"], legend=False) plt.title(\"Frequency of Arrival Time\") sns.despine()   plt.subplot(4, 2, 6) sns.countplot(x=df[\"destination_city\"], data=df, palette=palette_colors,                hue=df[\"destination_city\"], legend=False) plt.title(\"Frequency of Destination City\") sns.despine()   plt.subplot(4, 2, 7) sns.countplot(x=df[\"class\"], data=df, palette=\"pastel\",                hue=df[\"class\"], legend=False)  plt.title(\"Class Frequency\") sns.despine()   plt.tight_layout()  plt.show() In\u00a0[20]: Copied! <pre>numeric_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (column not in categoric_features)]\n\ndf[numeric_features]\n</pre> numeric_features = [column for column in df.columns if (df[column].dtype in [\"int64\", \"float64\"]) and (column not in categoric_features)]  df[numeric_features] Out[20]: duration days_left price 0 2.17 1 5953 1 2.33 1 5953 2 2.17 1 5956 3 2.25 1 5955 4 2.33 1 5955 ... ... ... ... 300148 10.08 49 69265 300149 10.42 49 77105 300150 13.83 49 79099 300151 10.00 49 81585 300152 10.08 49 81585 <p>300153 rows \u00d7 3 columns</p> In\u00a0[25]: Copied! <pre>plt.figure(figsize=(20,6))\n\nfor i, col in enumerate([\"duration\", \"days_left\", \"price\"], 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(df[col], kde=True, color=\"skyblue\", bins=30)\n    plt.title(f\"Distribui\u00e7\u00e3o de {col.capitalize()}\", fontsize=14)\n    plt.xlabel(col.capitalize())\n    plt.ylabel(\"Frequ\u00eancia\")\n\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(20,6))  for i, col in enumerate([\"duration\", \"days_left\", \"price\"], 1):     plt.subplot(1, 3, i)     sns.histplot(df[col], kde=True, color=\"skyblue\", bins=30)     plt.title(f\"Distribui\u00e7\u00e3o de {col.capitalize()}\", fontsize=14)     plt.xlabel(col.capitalize())     plt.ylabel(\"Frequ\u00eancia\")  plt.tight_layout() plt.show()  In\u00a0[30]: Copied! <pre>corr = df[\n    [c for c in numeric_features if not c.startswith('Unnamed')]\n].corr()\n\nplt.figure(figsize=(7, 6))\n\ncmap = sns.color_palette(\"vlag\", as_cmap=True) \n\nax = sns.heatmap(\n    corr,\n    cmap=cmap,\n    vmin=-1, vmax=1, center=0,       \n    annot=True, fmt=\".2f\",            \n    annot_kws={\"size\":8},\n    square=True,\n    linewidths=0.4, linecolor=\"white\",\n    cbar_kws={\"shrink\":0.8, \"label\":\"Pearson r\"},\n)\n\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.title(\"Matriz de Correla\u00e7\u00e3o (Pearson)\")\nplt.tight_layout()\nplt.show()\n</pre> corr = df[     [c for c in numeric_features if not c.startswith('Unnamed')] ].corr()  plt.figure(figsize=(7, 6))  cmap = sns.color_palette(\"vlag\", as_cmap=True)   ax = sns.heatmap(     corr,     cmap=cmap,     vmin=-1, vmax=1, center=0,            annot=True, fmt=\".2f\",                 annot_kws={\"size\":8},     square=True,     linewidths=0.4, linecolor=\"white\",     cbar_kws={\"shrink\":0.8, \"label\":\"Pearson r\"}, )  plt.xticks(rotation=45, ha=\"right\") plt.yticks(rotation=0) plt.title(\"Matriz de Correla\u00e7\u00e3o (Pearson)\") plt.tight_layout() plt.show() <p>Uma vez que \"stop\" e \"class\" s\u00e3o vari\u00e1veis orin\u00e1rias, o encoding tamb\u00e9m deve ser de acordo.</p> In\u00a0[31]: Copied! <pre>df[\"stops\"] = df[\"stops\"].replace({'zero': 0,\n                                   'one': 1,\n                                   'two_or_more': 2})\n\ndf[\"class\"] = df[\"class\"].replace({'Economy': 0,\n                                   'Business': 1})\n</pre> df[\"stops\"] = df[\"stops\"].replace({'zero': 0,                                    'one': 1,                                    'two_or_more': 2})  df[\"class\"] = df[\"class\"].replace({'Economy': 0,                                    'Business': 1}) <pre>/tmp/ipykernel_87733/3553766138.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"stops\"] = df[\"stops\"].replace({'zero': 0,\n/tmp/ipykernel_87733/3553766138.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"class\"] = df[\"class\"].replace({'Economy': 0,\n</pre> In\u00a0[\u00a0]: Copied! <pre># One Hot Encoding\ndummies_variables = [\"airline\", \"source_city\", \"departure_time\", \"arrival_time\", \"destination_city\"]\ndummies = pd.get_dummies(df[dummies_variables], drop_first= True)\n\ndf = pd.concat([df, dummies], axis=1)\ndf = df.drop(dummies_variables + ['flight'], axis=1) # Excluindo colunas antigas antes do one hot.\n\ndf.head()\n</pre> # One Hot Encoding dummies_variables = [\"airline\", \"source_city\", \"departure_time\", \"arrival_time\", \"destination_city\"] dummies = pd.get_dummies(df[dummies_variables], drop_first= True)  df = pd.concat([df, dummies], axis=1) df = df.drop(dummies_variables + ['flight'], axis=1) # Excluindo colunas antigas antes do one hot.  df.head() Out[\u00a0]: stops class duration days_left price airline_Air_India airline_GO_FIRST airline_Indigo airline_SpiceJet airline_Vistara ... arrival_time_Early_Morning arrival_time_Evening arrival_time_Late_Night arrival_time_Morning arrival_time_Night destination_city_Chennai destination_city_Delhi destination_city_Hyderabad destination_city_Kolkata destination_city_Mumbai 0 0 0 2.17 1 5953 False False False True False ... False False False False True False False False False True 1 0 0 2.33 1 5953 False False False True False ... False False False True False False False False False True 2 0 0 2.17 1 5956 False False False False False ... True False False False False False False False False True 3 0 0 2.25 1 5955 False False False False True ... False False False False False False False False False True 4 0 0 2.33 1 5955 False False False False True ... False False False True False False False False False True <p>5 rows \u00d7 30 columns</p> In\u00a0[33]: Copied! <pre>df.shape\n</pre> df.shape Out[33]: <pre>(300153, 30)</pre> <p>Ap\u00f3s o one hot, 18 novas featutes surgiram.</p> In\u00a0[34]: Copied! <pre>corr = df.corr(numeric_only=True)\nplt.figure(figsize=(35, 25))\nax = sns.heatmap(\n    corr,                      \n    annot=True, fmt=\".2f\",\n    vmin=-1.0, vmax=1.0, center=0,\n    cmap=\"vlag\",             \n    square=True,\n    linewidths=0.4, linecolor=\"white\",\n    cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"},\n)\n\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.title(\"Matriz de Correla\u00e7\u00e3o\")\nplt.tight_layout()\nplt.show()\n</pre> corr = df.corr(numeric_only=True) plt.figure(figsize=(35, 25)) ax = sns.heatmap(     corr,                           annot=True, fmt=\".2f\",     vmin=-1.0, vmax=1.0, center=0,     cmap=\"vlag\",                  square=True,     linewidths=0.4, linecolor=\"white\",     cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson r\"}, )  plt.xticks(rotation=45, ha=\"right\") plt.yticks(rotation=0) plt.title(\"Matriz de Correla\u00e7\u00e3o\") plt.tight_layout() plt.show() In\u00a0[37]: Copied! <pre>import numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nSEED = 42\n\ny = df['price']\nX = df.drop('price', axis=1)\n</pre> import numpy as np, pandas as pd from sklearn.model_selection import train_test_split SEED = 42  y = df['price'] X = df.drop('price', axis=1) In\u00a0[38]: Copied! <pre>X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.30, random_state=SEED)\nX_val,   X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=SEED)\n\nprint(f\"Train ({(len(X_train)/len(X)):.2f}): {len(X_train)} | Val ({len(X_val)/len(X):.2f}): {len(X_val)} | Test ({len(X_test)/len(X):.2f}): {len(X_test)}\")\n</pre> X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.30, random_state=SEED) X_val,   X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.50, random_state=SEED)  print(f\"Train ({(len(X_train)/len(X)):.2f}): {len(X_train)} | Val ({len(X_val)/len(X):.2f}): {len(X_val)} | Test ({len(X_test)/len(X):.2f}): {len(X_test)}\") <pre>Train (0.70): 210107 | Val (0.15): 45023 | Test (0.15): 45023\n</pre> In\u00a0[39]: Copied! <pre>X_train\n</pre> X_train Out[39]: stops class duration days_left airline_Air_India airline_GO_FIRST airline_Indigo airline_SpiceJet airline_Vistara source_city_Chennai ... arrival_time_Early_Morning arrival_time_Evening arrival_time_Late_Night arrival_time_Morning arrival_time_Night destination_city_Chennai destination_city_Delhi destination_city_Hyderabad destination_city_Kolkata destination_city_Mumbai 2406 1 0 13.42 14 False False False False True False ... False False False True False False False False False True 275865 1 1 9.58 23 False False False False True False ... False True False False False False True False False False 297156 1 1 11.17 29 False False False False True True ... False True False False False False False False True False 12826 1 0 5.08 16 False False False False True False ... False False False False True False False False False False 93166 1 0 12.58 45 False True False False False False ... False False False False True False True False False False ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 119879 1 0 20.50 2 False False False False True False ... False False False False False False True False False False 259178 1 1 25.42 7 False False False False True False ... False False False False True True False False False False 131932 1 0 13.67 29 True False False False False False ... False False False True False False False False False True 146867 1 0 8.33 39 False True False False False False ... False False False False True False False True False False 121958 1 0 20.17 17 True False False False False False ... False False False True False False True False False False <p>210107 rows \u00d7 29 columns</p> In\u00a0[40]: Copied! <pre># Scaling AP\u00d3S o split (sem tocar na vari\u00e1vel-alvo)\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nnum_vars = ['duration', 'days_left']  # \u2b05\ufe0f n\u00e3o inclui 'price'\n\n# Ajusta no treino e aplica em val/test (sem vazamento)\nX_train.loc[:, num_vars] = scaler.fit_transform(X_train[num_vars])\nX_val.loc[:,   num_vars] = scaler.transform(X_val[num_vars])\nX_test.loc[:,  num_vars] = scaler.transform(X_test[num_vars])\n</pre> # Scaling AP\u00d3S o split (sem tocar na vari\u00e1vel-alvo) from sklearn.preprocessing import MinMaxScaler  scaler = MinMaxScaler() num_vars = ['duration', 'days_left']  # \u2b05\ufe0f n\u00e3o inclui 'price'  # Ajusta no treino e aplica em val/test (sem vazamento) X_train.loc[:, num_vars] = scaler.fit_transform(X_train[num_vars]) X_val.loc[:,   num_vars] = scaler.transform(X_val[num_vars]) X_test.loc[:,  num_vars] = scaler.transform(X_test[num_vars]) <pre>/tmp/ipykernel_87733/2227656102.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.27083333 0.45833333 0.58333333 ... 0.58333333 0.79166667 0.33333333]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X_train.loc[:, num_vars] = scaler.fit_transform(X_train[num_vars])\n/tmp/ipykernel_87733/2227656102.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.20833333 0.875      0.27083333 ... 1.         1.         0.47916667]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X_val.loc[:,   num_vars] = scaler.transform(X_val[num_vars])\n/tmp/ipykernel_87733/2227656102.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.0625     0.5625     0.77083333 ... 0.25       0.10416667 0.64583333]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X_test.loc[:,  num_vars] = scaler.transform(X_test[num_vars])\n</pre> <p>A rede neural artificial (ANN) implementada realiza uma tarefa de regress\u00e3o supervisionada, com o objetivo de prever um valor cont\u00ednuo (no caso, o pre\u00e7o da passagem a\u00e9rea) a partir de 30 vari\u00e1veis de entrada previamente processadas.</p> <p>O modelo foi constru\u00eddo do zero, utilizando apenas NumPy, sem o uso de frameworks de alto n\u00edvel como TensorFlow ou PyTorch, o que permite controle total sobre o fluxo de c\u00e1lculo e o aprendizado.</p> <p>A arquitetura definida segue o padr\u00e3o feedforward totalmente conectado (Multilayer Perceptron \u2013 MLP), composta por:</p> <ul> <li>Camada de entrada: 30 neur\u00f4nios (uma para cada feature num\u00e9rica e categ\u00f3rica codificada).</li> <li>1\u00aa camada oculta: 32 neur\u00f4nios com fun\u00e7\u00e3o de ativa\u00e7\u00e3o ReLU (Rectified Linear Unit).</li> <li>2\u00aa camada oculta: 16 neur\u00f4nios, tamb\u00e9m com ativa\u00e7\u00e3o ReLU.</li> <li>Camada de sa\u00edda: 1 neur\u00f4nio com sa\u00edda linear, adequada para tarefas de regress\u00e3o (onde o valor previsto \u00e9 cont\u00ednuo).</li> </ul> In\u00a0[54]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom math import sqrt\n\nX_train_base = X_train   \nX_val_base = X_val   \nX_test_base = X_test\n\nX_train = np.asarray(X_train_base, dtype=float)\nX_val   = np.asarray(X_val_base,   dtype=float)\nX_test  = np.asarray(X_test_base,  dtype=float)\n\ny_train = np.asarray(y_train, dtype=float).ravel()\ny_val   = np.asarray(y_val,   dtype=float).ravel()\ny_test  = np.asarray(y_test,  dtype=float).ravel()\n\ny_train_col = y_train.reshape(-1, 1)\ny_val_col   = y_val.reshape(-1, 1)\n\nN, D = X_train.shape\n\n# 1) M\u00e9tricas de regress\u00e3o\ndef metrics(y_true, y_pred):\n    y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()\n    mae  = np.mean(np.abs(y_true - y_pred))\n    rmse = sqrt(np.mean((y_true - y_pred)**2))\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n    r2 = 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan\n    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n\n# 2) Arquitetura: ReLU 128\u219264\u21921 (linear)\nrng = np.random.default_rng(42)\nH1, H2 = 32, 16\n\nW1 = rng.standard_normal((H1, D)) * np.sqrt(2.0 / D)\nb1 = np.zeros((1, H1))\n\nW2 = rng.standard_normal((H2, H1)) * np.sqrt(2.0 / H1) \nb2 = np.zeros((1, H2))\n\nW3 = rng.standard_normal((1, H2)) * np.sqrt(2.0 / H2)\nb3 = np.zeros((1, 1))\n\ndef relu(x):    \n    return np.maximum(0, x)\n\ndef relu_d(x):  \n    return (x &gt; 0).astype(x.dtype)\n\ndef forward(X):\n    Z1 = X @ W1.T + b1; A1 = relu(Z1)\n    Z2 = A1 @ W2.T + b2; A2 = relu(Z2)\n    Y  = A2 @ W3.T + b3          # sa\u00edda linear (regress\u00e3o)\n    return Z1, A1, Z2, A2, Y\n\n# 3) Treino: Adam + MSE\nlr = 1e-3\nepochs = 100\nbatch  = 128\n\nbeta1, beta2, eps = 0.9, 0.999, 1e-8\n\n# Adam buffers\nmW1 = np.zeros_like(W1) \nvW1 = np.zeros_like(W1)\n\nmW2 = np.zeros_like(W2)\nvW2 = np.zeros_like(W2)\n\nmW3 = np.zeros_like(W3)\nvW3 = np.zeros_like(W3)\n\nmb1 = np.zeros_like(b1)\nvb1 = np.zeros_like(b1)\n\nmb2 = np.zeros_like(b2) \nvb2 = np.zeros_like(b2)\n\nmb3 = np.zeros_like(b3)\nvb3 = np.zeros_like(b3)\n\nt = 0\n\ndef adam_update(param, grad, m, v):\n    m = beta1*m + (1-beta1)*grad\n    v = beta2*v + (1-beta2)*(grad*grad)\n    m_hat = m / (1 - beta1**t)\n    v_hat = v / (1 - beta2**t)\n    param -= lr * m_hat / (np.sqrt(v_hat) + eps)\n    return param, m, v\n\ntrain_losses, val_losses = [], []\n\nfor ep in range(1, epochs+1):\n    idx = rng.permutation(N)\n    epoch_loss = 0.0\n\n    for i in range(0, N, batch):\n        bidx = idx[i:i+batch]\n        xb = X_train[bidx]\n        yb = y_train_col[bidx]\n        B  = xb.shape[0]\n\n        #  forward \n        Z1, A1, Z2, A2, Yp = forward(xb)\n\n        #  loss (MSE)\n        mse = np.mean((Yp - yb)**2)\n        epoch_loss += mse * B\n\n        #  backprop \n        d3  = (2.0/B) * (Yp - yb)     # (B,1)\n        gW3 = d3.T @ A2               # (1,H2)\n        gb3 = d3.sum(axis=0, keepdims=True)\n\n        dA2 = d3 @ W3                 # (B,H2)\n        dZ2 = dA2 * relu_d(Z2)\n        gW2 = dZ2.T @ A1              # (H2,H1)\n        gb2 = dZ2.sum(axis=0, keepdims=True)\n\n        dA1 = dZ2 @ W2                # (B,H1)\n        dZ1 = dA1 * relu_d(Z1)\n        gW1 = dZ1.T @ xb              # (H1,D)\n        gb1 = dZ1.sum(axis=0, keepdims=True)\n\n        #  Adam step (incrementa t uma vez por batch) \n        t += 1\n        W3, mW3, vW3 = adam_update(W3, gW3, mW3, vW3)\n        b3, mb3, vb3 = adam_update(b3, gb3, mb3, vb3)\n        W2, mW2, vW2 = adam_update(W2, gW2, mW2, vW2)\n        b2, mb2, vb2 = adam_update(b2, gb2, mb2, vb2)\n        W1, mW1, vW1 = adam_update(W1, gW1, mW1, vW1)\n        b1, mb1, vb1 = adam_update(b1, gb1, mb1, vb1)\n\n    epoch_loss /= N\n    train_losses.append(epoch_loss)\n\n    # valida\u00e7\u00e3o\n    _, _, _, _, Yv = forward(X_val)\n    val_mse = np.mean((Yv - y_val_col)**2)\n    val_losses.append(val_mse)\n\n    print(f\"\u00c9poca {ep:02d} | Train MSE {epoch_loss:.4f} | Val MSE {val_mse:.4f}\")\n\n# 4) Avalia\u00e7\u00e3o e gr\u00e1ficos\ndef predict(X):\n    _, _, _, _, Y = forward(X)\n    return Y.ravel()\n\npred_val  = predict(X_val)\npred_test = predict(X_test)\n\nprint(\"Val :\", metrics(y_val,  pred_val))\nprint(\"Test:\", metrics(y_test, pred_test))\n\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(train_losses, label=\"Train\")\nplt.plot(val_losses,   label=\"Val\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\"); plt.title(\"Curva de Loss\"); plt.legend()\n\nplt.subplot(1,2,2)\nplt.scatter(y_val, pred_val, s=8, alpha=0.4)\nlims = [min(y_val.min(), pred_val.min()), max(y_val.max(), pred_val.max())]\nplt.plot(lims, lims, \"--\")\nplt.xlabel(\"Verdadeiro\"); plt.ylabel(\"Previsto\"); plt.title(\"Val: y_true vs y_pred\")\nplt.tight_layout(); plt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from math import sqrt  X_train_base = X_train    X_val_base = X_val    X_test_base = X_test  X_train = np.asarray(X_train_base, dtype=float) X_val   = np.asarray(X_val_base,   dtype=float) X_test  = np.asarray(X_test_base,  dtype=float)  y_train = np.asarray(y_train, dtype=float).ravel() y_val   = np.asarray(y_val,   dtype=float).ravel() y_test  = np.asarray(y_test,  dtype=float).ravel()  y_train_col = y_train.reshape(-1, 1) y_val_col   = y_val.reshape(-1, 1)  N, D = X_train.shape  # 1) M\u00e9tricas de regress\u00e3o def metrics(y_true, y_pred):     y_true = np.asarray(y_true).ravel(); y_pred = np.asarray(y_pred).ravel()     mae  = np.mean(np.abs(y_true - y_pred))     rmse = sqrt(np.mean((y_true - y_pred)**2))     ss_res = np.sum((y_true - y_pred)**2)     ss_tot = np.sum((y_true - np.mean(y_true))**2)     r2 = 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan     return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}  # 2) Arquitetura: ReLU 128\u219264\u21921 (linear) rng = np.random.default_rng(42) H1, H2 = 32, 16  W1 = rng.standard_normal((H1, D)) * np.sqrt(2.0 / D) b1 = np.zeros((1, H1))  W2 = rng.standard_normal((H2, H1)) * np.sqrt(2.0 / H1)  b2 = np.zeros((1, H2))  W3 = rng.standard_normal((1, H2)) * np.sqrt(2.0 / H2) b3 = np.zeros((1, 1))  def relu(x):         return np.maximum(0, x)  def relu_d(x):       return (x &gt; 0).astype(x.dtype)  def forward(X):     Z1 = X @ W1.T + b1; A1 = relu(Z1)     Z2 = A1 @ W2.T + b2; A2 = relu(Z2)     Y  = A2 @ W3.T + b3          # sa\u00edda linear (regress\u00e3o)     return Z1, A1, Z2, A2, Y  # 3) Treino: Adam + MSE lr = 1e-3 epochs = 100 batch  = 128  beta1, beta2, eps = 0.9, 0.999, 1e-8  # Adam buffers mW1 = np.zeros_like(W1)  vW1 = np.zeros_like(W1)  mW2 = np.zeros_like(W2) vW2 = np.zeros_like(W2)  mW3 = np.zeros_like(W3) vW3 = np.zeros_like(W3)  mb1 = np.zeros_like(b1) vb1 = np.zeros_like(b1)  mb2 = np.zeros_like(b2)  vb2 = np.zeros_like(b2)  mb3 = np.zeros_like(b3) vb3 = np.zeros_like(b3)  t = 0  def adam_update(param, grad, m, v):     m = beta1*m + (1-beta1)*grad     v = beta2*v + (1-beta2)*(grad*grad)     m_hat = m / (1 - beta1**t)     v_hat = v / (1 - beta2**t)     param -= lr * m_hat / (np.sqrt(v_hat) + eps)     return param, m, v  train_losses, val_losses = [], []  for ep in range(1, epochs+1):     idx = rng.permutation(N)     epoch_loss = 0.0      for i in range(0, N, batch):         bidx = idx[i:i+batch]         xb = X_train[bidx]         yb = y_train_col[bidx]         B  = xb.shape[0]          #  forward          Z1, A1, Z2, A2, Yp = forward(xb)          #  loss (MSE)         mse = np.mean((Yp - yb)**2)         epoch_loss += mse * B          #  backprop          d3  = (2.0/B) * (Yp - yb)     # (B,1)         gW3 = d3.T @ A2               # (1,H2)         gb3 = d3.sum(axis=0, keepdims=True)          dA2 = d3 @ W3                 # (B,H2)         dZ2 = dA2 * relu_d(Z2)         gW2 = dZ2.T @ A1              # (H2,H1)         gb2 = dZ2.sum(axis=0, keepdims=True)          dA1 = dZ2 @ W2                # (B,H1)         dZ1 = dA1 * relu_d(Z1)         gW1 = dZ1.T @ xb              # (H1,D)         gb1 = dZ1.sum(axis=0, keepdims=True)          #  Adam step (incrementa t uma vez por batch)          t += 1         W3, mW3, vW3 = adam_update(W3, gW3, mW3, vW3)         b3, mb3, vb3 = adam_update(b3, gb3, mb3, vb3)         W2, mW2, vW2 = adam_update(W2, gW2, mW2, vW2)         b2, mb2, vb2 = adam_update(b2, gb2, mb2, vb2)         W1, mW1, vW1 = adam_update(W1, gW1, mW1, vW1)         b1, mb1, vb1 = adam_update(b1, gb1, mb1, vb1)      epoch_loss /= N     train_losses.append(epoch_loss)      # valida\u00e7\u00e3o     _, _, _, _, Yv = forward(X_val)     val_mse = np.mean((Yv - y_val_col)**2)     val_losses.append(val_mse)      print(f\"\u00c9poca {ep:02d} | Train MSE {epoch_loss:.4f} | Val MSE {val_mse:.4f}\")  # 4) Avalia\u00e7\u00e3o e gr\u00e1ficos def predict(X):     _, _, _, _, Y = forward(X)     return Y.ravel()  pred_val  = predict(X_val) pred_test = predict(X_test)  print(\"Val :\", metrics(y_val,  pred_val)) print(\"Test:\", metrics(y_test, pred_test))  plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.plot(train_losses, label=\"Train\") plt.plot(val_losses,   label=\"Val\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\"); plt.title(\"Curva de Loss\"); plt.legend()  plt.subplot(1,2,2) plt.scatter(y_val, pred_val, s=8, alpha=0.4) lims = [min(y_val.min(), pred_val.min()), max(y_val.max(), pred_val.max())] plt.plot(lims, lims, \"--\") plt.xlabel(\"Verdadeiro\"); plt.ylabel(\"Previsto\"); plt.title(\"Val: y_true vs y_pred\") plt.tight_layout(); plt.show() <pre>\u00c9poca 01 | Train MSE 678509857.4816 | Val MSE 398164542.4253\n\u00c9poca 02 | Train MSE 326570275.8006 | Val MSE 248977535.9151\n\u00c9poca 03 | Train MSE 167966263.4366 | Val MSE 95650901.6718\n\u00c9poca 04 | Train MSE 62077396.4720 | Val MSE 47291686.2173\n\u00c9poca 05 | Train MSE 42988600.4069 | Val MSE 42002038.3514\n\u00c9poca 06 | Train MSE 39708288.1543 | Val MSE 39712292.8742\n\u00c9poca 07 | Train MSE 37961861.7792 | Val MSE 38414171.9839\n\u00c9poca 08 | Train MSE 36873997.5994 | Val MSE 37462698.1271\n\u00c9poca 09 | Train MSE 36046750.1699 | Val MSE 36682944.0764\n\u00c9poca 10 | Train MSE 35317419.2662 | Val MSE 35993500.8448\n\u00c9poca 11 | Train MSE 34632413.1965 | Val MSE 35377141.7630\n\u00c9poca 12 | Train MSE 34014803.2772 | Val MSE 34735166.3910\n\u00c9poca 13 | Train MSE 33459533.5891 | Val MSE 34206057.7083\n\u00c9poca 14 | Train MSE 32954468.3794 | Val MSE 33734255.0602\n\u00c9poca 15 | Train MSE 32493218.0879 | Val MSE 33285006.0355\n\u00c9poca 16 | Train MSE 32073442.7710 | Val MSE 32837303.2367\n\u00c9poca 17 | Train MSE 31692858.1649 | Val MSE 32475898.6631\n\u00c9poca 18 | Train MSE 31347365.0347 | Val MSE 32104119.8431\n\u00c9poca 19 | Train MSE 31028605.0552 | Val MSE 31797864.3642\n\u00c9poca 20 | Train MSE 30761997.4167 | Val MSE 31528169.5052\n\u00c9poca 21 | Train MSE 30514767.7398 | Val MSE 31282686.8446\n\u00c9poca 22 | Train MSE 30295995.6337 | Val MSE 31077958.8826\n\u00c9poca 23 | Train MSE 30112897.3051 | Val MSE 30914770.8588\n\u00c9poca 24 | Train MSE 29949203.5508 | Val MSE 30743507.9177\n\u00c9poca 25 | Train MSE 29802686.8104 | Val MSE 30548239.1552\n\u00c9poca 26 | Train MSE 29661708.6134 | Val MSE 30393361.4386\n\u00c9poca 27 | Train MSE 29535435.6979 | Val MSE 30309425.7199\n\u00c9poca 28 | Train MSE 29417528.0130 | Val MSE 30166478.4950\n\u00c9poca 29 | Train MSE 29299797.7066 | Val MSE 30040308.5257\n\u00c9poca 30 | Train MSE 29200205.5227 | Val MSE 29921136.0605\n\u00c9poca 31 | Train MSE 29111662.9087 | Val MSE 29811439.1397\n\u00c9poca 32 | Train MSE 29003781.6697 | Val MSE 29766148.0476\n\u00c9poca 33 | Train MSE 28918996.7807 | Val MSE 29635477.4848\n\u00c9poca 34 | Train MSE 28827376.0748 | Val MSE 29520364.5602\n\u00c9poca 35 | Train MSE 28726392.2459 | Val MSE 29415309.2885\n\u00c9poca 36 | Train MSE 28626951.2461 | Val MSE 29322534.5992\n\u00c9poca 37 | Train MSE 28526149.2728 | Val MSE 29232215.9211\n\u00c9poca 38 | Train MSE 28406294.0774 | Val MSE 29120707.5696\n\u00c9poca 39 | Train MSE 28267459.0380 | Val MSE 28930305.1212\n\u00c9poca 40 | Train MSE 28096645.9363 | Val MSE 28755814.9254\n\u00c9poca 41 | Train MSE 27872789.8970 | Val MSE 28505155.5965\n\u00c9poca 42 | Train MSE 27645605.1708 | Val MSE 28317125.3604\n\u00c9poca 43 | Train MSE 27435152.8005 | Val MSE 28058055.6164\n\u00c9poca 44 | Train MSE 27210915.4433 | Val MSE 27826197.8575\n\u00c9poca 45 | Train MSE 26984133.8583 | Val MSE 27610014.3938\n\u00c9poca 46 | Train MSE 26746877.4561 | Val MSE 27391632.2307\n\u00c9poca 47 | Train MSE 26505597.2339 | Val MSE 27138844.3174\n\u00c9poca 48 | Train MSE 26260932.3331 | Val MSE 26874781.5938\n\u00c9poca 49 | Train MSE 25980388.0590 | Val MSE 26596627.9164\n\u00c9poca 50 | Train MSE 25732835.4208 | Val MSE 26380494.2580\n\u00c9poca 51 | Train MSE 25490131.6671 | Val MSE 26261886.9336\n\u00c9poca 52 | Train MSE 25283257.2011 | Val MSE 25974609.5864\n\u00c9poca 53 | Train MSE 25083967.0691 | Val MSE 25781837.3601\n\u00c9poca 54 | Train MSE 24887912.7925 | Val MSE 25893364.7678\n\u00c9poca 55 | Train MSE 24725293.9297 | Val MSE 25417175.5198\n\u00c9poca 56 | Train MSE 24559105.0121 | Val MSE 25283222.6262\n\u00c9poca 57 | Train MSE 24410687.5923 | Val MSE 25132808.3956\n\u00c9poca 58 | Train MSE 24256907.3456 | Val MSE 25098362.9468\n\u00c9poca 59 | Train MSE 24104936.0335 | Val MSE 24815227.1668\n\u00c9poca 60 | Train MSE 23962774.6408 | Val MSE 24700838.6018\n\u00c9poca 61 | Train MSE 23818127.3866 | Val MSE 24525977.2965\n\u00c9poca 62 | Train MSE 23688804.0406 | Val MSE 24414001.4563\n\u00c9poca 63 | Train MSE 23559978.4693 | Val MSE 24265398.2340\n\u00c9poca 64 | Train MSE 23443781.1897 | Val MSE 24157470.8105\n\u00c9poca 65 | Train MSE 23330519.8728 | Val MSE 24084077.5306\n\u00c9poca 66 | Train MSE 23224405.8256 | Val MSE 23958684.3995\n\u00c9poca 67 | Train MSE 23117822.4916 | Val MSE 23851343.9213\n\u00c9poca 68 | Train MSE 23032522.0461 | Val MSE 23773041.7690\n\u00c9poca 69 | Train MSE 22945403.2833 | Val MSE 23673327.5016\n\u00c9poca 70 | Train MSE 22862552.0517 | Val MSE 23580895.8501\n\u00c9poca 71 | Train MSE 22788407.7972 | Val MSE 23508502.2706\n\u00c9poca 72 | Train MSE 22716801.7800 | Val MSE 23448826.7116\n\u00c9poca 73 | Train MSE 22651504.6391 | Val MSE 23374096.7541\n\u00c9poca 74 | Train MSE 22543559.8271 | Val MSE 23260535.9626\n\u00c9poca 75 | Train MSE 22429042.1169 | Val MSE 23150273.1690\n\u00c9poca 76 | Train MSE 22327322.6257 | Val MSE 23034294.4389\n\u00c9poca 77 | Train MSE 22228108.5185 | Val MSE 22934799.4622\n\u00c9poca 78 | Train MSE 22140555.7481 | Val MSE 22883420.4062\n\u00c9poca 79 | Train MSE 22062846.1529 | Val MSE 22800066.4794\n\u00c9poca 80 | Train MSE 21978606.1706 | Val MSE 22687620.6776\n\u00c9poca 81 | Train MSE 21894684.1021 | Val MSE 22642557.3078\n\u00c9poca 82 | Train MSE 21820379.9311 | Val MSE 22552175.5755\n\u00c9poca 83 | Train MSE 21756795.1734 | Val MSE 22576422.2681\n\u00c9poca 84 | Train MSE 21688711.3699 | Val MSE 22432106.0638\n\u00c9poca 85 | Train MSE 21633667.4533 | Val MSE 22344762.2909\n\u00c9poca 86 | Train MSE 21584896.0175 | Val MSE 22404937.2086\n\u00c9poca 87 | Train MSE 21529884.7170 | Val MSE 22264439.5486\n\u00c9poca 88 | Train MSE 21469470.9937 | Val MSE 22222362.1503\n\u00c9poca 89 | Train MSE 21410579.3042 | Val MSE 22165393.1789\n\u00c9poca 90 | Train MSE 21354969.2609 | Val MSE 22095142.5159\n\u00c9poca 91 | Train MSE 21301860.7765 | Val MSE 22090995.9187\n\u00c9poca 92 | Train MSE 21259211.6110 | Val MSE 21968464.5830\n\u00c9poca 93 | Train MSE 21201740.6738 | Val MSE 21941721.5422\n\u00c9poca 94 | Train MSE 21162246.5955 | Val MSE 21897818.8100\n\u00c9poca 95 | Train MSE 21123956.3953 | Val MSE 21840419.5727\n\u00c9poca 96 | Train MSE 21081397.3961 | Val MSE 21812064.0865\n\u00c9poca 97 | Train MSE 21042992.2923 | Val MSE 21763704.7335\n\u00c9poca 98 | Train MSE 21012378.9608 | Val MSE 21809897.7625\n\u00c9poca 99 | Train MSE 20981444.4583 | Val MSE 21741157.0445\n\u00c9poca 100 | Train MSE 20943550.3786 | Val MSE 21706152.6864\nVal : {'MAE': np.float64(2719.284140828667), 'RMSE': 4658.986229469688, 'R2': np.float64(0.9581852820771005)}\nTest: {'MAE': np.float64(2702.177655808821), 'RMSE': 4554.172782841557, 'R2': np.float64(0.959349325120191)}\n</pre> In\u00a0[55]: Copied! <pre>from math import sqrt\nimport matplotlib.pyplot as plt\nplt.rcParams.update({\n    \"font.size\": 10,\n    \"axes.titlesize\": 12,\n    \"axes.labelsize\": 10,\n    \"xtick.labelsize\": 9,\n    \"ytick.labelsize\": 9,\n    \"legend.fontsize\": 9,\n    \"figure.titlesize\": 13\n})\n\n# Fun\u00e7\u00f5es auxiliares de m\u00e9tricas\ndef _to1d(a):\n    a = np.asarray(a, dtype=float)\n    return a.ravel() if a.ndim &gt; 1 else a\n\ndef mape(y_true, y_pred, eps=1e-8):\n    y_true = np.asarray(y_true, dtype=float)\n    denom = np.maximum(np.abs(y_true), eps)\n    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0\n\ndef smape(y_true, y_pred, eps=1e-8):\n    y_true = np.asarray(y_true, dtype=float)\n    y_pred = np.asarray(y_pred, dtype=float)\n    denom = np.maximum((np.abs(y_true) + np.abs(y_pred)) / 2, eps)\n    return np.mean(np.abs(y_true - y_pred) / denom) * 100.0\n\ndef r2_score_np(y_true, y_pred):\n    y_true = np.asarray(y_true, dtype=float)\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n    return 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan\n\n# Dados e previs\u00f5es\nX_val   = np.asarray(X_val, dtype=float)\nX_test  = np.asarray(X_test, dtype=float)\ny_train = _to1d(y_train)\ny_val   = _to1d(y_val)\ny_test  = _to1d(y_test)\n\ny_pred_val  = _to1d(predict(X_val))\ny_pred_test = _to1d(predict(X_test))\n\nbaseline_val  = np.full_like(y_val,  fill_value=float(np.mean(y_train)), dtype=float)\nbaseline_test = np.full_like(y_test, fill_value=float(np.mean(y_train)), dtype=float)\n\n# Fun\u00e7\u00e3o de m\u00e9tricas gerais\ndef print_metrics(title, y_true, y_pred, y_base):\n    mae  = np.mean(np.abs(y_true - y_pred))\n    mse  = np.mean((y_true - y_pred)**2)\n    rmse = sqrt(mse)\n    r2   = r2_score_np(y_true, y_pred)\n    mp   = mape(y_true, y_pred)\n    smp  = smape(y_true, y_pred)\n\n    mae_b  = np.mean(np.abs(y_true - y_base))\n    mse_b  = np.mean((y_true - y_base)**2)\n    rmse_b = sqrt(mse_b)\n    r2_b   = r2_score_np(y_true, y_base)\n    mp_b   = mape(y_true, y_base)\n    smp_b  = smape(y_true, y_base)\n\n    print(f\"\\n=== {title} ===\")\n    print(\"Metric           Model         Baseline(mean y_train)\")\n    print(f\"MAE      :   {mae:10.4f}   {mae_b:10.4f}\")\n    print(f\"MSE      :   {mse:10.4f}   {mse_b:10.4f}\")\n    print(f\"RMSE     :   {rmse:10.4f}   {rmse_b:10.4f}\")\n    print(f\"R2       :   {r2:10.4f}   {r2_b:10.4f}\")\n    print(f\"MAPE(%)  :   {mp:10.4f}   {mp_b:10.4f}\")\n    print(f\"sMAPE(%) :   {smp:10.4f}   {smp_b:10.4f}\")\n\nprint_metrics(\"Validation\", y_val,  y_pred_val,  baseline_val)\nprint_metrics(\"Test\",       y_test, y_pred_test, baseline_test)\n\n# ------------------------------------------------------------\ntrain_acc = 1 - np.array(train_losses) / np.max(train_losses)\nval_acc   = 1 - np.array(val_losses) / np.max(val_losses)\n\nss_tot_val = np.sum((y_val - np.mean(y_val))**2)\nr2_by_epoch = [1 - (mse * len(y_val)) / ss_tot_val for mse in val_losses]\n\nfinal_mse_test = np.mean((y_pred_test - y_test)**2)\ntest_acc = 1 - final_mse_test / np.max(train_losses)\n\nplt.figure(figsize=(15,4))\n\nplt.subplot(1,3,1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\")\nplt.title(\"Curva de Loss por \u00c9poca\")\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(train_acc, label=\"Train Accuracy (1 - Loss Normalizada)\")\nplt.plot(val_acc, label=\"Val Accuracy (1 - Loss Normalizada)\")\nplt.hlines(test_acc, 0, len(train_acc)-1, colors='orange', linestyles='--', label=f\"Test Accuracy ({test_acc:.3f})\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"Acur\u00e1cia (Normalizada)\")\nplt.title(\"Acur\u00e1cia por \u00c9poca (Train / Val / Test)\")\nplt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(r2_by_epoch, color='green', label=\"R\u00b2 (Val)\")\nplt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"R\u00b2\")\nplt.title(\"R\u00b2 por \u00c9poca (Valida\u00e7\u00e3o)\")\nplt.ylim(0, 1.05)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# 2) Gr\u00e1ficos de res\u00edduos e previs\u00e3o x real\nres_val = y_val - y_pred_val\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.scatter(y_pred_val, res_val, alpha=0.6)\nplt.axhline(0, linestyle=\"--\")\nplt.xlabel(\"Predicted (val)\"); plt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot (Val)\")\n\nplt.subplot(1,2,2)\nplt.scatter(y_val, y_pred_val, alpha=0.6)\nmn = float(min(np.min(y_val), np.min(y_pred_val)))\nmx = float(max(np.max(y_val), np.max(y_pred_val)))\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(\"Actual (y_val)\"); plt.ylabel(\"Predicted (val)\")\nplt.title(\"Actual vs Predicted (Val)\")\nplt.tight_layout()\nplt.show()\n\nres_test = y_test - y_pred_test\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.scatter(y_pred_test, res_test, alpha=0.6)\nplt.axhline(0, linestyle=\"--\")\nplt.xlabel(\"Predicted (test)\"); plt.ylabel(\"Residuals\")\nplt.title(\"Residual Plot (Test)\")\n\nplt.subplot(1,2,2)\nplt.scatter(y_test, y_pred_test, alpha=0.6)\nmn = float(min(np.min(y_test), np.min(y_pred_test)))\nmx = float(max(np.max(y_test), np.max(y_pred_test)))\nplt.plot([mn, mx], [mn, mx], linestyle=\"--\")\nplt.xlabel(\"Actual (y_test)\"); plt.ylabel(\"Predicted (test)\")\nplt.title(\"Actual vs Predicted (Test)\")\nplt.tight_layout()\nplt.show()\n</pre> from math import sqrt import matplotlib.pyplot as plt plt.rcParams.update({     \"font.size\": 10,     \"axes.titlesize\": 12,     \"axes.labelsize\": 10,     \"xtick.labelsize\": 9,     \"ytick.labelsize\": 9,     \"legend.fontsize\": 9,     \"figure.titlesize\": 13 })  # Fun\u00e7\u00f5es auxiliares de m\u00e9tricas def _to1d(a):     a = np.asarray(a, dtype=float)     return a.ravel() if a.ndim &gt; 1 else a  def mape(y_true, y_pred, eps=1e-8):     y_true = np.asarray(y_true, dtype=float)     denom = np.maximum(np.abs(y_true), eps)     return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0  def smape(y_true, y_pred, eps=1e-8):     y_true = np.asarray(y_true, dtype=float)     y_pred = np.asarray(y_pred, dtype=float)     denom = np.maximum((np.abs(y_true) + np.abs(y_pred)) / 2, eps)     return np.mean(np.abs(y_true - y_pred) / denom) * 100.0  def r2_score_np(y_true, y_pred):     y_true = np.asarray(y_true, dtype=float)     ss_res = np.sum((y_true - y_pred)**2)     ss_tot = np.sum((y_true - np.mean(y_true))**2)     return 1 - ss_res/ss_tot if ss_tot &gt; 0 else np.nan  # Dados e previs\u00f5es X_val   = np.asarray(X_val, dtype=float) X_test  = np.asarray(X_test, dtype=float) y_train = _to1d(y_train) y_val   = _to1d(y_val) y_test  = _to1d(y_test)  y_pred_val  = _to1d(predict(X_val)) y_pred_test = _to1d(predict(X_test))  baseline_val  = np.full_like(y_val,  fill_value=float(np.mean(y_train)), dtype=float) baseline_test = np.full_like(y_test, fill_value=float(np.mean(y_train)), dtype=float)  # Fun\u00e7\u00e3o de m\u00e9tricas gerais def print_metrics(title, y_true, y_pred, y_base):     mae  = np.mean(np.abs(y_true - y_pred))     mse  = np.mean((y_true - y_pred)**2)     rmse = sqrt(mse)     r2   = r2_score_np(y_true, y_pred)     mp   = mape(y_true, y_pred)     smp  = smape(y_true, y_pred)      mae_b  = np.mean(np.abs(y_true - y_base))     mse_b  = np.mean((y_true - y_base)**2)     rmse_b = sqrt(mse_b)     r2_b   = r2_score_np(y_true, y_base)     mp_b   = mape(y_true, y_base)     smp_b  = smape(y_true, y_base)      print(f\"\\n=== {title} ===\")     print(\"Metric           Model         Baseline(mean y_train)\")     print(f\"MAE      :   {mae:10.4f}   {mae_b:10.4f}\")     print(f\"MSE      :   {mse:10.4f}   {mse_b:10.4f}\")     print(f\"RMSE     :   {rmse:10.4f}   {rmse_b:10.4f}\")     print(f\"R2       :   {r2:10.4f}   {r2_b:10.4f}\")     print(f\"MAPE(%)  :   {mp:10.4f}   {mp_b:10.4f}\")     print(f\"sMAPE(%) :   {smp:10.4f}   {smp_b:10.4f}\")  print_metrics(\"Validation\", y_val,  y_pred_val,  baseline_val) print_metrics(\"Test\",       y_test, y_pred_test, baseline_test)  # ------------------------------------------------------------ train_acc = 1 - np.array(train_losses) / np.max(train_losses) val_acc   = 1 - np.array(val_losses) / np.max(val_losses)  ss_tot_val = np.sum((y_val - np.mean(y_val))**2) r2_by_epoch = [1 - (mse * len(y_val)) / ss_tot_val for mse in val_losses]  final_mse_test = np.mean((y_pred_test - y_test)**2) test_acc = 1 - final_mse_test / np.max(train_losses)  plt.figure(figsize=(15,4))  plt.subplot(1,3,1) plt.plot(train_losses, label=\"Train Loss\") plt.plot(val_losses, label=\"Val Loss\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"MSE\") plt.title(\"Curva de Loss por \u00c9poca\") plt.legend()  plt.subplot(1,3,2) plt.plot(train_acc, label=\"Train Accuracy (1 - Loss Normalizada)\") plt.plot(val_acc, label=\"Val Accuracy (1 - Loss Normalizada)\") plt.hlines(test_acc, 0, len(train_acc)-1, colors='orange', linestyles='--', label=f\"Test Accuracy ({test_acc:.3f})\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"Acur\u00e1cia (Normalizada)\") plt.title(\"Acur\u00e1cia por \u00c9poca (Train / Val / Test)\") plt.legend()  plt.subplot(1,3,3) plt.plot(r2_by_epoch, color='green', label=\"R\u00b2 (Val)\") plt.xlabel(\"\u00c9pocas\"); plt.ylabel(\"R\u00b2\") plt.title(\"R\u00b2 por \u00c9poca (Valida\u00e7\u00e3o)\") plt.ylim(0, 1.05) plt.legend()  plt.tight_layout() plt.show()  # 2) Gr\u00e1ficos de res\u00edduos e previs\u00e3o x real res_val = y_val - y_pred_val plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.scatter(y_pred_val, res_val, alpha=0.6) plt.axhline(0, linestyle=\"--\") plt.xlabel(\"Predicted (val)\"); plt.ylabel(\"Residuals\") plt.title(\"Residual Plot (Val)\")  plt.subplot(1,2,2) plt.scatter(y_val, y_pred_val, alpha=0.6) mn = float(min(np.min(y_val), np.min(y_pred_val))) mx = float(max(np.max(y_val), np.max(y_pred_val))) plt.plot([mn, mx], [mn, mx], linestyle=\"--\") plt.xlabel(\"Actual (y_val)\"); plt.ylabel(\"Predicted (val)\") plt.title(\"Actual vs Predicted (Val)\") plt.tight_layout() plt.show()  res_test = y_test - y_pred_test plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.scatter(y_pred_test, res_test, alpha=0.6) plt.axhline(0, linestyle=\"--\") plt.xlabel(\"Predicted (test)\"); plt.ylabel(\"Residuals\") plt.title(\"Residual Plot (Test)\")  plt.subplot(1,2,2) plt.scatter(y_test, y_pred_test, alpha=0.6) mn = float(min(np.min(y_test), np.min(y_pred_test))) mx = float(max(np.max(y_test), np.max(y_pred_test))) plt.plot([mn, mx], [mn, mx], linestyle=\"--\") plt.xlabel(\"Actual (y_test)\"); plt.ylabel(\"Predicted (test)\") plt.title(\"Actual vs Predicted (Test)\") plt.tight_layout() plt.show()  <pre>=== Validation ===\nMetric           Model         Baseline(mean y_train)\nMAE      :    2719.2841   19825.9131\nMSE      :   21706152.6864   519108303.5968\nRMSE     :    4658.9862   22783.9484\nR2       :       0.9582      -0.0000\nMAPE(%)  :      20.3035     238.0368\nsMAPE(%) :      18.8756     101.0205\n\n=== Test ===\nMetric           Model         Baseline(mean y_train)\nMAE      :    2702.1777   19702.5628\nMSE      :   20740489.7360   510225579.3527\nRMSE     :    4554.1728   22588.1734\nR2       :       0.9593      -0.0000\nMAPE(%)  :      20.5237     239.3193\nsMAPE(%) :      19.0806     100.9381\n</pre>"},{"location":"projetos/ann-regression/#ann-regression","title":"ANN Regression\u00b6","text":"<p>Autores: Lucas Lima, Henrique Badin e Eduardo Selber.</p> <p>Dataset Escolhido: Fligt Price Prediction.</p> <p>Notebook De Refer\u00eancia: Flight Price Prediction | EDA | Regression &amp; ANN</p>"},{"location":"projetos/ann-regression/#descricao-do-conjunto-de-dados","title":"Descri\u00e7\u00e3o do Conjunto de Dados\u00b6","text":"<p>O conjunto de dados cont\u00e9m informa\u00e7\u00f5es sobre passagens a\u00e9reas obtidas do site EaseMyTrip, com o objetivo de analisar fatores que influenciam o pre\u00e7o das passagens e treinar um modelo de regress\u00e3o linear para previs\u00e3o. Foram coletados 300.261 registros entre 11 de fevereiro e 31 de mar\u00e7o de 2022, abrangendo voos entre as seis principais cidades da \u00cdndia.</p>"},{"location":"projetos/ann-regression/#estrutura-do-dataset","title":"Estrutura do Dataset\u00b6","text":"<p>O conjunto possui 11 vari\u00e1veis, sendo 10 preditoras e 1 alvo (Price):</p> Vari\u00e1vel Tipo Descri\u00e7\u00e3o Airline Categ\u00f3rica Companhia a\u00e9rea (6 valores \u00fanicos) Flight Categ\u00f3rica C\u00f3digo do voo Source City Categ\u00f3rica Cidade de origem Departure Time Categ\u00f3rica Faixa de hor\u00e1rio de partida Stops Categ\u00f3rica N\u00famero de escalas Arrival Time Categ\u00f3rica Faixa de hor\u00e1rio de chegada Destination City Categ\u00f3rica Cidade de destino Class Categ\u00f3rica Classe da passagem (Economy ou Business) Duration Num\u00e9rica cont\u00ednua Dura\u00e7\u00e3o do voo em horas Days Left Num\u00e9rica discreta Dias entre a compra e a data do voo Price Num\u00e9rica cont\u00ednua Vari\u00e1vel alvo \u2013 pre\u00e7o da passagem"},{"location":"projetos/ann-regression/#tipos-de-variaveis","title":"Tipos de Vari\u00e1veis\u00b6","text":"<ul> <li>Categ\u00f3ricas nominais: Airline, Flight, Source City, Destination City, Class</li> <li>Categ\u00f3ricas ordinais: Departure Time, Arrival Time, Stops</li> <li>Num\u00e9ricas: Duration, Days Left, Price</li> </ul>"},{"location":"projetos/ann-regression/#dataset-observation","title":"Dataset Observation\u00b6","text":""},{"location":"projetos/ann-regression/#analise-das-variaveis-categoricas","title":"An\u00e1lise das vari\u00e1veis categ\u00f3ricas\u00b6","text":""},{"location":"projetos/ann-regression/#analise-das-variaveis-numericas","title":"An\u00e1lise das vari\u00e1veis num\u00e9ricas\u00b6","text":""},{"location":"projetos/ann-regression/#analise-de-correlacao-da-variavel-target","title":"An\u00e1lise de correla\u00e7\u00e3o da vari\u00e1vel target\u00b6","text":""},{"location":"projetos/ann-regression/#pre-processamento","title":"Pr\u00e9 Processamento\u00b6","text":""},{"location":"projetos/ann-regression/#separacao-do-conjunto-de-testes-e-treino","title":"Separa\u00e7\u00e3o do conjunto de testes e treino\u00b6","text":""},{"location":"projetos/ann-regression/#normalizacao-das-variaveis","title":"Normaliza\u00e7\u00e3o das vari\u00e1veis\u00b6","text":""},{"location":"projetos/ann-regression/#rede-neural-artificial","title":"Rede Neural Artificial\u00b6","text":""},{"location":"projetos/ann-regression/#processo-de-treinamento","title":"Processo de Treinamento\u00b6","text":"<p>O treinamento ocorre via gradiente descendente estoc\u00e1stico com o otimizador Adam, uma varia\u00e7\u00e3o adaptativa que combina momentum e RMSProp para ajustar dinamicamente a taxa de aprendizado.</p> <p>Os principais par\u00e2metros adotados foram:</p> <ul> <li>Taxa de aprendizado (<code>lr</code>): 0.001</li> <li>Tamanho do batch (<code>batch size</code>): 128 amostras por atualiza\u00e7\u00e3o de gradiente</li> <li>N\u00famero de \u00e9pocas: 100</li> <li>Fun\u00e7\u00e3o de custo: Erro Quadr\u00e1tico M\u00e9dio (MSE), apropriada para regress\u00e3o.</li> </ul> <p>Durante o treinamento:</p> <ol> <li>O conjunto de treino \u00e9 embaralhado e dividido em batches.</li> <li>Para cada batch, \u00e9 feita a propaga\u00e7\u00e3o direta (forward pass) para gerar previs\u00f5es.</li> <li>O erro \u00e9 calculado via MSE.</li> <li>O erro \u00e9 propagado para tr\u00e1s (backpropagation) para calcular os gradientes das camadas.</li> <li>Os pesos e vieses s\u00e3o atualizados conforme as regras do Adam.</li> </ol> <p>Ao final de cada \u00e9poca, o c\u00f3digo tamb\u00e9m calcula a loss de valida\u00e7\u00e3o, permitindo monitorar a generaliza\u00e7\u00e3o do modelo ao longo do tempo.</p>"},{"location":"projetos/ann-regression/#avaliacao-e-resultados","title":"Avalia\u00e7\u00e3o e Resultados\u00b6","text":"<p>Ap\u00f3s o treinamento, o modelo foi avaliado nos conjuntos de valida\u00e7\u00e3o e teste, com as m\u00e9tricas:</p> <ul> <li>MAE (Mean Absolute Error)</li> <li>RMSE (Root Mean Squared Error)</li> <li>R\u00b2 (Coeficiente de Determina\u00e7\u00e3o)</li> </ul> <p>Os resultados obtidos mostram um modelo com erro m\u00e9dio relativamente baixo e alto coeficiente de determina\u00e7\u00e3o (R\u00b2 \u2248 0.96), indicando que a rede aprendeu bem as rela\u00e7\u00f5es entre as vari\u00e1veis de entrada e o pre\u00e7o.</p> <p>Al\u00e9m disso, foram gerados dois gr\u00e1ficos principais:</p> <ul> <li>Curva de loss (MSE) para treino e valida\u00e7\u00e3o, evidenciando a converg\u00eancia do modelo.</li> <li>Dispers\u00e3o entre valores reais e previstos (y_true vs y_pred), mostrando boa correla\u00e7\u00e3o e baixa dispers\u00e3o residual.</li> </ul>"},{"location":"projetos/ann-regression/#visualizcao-dos-resultados","title":"Visualiz\u00e7\u00e3o dos resultados\u00b6","text":""},{"location":"projetos/ann-regression/#conclusoes","title":"Conclus\u00f5es\u00b6","text":"<p>Foram analisadas as curvas de treinamento, as m\u00e9tricas de regress\u00e3o e os gr\u00e1ficos de desempenho do modelo de rede neural aplicado \u00e0 predi\u00e7\u00e3o do pre\u00e7o de passagens a\u00e9reas.</p> <p>Durante o treinamento, as curvas de loss (MSE) de treino e valida\u00e7\u00e3o apresentaram queda acentuada nas primeiras \u00e9pocas e estabiliza\u00e7\u00e3o progressiva a partir da 80\u00aa \u00e9poca, indicando converg\u00eancia e aus\u00eancia de overfitting. As curvas de treino e valida\u00e7\u00e3o mant\u00eam-se pr\u00f3ximas, o que evidencia boa generaliza\u00e7\u00e3o do modelo. O gr\u00e1fico de R\u00b2 por \u00e9poca confirma essa estabilidade, atingindo valores pr\u00f3ximos de 0,96, o que mostra que o modelo explica cerca de 96% da variabilidade dos pre\u00e7os.</p> <p>Os gr\u00e1ficos de res\u00edduos (Residual Plots) e Actual vs Predicted indicam uma forte correla\u00e7\u00e3o entre valores reais e previstos, com os pontos bem distribu\u00eddos ao longo da linha de refer\u00eancia. H\u00e1, contudo, ligeiro aumento da dispers\u00e3o para pre\u00e7os mais altos, o que sugere heterocedasticidade \u2014 isto \u00e9, erros ligeiramente maiores em passagens mais caras, um comportamento esperado em dados econ\u00f4micos.</p> <p>As m\u00e9tricas de regress\u00e3o confirmam a boa performance do modelo em rela\u00e7\u00e3o ao baseline (previs\u00e3o pela m\u00e9dia do treino):</p> M\u00e9trica Valida\u00e7\u00e3o (Modelo / Baseline) Teste (Modelo / Baseline) MAE 2.719 / 19.826 2.702 / 19.703 MSE 21.706.152 / 519.108.303 20.740.489 / 510.225.579 RMSE 4.659 / 22.783 4.554 / 22.588 R\u00b2 0.958 0.959 MAPE (%) 20.30 / 238.03 20.52 / 239.31 sMAPE (%) 18.88 / 101.02 19.08 / 100.94 <p>Os resultados mostram que a rede neural supera amplamente o baseline, apresentando erros absolutos e percentuais significativamente menores e um R\u00b2 elevado, o que evidencia um modelo robusto e bem ajustado.</p> <p>Em s\u00edntese:</p> <ul> <li>O modelo apresenta excelente capacidade preditiva e estabilidade, com erro m\u00e9dio (MAE) em torno de 2.700 unidades monet\u00e1rias e erro percentual pr\u00f3ximo de 20%.</li> <li>As curvas e o R\u00b2 mostram converg\u00eancia est\u00e1vel e boa generaliza\u00e7\u00e3o.</li> <li>Pequenos desvios para valores altos de pre\u00e7o indicam espa\u00e7o para aprimoramentos, como aplicar transforma\u00e7\u00f5es no alvo (<code>log(price)</code>) ou empregar fun\u00e7\u00f5es de perda mais robustas (MAE ou Huber).</li> <li>O desempenho geral demonstra que a arquitetura da rede foi adequada para capturar padr\u00f5es complexos no comportamento dos pre\u00e7os de passagens a\u00e9reas.</li> </ul> <p>OBS: Fizemos uma m\u00e9trica de acur\u00e1cia baseada no MSE, esta m\u00e9trica foi criada simplesmente pois na p\u00e1gina da disciplina, que descreve o projeto, pede um gr\u00e1fico de acur\u00e1cia por \u00e9poca.</p>"},{"location":"projetos/generative/1_documented/","title":"Projeto \u2014 ComfyUI / Stable Diffusion (documentado)","text":"In\u00a0[\u00a0]: Copied! <pre>import subprocess\nimport os\nfrom os import path\nimport warnings\nfrom IPython.display import clear_output\nwarnings.filterwarnings('ignore')\nfrom multiprocessing import Process\nimport sys\nimport time\n\nBASE = os.getcwd()\n\ndef download_from_civitai(URL_CITVITAI, PASTA_ONDE_DEVE_SER_INSTALADA, NOME_DO_ARQUIVO):\n    tok = COLE_SEU_TOKEB_CTIVITAI_AQUI\n    model_url = f\"{url}&amp;token={tok}\"\n    save_path = f\"{BASE}/ComfyUI/models/{folder}/{name}\"\n    get_ipython().system(f'wget -O \"{save_path}\" \"{model_url}\"')\n\n\ndef delete_model(PASTA, NOME):\n    %cd $BASE/ComfyUI/models/$folder\n    all_names = os.listdir()\n    print(\"O caminho atual \u00e9: \", os.getcwd())\n    print(\"Todos os arquivos no diret\u00f3rio atual: \", all_names)\n    \n    if name in all_names:\n        %rm $name\n        print(\"O ARQUIVO DO MODELO FOI EXCLU\u00cdDO COM SUCESSO\")\n    else:\n        print(\"O ARQUIVO DO MODELO FORNECIDO N\u00c3O FOI ENCONTRADO\")\n\n%cd $BASE\n!git clone https://github.com/comfyanonymous/ComfyUI.git\n%cd ComfyUI\nget_ipython().system('git pull')\n!pip install - torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu124\n!pip install -r requirements.txt\n!pip install --upgrade gguf\n!pip install bitsandbytes&gt;=0.43.0\n!mamba install openssh -y\n \n%cd $BASE/ComfyUI/custom_nodes\n!git clone https://github.com/ltdrdata/ComfyUI-Manager.git\n%cd ComfyUI-Manager\nget_ipython().system('git pull')\n!pip install -r requirements.txt\n\n%cd $BASE/ComfyUI/custom_nodes\n!git clone https://github.com/chrisgoringe/cg-use-everywhere.git  \n!git clone https://github.com/pythongosssss/ComfyUI-Custom-Scripts.git  \n!git clone https://github.com/WASasquatch/was-node-suite-comfyui.git  \n!git clone https://github.com/rgthree/rgthree-comfy.git  \n!git clone https://github.com/city96/ComfyUI-GGUF  \n!git clone https://github.com/crystian/ComfyUI-Crystools.git  \n%cd ComfyUI-Crystools\n!pip install -r requirements.txt\n!git clone https://github.com/kijai/ComfyUI-KJNodes.git\n!git clone https://github.com/11cafe/comfyui-workspace-manager.git\n%cd /kaggle/working/ComfyUI/custom_nodes\n!pip install -r requirements.txt\n\n\nclear_output(wait=True)\n%cd $BASE/ComfyUI\n\nprint(f\"Current path: {BASE}/ComfyUI\")\nprint(\"COMFYUI MANAGER + CUSTOM NODES FORAM INSTALADOS COM SUCESSO!\")\n</pre> import subprocess import os from os import path import warnings from IPython.display import clear_output warnings.filterwarnings('ignore') from multiprocessing import Process import sys import time  BASE = os.getcwd()  def download_from_civitai(URL_CITVITAI, PASTA_ONDE_DEVE_SER_INSTALADA, NOME_DO_ARQUIVO):     tok = COLE_SEU_TOKEB_CTIVITAI_AQUI     model_url = f\"{url}&amp;token={tok}\"     save_path = f\"{BASE}/ComfyUI/models/{folder}/{name}\"     get_ipython().system(f'wget -O \"{save_path}\" \"{model_url}\"')   def delete_model(PASTA, NOME):     %cd $BASE/ComfyUI/models/$folder     all_names = os.listdir()     print(\"O caminho atual \u00e9: \", os.getcwd())     print(\"Todos os arquivos no diret\u00f3rio atual: \", all_names)          if name in all_names:         %rm $name         print(\"O ARQUIVO DO MODELO FOI EXCLU\u00cdDO COM SUCESSO\")     else:         print(\"O ARQUIVO DO MODELO FORNECIDO N\u00c3O FOI ENCONTRADO\")  %cd $BASE !git clone https://github.com/comfyanonymous/ComfyUI.git %cd ComfyUI get_ipython().system('git pull') !pip install - torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu124 !pip install -r requirements.txt !pip install --upgrade gguf !pip install bitsandbytes&gt;=0.43.0 !mamba install openssh -y   %cd $BASE/ComfyUI/custom_nodes !git clone https://github.com/ltdrdata/ComfyUI-Manager.git %cd ComfyUI-Manager get_ipython().system('git pull') !pip install -r requirements.txt  %cd $BASE/ComfyUI/custom_nodes !git clone https://github.com/chrisgoringe/cg-use-everywhere.git   !git clone https://github.com/pythongosssss/ComfyUI-Custom-Scripts.git   !git clone https://github.com/WASasquatch/was-node-suite-comfyui.git   !git clone https://github.com/rgthree/rgthree-comfy.git   !git clone https://github.com/city96/ComfyUI-GGUF   !git clone https://github.com/crystian/ComfyUI-Crystools.git   %cd ComfyUI-Crystools !pip install -r requirements.txt !git clone https://github.com/kijai/ComfyUI-KJNodes.git !git clone https://github.com/11cafe/comfyui-workspace-manager.git %cd /kaggle/working/ComfyUI/custom_nodes !pip install -r requirements.txt   clear_output(wait=True) %cd $BASE/ComfyUI  print(f\"Current path: {BASE}/ComfyUI\") print(\"COMFYUI MANAGER + CUSTOM NODES FORAM INSTALADOS COM SUCESSO!\") In\u00a0[\u00a0]: Copied! <pre>### Dreamshaper_8\nmodel_url = \"https://civitai.com/api/download/models/128713?type=Model&amp;format=SafeTensor&amp;size=pruned&amp;fp=fp16\"\nmodel_name = \"dreamshaper_8.safetensors\"\n%cd /kaggle/working/ComfyUI/models/checkpoints\nget_ipython().system(f'wget -O \"{model_name}\" \"{model_url}\"')\n\n### blindbox_V1Mix\nmodel_url = \"https://civitai.com/api/download/models/32988?type=Model&amp;format=SafeTensor&amp;size=full&amp;fp=fp16\"\nmodel_name = \"blindbox_V1Mix.safetensors\"\n%cd /kaggle/working/ComfyUI/models/loras\nget_ipython().system(f'wget -O \"{model_name}\" \"{model_url}\"')\n\n%cd /kaggle/working/ComfyUI\nprint(\"FLUX + CLIP + VAE FORAM INSTALADOS COM SUCESSO! \ud83d\udc4d\")\n</pre> ### Dreamshaper_8 model_url = \"https://civitai.com/api/download/models/128713?type=Model&amp;format=SafeTensor&amp;size=pruned&amp;fp=fp16\" model_name = \"dreamshaper_8.safetensors\" %cd /kaggle/working/ComfyUI/models/checkpoints get_ipython().system(f'wget -O \"{model_name}\" \"{model_url}\"')  ### blindbox_V1Mix model_url = \"https://civitai.com/api/download/models/32988?type=Model&amp;format=SafeTensor&amp;size=full&amp;fp=fp16\" model_name = \"blindbox_V1Mix.safetensors\" %cd /kaggle/working/ComfyUI/models/loras get_ipython().system(f'wget -O \"{model_name}\" \"{model_url}\"')  %cd /kaggle/working/ComfyUI print(\"FLUX + CLIP + VAE FORAM INSTALADOS COM SUCESSO! \ud83d\udc4d\") In\u00a0[\u00a0]: Copied! <pre>%cd $BASE/ComfyUI\n\n!touch log.txt\nopen('log.txt', 'w').close()\nclear_output(wait=True)\n\ndef run_app():\n    cmd = f\"python {BASE}/ComfyUI/main.py &amp; ssh -o StrictHostKeyChecking=no -p 80 -R0:localhost:8188 a.pinggy.io &gt; log.txt\"\n    get_ipython().system(cmd)\n    \ndef print_url():\n    print(\"waiting for output\")\n    time.sleep(2)\n    sys.stdout.flush()\n    \n    found = False\n    with open('log.txt', 'r') as file:\n        end_word = '.pinggy.link'\n        for line in file:\n            start_index = line.find(\"http:\")\n            if start_index != -1:\n                end_index = line.find(end_word, start_index)\n                if end_index != -1:\n                    print(\"\ud83d\ude01 \ud83d\ude01 \ud83d\ude01\")\n                    print(\"URL: \" + line[start_index:end_index + len(end_word)])\n                    print(\"\ud83d\ude01 \ud83d\ude01 \ud83d\ude01\")\n                    found = True\n    if not found:\n        print_url()\n    else:\n        with open('log.txt', 'r') as file:\n            for line in file:\n                print(line)\n    \np_app = Process(target=run_app)\np_url = Process(target=print_url)\np_app.start()\np_url.start()\np_app.join()\np_url.join()\n</pre> %cd $BASE/ComfyUI  !touch log.txt open('log.txt', 'w').close() clear_output(wait=True)  def run_app():     cmd = f\"python {BASE}/ComfyUI/main.py &amp; ssh -o StrictHostKeyChecking=no -p 80 -R0:localhost:8188 a.pinggy.io &gt; log.txt\"     get_ipython().system(cmd)      def print_url():     print(\"waiting for output\")     time.sleep(2)     sys.stdout.flush()          found = False     with open('log.txt', 'r') as file:         end_word = '.pinggy.link'         for line in file:             start_index = line.find(\"http:\")             if start_index != -1:                 end_index = line.find(end_word, start_index)                 if end_index != -1:                     print(\"\ud83d\ude01 \ud83d\ude01 \ud83d\ude01\")                     print(\"URL: \" + line[start_index:end_index + len(end_word)])                     print(\"\ud83d\ude01 \ud83d\ude01 \ud83d\ude01\")                     found = True     if not found:         print_url()     else:         with open('log.txt', 'r') as file:             for line in file:                 print(line)      p_app = Process(target=run_app) p_url = Process(target=print_url) p_app.start() p_url.start() p_app.join() p_url.join() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"projetos/generative/1_documented/#projeto-comfyui-stable-diffusion-documentado","title":"Projeto \u2014 ComfyUI / Stable Diffusion (documentado)\u00b6","text":""},{"location":"projetos/generative/1_documented/#objetivo-do-projeto","title":"Objetivo do projeto\u00b6","text":"<p>Este notebook implementa um pipeline que usa Stable Diffusion via ComfyUI (ou downloads de checkpoints) para gerar imagens. O objetivo desta documenta\u00e7\u00e3o \u00e9 atender aos requisitos do trabalho: descrever a arquitetura dos modelos, explicar os blocos usados no fluxo, e fornecer instru\u00e7\u00f5es para reproducibilidade e execu\u00e7\u00e3o.</p> <p>Crit\u00e9rios importantes (resumido):</p> <ul> <li>Explicar arquitetura e blocos do pipeline (entrada \u2192 codificadores CLIP \u2192 difus\u00e3o \u2192 VAE \u2192 sa\u00edda).</li> <li>Incluir exemplos de entrada/sa\u00edda e par\u00e2metros usados.</li> <li>Mencionar depend\u00eancias e instru\u00e7\u00f5es para executar o notebook/ComfyUI localmente.</li> <li>Opcional: uso de LoRA, ControlNet, etc. (adicionar se aplic\u00e1vel).</li> </ul>"},{"location":"projetos/generative/1_documented/#sumario-rapido-do-que-ha-neste-notebook","title":"Sum\u00e1rio r\u00e1pido do que h\u00e1 neste notebook\u00b6","text":"<ul> <li>Prepara\u00e7\u00e3o do ambiente (vari\u00e1veis, paths).</li> <li>Download autom\u00e1tico de checkpoints/VAE/CLIP.</li> <li>Inicializa\u00e7\u00e3o / controle do ComfyUI (scripts para checar logs / iniciar servidor).</li> </ul>"},{"location":"projetos/generative/1_documented/#como-usar-este-notebook-instrucoes-rapidas","title":"Como usar este notebook (instru\u00e7\u00f5es r\u00e1pidas)\u00b6","text":"<ol> <li>Coloque este notebook na mesma pasta onde est\u00e1 a instala\u00e7\u00e3o do ComfyUI ou ajuste a vari\u00e1vel <code>BASE</code> nas c\u00e9lulas para apontar para o diret\u00f3rio correto.</li> <li>Garanta que o ambiente virtual com PyTorch + CUDA est\u00e1 ativado (ou rode o ComfyUI com <code>--lowvram --force-fp16</code> se necess\u00e1rio).</li> <li>Execute as c\u00e9lulas na ordem, lendo os coment\u00e1rios e observando mensagens de log.</li> <li>Se houver downloads grandes (checkpoints ~5\u201310 GB), verifique espa\u00e7o em disco e integridade do arquivo (.safetensors completo).</li> </ol>"},{"location":"projetos/generative/1_documented/#arquitetura-descricao-generica-aplicada-ao-notebook","title":"Arquitetura (descri\u00e7\u00e3o gen\u00e9rica aplicada ao notebook)\u00b6","text":"<p>Um pipeline de Text\u2192Image t\u00edpico cont\u00e9m os seguintes blocos (e mapeamento para o que o notebook / ComfyUI faz):</p> <ul> <li>Prompt / Texto de entrada \u2192 alimenta o text encoder (CLIP / text encoder do modelo).</li> <li>Text encoder (CLIP) \u2192 produz embeddings condicionais para o modelo de difus\u00e3o.</li> <li>U-Net / Diffusion Model \u2192 gera latentes condicionados pelos embeddings.</li> <li>VAE (Decoder) \u2192 converte latentes para imagens RGB finais.</li> <li>(Opcional) LoRA / Hypernetwork \u2192 adapta pesos finos para estilos ou personagens espec\u00edficos.</li> <li>(Opcional) ControlNet / Conditioning \u2192 adiciona condicionamento estrutural (esbo\u00e7o, canny, pose).</li> <li>Output \u2192 upscale / save / export (imagem final).</li> </ul>"},{"location":"projetos/generative/1_documented/#comfyui-manager-nodes","title":"COMFYUI MANAGER + NODES\u00b6","text":""},{"location":"projetos/generative/1_documented/#explicacao-da-celula-de-codigo","title":"Explica\u00e7\u00e3o da c\u00e9lula de c\u00f3digo\u00b6","text":"<p>Objetivo: Baixar/checkpoints/Modelos (checkpoint, VAE, CLIP). Esta c\u00e9lula realiza o download de arquivos de modelos e salva em <code>models/checkpoints/</code>. Verifique espa\u00e7o em disco e integridade do arquivo ap\u00f3s o download.</p> <p>Observa\u00e7\u00f5es para execu\u00e7\u00e3o:</p> <ul> <li>Execute em ordem.</li> <li>Se a c\u00e9lula faz download, confirme o arquivo final em <code>models/checkpoints/</code>.</li> <li>Se houver falhas de VRAM, considere reiniciar ComfyUI com <code>--lowvram --force-fp16</code>.</li> </ul>"},{"location":"projetos/generative/1_documented/#models-clip-vae","title":"MODELS + CLIP + VAE\u00b6","text":""},{"location":"projetos/generative/1_documented/#explicacao-da-celula-de-codigo","title":"Explica\u00e7\u00e3o da c\u00e9lula de c\u00f3digo\u00b6","text":"<p>Objetivo: Baixar/checkpoints/Modelos (checkpoint, VAE, CLIP). Esta c\u00e9lula realiza o download de arquivos de modelos e salva em <code>models/checkpoints/</code>. Verifique espa\u00e7o em disco e integridade do arquivo ap\u00f3s o download.</p> <p>Observa\u00e7\u00f5es para execu\u00e7\u00e3o:</p> <ul> <li>Execute em ordem.</li> <li>Se a c\u00e9lula faz download, confirme o arquivo final em <code>models/checkpoints/</code>.</li> <li>Se houver falhas de VRAM, considere reiniciar ComfyUI com <code>--lowvram --force-fp16</code>.</li> </ul>"},{"location":"projetos/generative/1_documented/#start-comfyui","title":"START COMFYUI\u00b6","text":""},{"location":"projetos/generative/1_documented/#explicacao-da-celula-de-codigo","title":"Explica\u00e7\u00e3o da c\u00e9lula de c\u00f3digo\u00b6","text":"<p>Objetivo: Iniciar ou controlar o ComfyUI localmente. Pode criar/abrir logs, iniciar o servidor e monitorar a sa\u00edda.</p> <p>Observa\u00e7\u00f5es para execu\u00e7\u00e3o:</p> <ul> <li>Execute em ordem.</li> <li>Se a c\u00e9lula faz download, confirme o arquivo final em <code>models/checkpoints/</code>.</li> <li>Se houver falhas de VRAM, considere reiniciar ComfyUI com <code>--lowvram --force-fp16</code>.</li> </ul>"},{"location":"projetos/generative/1_documented/#explicacao-da-celula-de-codigo","title":"Explica\u00e7\u00e3o da c\u00e9lula de c\u00f3digo\u00b6","text":"<p>Objetivo: C\u00e9lula de execu\u00e7\u00e3o do pipeline. Revise o c\u00f3digo para entender opera\u00e7\u00f5es espec\u00edficas.</p> <p>Observa\u00e7\u00f5es para execu\u00e7\u00e3o:</p> <ul> <li>Execute em ordem.</li> <li>Se a c\u00e9lula faz download, confirme o arquivo final em <code>models/checkpoints/</code>.</li> <li>Se houver falhas de VRAM, considere reiniciar ComfyUI com <code>--lowvram --force-fp16</code>.</li> </ul>"},{"location":"projetos/generative/main/","title":"Projeto: Generative","text":""},{"location":"projetos/generative/main/#projeto-generative","title":"Projeto: Generative\u00b6","text":"<p>Autores: Lucas Lima, Henrique Badin e Eduardo Selber.</p> <p>Resumo: Este notebook apresenta o relat\u00f3rio do projeto que explora um pipeline de gera\u00e7\u00e3o de imagens usando Stable Diffusion (sugerido via ComfyUI). O foco est\u00e1 em explicar a arquitetura, apresentar experimentos (5 exemplos input\u2192output), discutir resultados e cumprir os crit\u00e9rios da avalia\u00e7\u00e3o.</p>"},{"location":"projetos/generative/main/#1-introducao","title":"1. Introdu\u00e7\u00e3o\u00b6","text":"<p>Este projeto investiga t\u00e9cnicas de gera\u00e7\u00e3o de imagens condicionadas por texto usando Stable Diffusion. O objetivo principal \u00e9 construir e documentar um pipeline (Text \u2192 Image), explicar a arquitetura empregada, fornecer exemplos de entrada e sa\u00edda e discutir os resultados de forma clara e objetiva, seguindo a rubrica do trabalho.</p>"},{"location":"projetos/generative/main/#2-arquitetura-do-pipeline-de-geracao","title":"2. Arquitetura do Pipeline de Gera\u00e7\u00e3o\u00b6","text":"<p>O pipeline completo \u00e9 composto por duas etapas principais: a gera\u00e7\u00e3o da imagem 2D e a reconstru\u00e7\u00e3o tridimensional dessa imagem. A seguir, descrevem-se os componentes envolvidos em cada fase.</p>"},{"location":"projetos/generative/main/#21-arquitetura-do-pipeline-de-geracao-2d","title":"2.1 Arquitetura do Pipeline de Gera\u00e7\u00e3o 2d\u00b6","text":"<p>O pipeline \u00e9 composto pelos seguintes componentes:</p> <ol> <li><p>Load Checkpoint Carrega o modelo base, o CLIP e o VAE utilizados na gera\u00e7\u00e3o.</p> </li> <li><p>Load LoRAs Aplica sequencialmente as LoRAs ao modelo principal, ajustando tanto o modelo como o CLIP.</p> </li> <li><p>CLIP Text Encode (Prompt Positivo e Negativo) Converte os textos dos prompts em embeddings que guiam o modelo durante a gera\u00e7\u00e3o.</p> </li> <li><p>Empty Latent Image Cria o espa\u00e7o latente inicial onde a imagem ser\u00e1 sintetizada.</p> </li> <li><p>KSampler Efetua o processo de difus\u00e3o, refinando o latent at\u00e9 formar o conceito requisitado.</p> </li> <li><p>VAE Decode Converte o latent final em uma imagem RGB.</p> </li> <li><p>Save Image Salva a imagem final no disco.</p> </li> </ol>"},{"location":"projetos/generative/main/#22-pipeline-de-reconstrucao-3d","title":"2.2. Pipeline de Reconstru\u00e7\u00e3o 3D\u00b6","text":"<p>Ap\u00f3s a gera\u00e7\u00e3o da imagem 2D, o pipeline segue para a etapa de reconstru\u00e7\u00e3o tridimensional utilizando o modelo Hunyuan3D-V2.</p> <ol> <li><p>Image-Only Checkpoint Loader (Hunyuan3D) Carrega o modelo especializado em reconstru\u00e7\u00e3o 3D, incluindo o encoder visual e o VAE pr\u00f3prio desse m\u00f3dulo.</p> </li> <li><p>CLIP Vision Encode Extrai embeddings visuais da imagem 2D gerada, capturando seus atributos de forma robusta para orientar o processo 3D.</p> </li> <li><p>Hunyuan3Dv2 Conditioning Converte os embeddings visuais em condicionamentos positivos e negativos adequados para a difus\u00e3o volum\u00e9trica.</p> </li> <li><p>Empty Latent Hunyuan3Dv2 Cria o espa\u00e7o latente tridimensional inicial, representado em voxels, que servir\u00e1 como estrutura base para a reconstru\u00e7\u00e3o.</p> </li> <li><p>KSampler (3D) Executa o processo de difus\u00e3o 3D, refinando progressivamente o volume latente at\u00e9 formar a geometria correspondente \u00e0 imagem 2D.</p> </li> <li><p>VAE Decode Hunyuan3D Decodifica o latent volum\u00e9trico, produzindo uma representa\u00e7\u00e3o voxelizada do modelo tridimensional.</p> </li> <li><p>VoxelToMesh Converte o volume voxelizado em uma malha poligonal utilizando algoritmos de reconstru\u00e7\u00e3o de superf\u00edcie.</p> </li> <li><p>SaveGLB Exporta a malha final no formato GLB, possibilitando visualiza\u00e7\u00e3o, manipula\u00e7\u00e3o e uso do modelo 3D em softwares externos.</p> </li> </ol>"},{"location":"projetos/generative/main/#codio-de-implementacao","title":"C\u00f3dio de implementa\u00e7\u00e3o\u00b6","text":"<pre>import subprocess\nimport os\nfrom os import path\nimport warnings\nfrom IPython.display import clear_output\nwarnings.filterwarnings('ignore')\nfrom multiprocessing import Process\nimport sys\nimport time\n\nBASE = os.getcwd()\n\ndef download_from_civitai(URL_CITVITAI, PASTA_ONDE_DEVE_SER_INSTALADA, NOME_DO_ARQUIVO):\n    tok = COLE_SEU_TOKEB_CTIVITAI_AQUI\n    model_url = f\"{url}&amp;token={tok}\"\n    save_path = f\"{BASE}/ComfyUI/models/{folder}/{name}\"\n    get_ipython().system(f'wget -O \"{save_path}\" \"{model_url}\"')\n\n\ndef delete_model(PASTA, NOME):\n    %cd $BASE/ComfyUI/models/$folder\n    all_names = os.listdir()\n    print(\"O caminho atual \u00e9: \", os.getcwd())\n    print(\"Todos os arquivos no diret\u00f3rio atual: \", all_names)\n    \n    if name in all_names:\n        %rm $name\n        print(\"O ARQUIVO DO MODELO FOI EXCLU\u00cdDO COM SUCESSO\")\n    else:\n        print(\"O ARQUIVO DO MODELO FORNECIDO N\u00c3O FOI ENCONTRADO\")\n\n%cd $BASE\n!git clone https://github.com/comfyanonymous/ComfyUI.git\n%cd ComfyUI\nget_ipython().system('git pull')\n!pip install - torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu124\n!pip install -r requirements.txt\n!pip install --upgrade gguf\n!pip install bitsandbytes&gt;=0.43.0\n!mamba install openssh -y\n \n%cd $BASE/ComfyUI/custom_nodes\n!git clone https://github.com/ltdrdata/ComfyUI-Manager.git\n%cd ComfyUI-Manager\nget_ipython().system('git pull')\n!pip install -r requirements.txt\n\n%cd $BASE/ComfyUI/custom_nodes\n!git clone https://github.com/chrisgoringe/cg-use-everywhere.git  \n!git clone https://github.com/pythongosssss/ComfyUI-Custom-Scripts.git  \n!git clone https://github.com/WASasquatch/was-node-suite-comfyui.git  \n!git clone https://github.com/rgthree/rgthree-comfy.git  \n!git clone https://github.com/city96/ComfyUI-GGUF  \n!git clone https://github.com/crystian/ComfyUI-Crystools.git  \n%cd ComfyUI-Crystools\n!pip install -r requirements.txt\n!git clone https://github.com/kijai/ComfyUI-KJNodes.git\n!git clone https://github.com/11cafe/comfyui-workspace-manager.git\n%cd /kaggle/working/ComfyUI/custom_nodes\n!pip install -r requirements.txt\n\n\nclear_output(wait=True)\n%cd $BASE/ComfyUI\n\nprint(f\"Current path: {BASE}/ComfyUI\")\nprint(\"COMFYUI MANAGER + CUSTOM NODES FORAM INSTALADOS COM SUCESSO!\ud83d\udc4d\")\n\n### Dreamshaper_8\nmodel_url = \"https://civitai.com/api/download/models/128713?type=Model&amp;format=SafeTensor&amp;size=pruned&amp;fp=fp16\"\nmodel_name = \"dreamshaper_8.safetensors\"\n%cd /kaggle/working/ComfyUI/models/checkpoints\nget_ipython().system(f'wget -O \"{model_name}\" \"{model_url}\"')\n\n### blindbox_V1Mix\nmodel_url = \"https://civitai.com/api/download/models/32988?type=Model&amp;format=SafeTensor&amp;size=full&amp;fp=fp16\"\nmodel_name = \"blindbox_V1Mix.safetensors\"\n%cd /kaggle/working/ComfyUI/models/loras\nget_ipython().system(f'wget -O \"{model_name}\" \"{model_url}\"')\n\n### MoXinV1\nmodel_url = \"https://civitai.com/api/download/models/14856?type=Model&amp;format=SafeTensor&amp;size=full&amp;fp=fp16\"\nmodel_name = \"MoXinV1.safetensors\"\n%cd /kaggle/working/ComfyUI/models/loras\nget_ipython().system(f'wget -O \"{model_name}\" \"{model_url}\"')\n\n### hunyuan3d-dit-v2-mv\nmodel_url = \"https://huggingface.co/Comfy-Org/hunyuan3D_2.1_repackaged/resolve/main/hunyuan_3d_v2.1.safetensors\"\nmodel_name = \"hunyuan_3d_v2.1.safetensors\"\n%cd /kaggle/working/ComfyUI/models/checkpoints\nget_ipython().system(f'wget -O \"{model_name}\" \"{model_url}\"')\n\n%cd /kaggle/working/ComfyUI\nprint(\"LoRA Template e WAN Template FORAM INSTALADOS COM SUCESSO! \ud83d\udc4d\")\n\n%cd $BASE/ComfyUI\n\n!touch log.txt\nopen('log.txt', 'w').close()\nclear_output(wait=True)\n\ndef run_app():\n    cmd = f\"python {BASE}/ComfyUI/main.py &amp; ssh -o StrictHostKeyChecking=no -p 80 -R0:localhost:8188 a.pinggy.io &gt; log.txt\"\n    get_ipython().system(cmd)\n    \ndef print_url():\n    print(\"waiting for output\")\n    time.sleep(2)\n    sys.stdout.flush()\n    \n    found = False\n    with open('log.txt', 'r') as file:\n        end_word = '.pinggy.link'\n        for line in file:\n            start_index = line.find(\"http:\")\n            if start_index != -1:\n                end_index = line.find(end_word, start_index)\n                if end_index != -1:\n                    print(\"#######\")\n                    print(\"URL: \" + line[start_index:end_index + len(end_word)])\n                    print(\"#######\")\n                    found = True\n    if not found:\n        print_url()\n    else:\n        with open('log.txt', 'r') as file:\n            for line in file:\n                print(line)\n    \np_app = Process(target=run_app)\np_url = Process(target=print_url)\np_app.start()\np_url.start()\np_app.join()\np_url.join()\n</pre>"},{"location":"projetos/generative/main/#4-metodologia-experimental-como-os-experimentos-foram-planejados","title":"4. Metodologia experimental (como os experimentos foram planejados)\u00b6","text":"<ul> <li><p>Estruturamos os testes em dois \u00e2mbitos principais de varia\u00e7\u00e3o de par\u00e2metros:</p> <ol> <li><p>n\u00famero de samples</p> </li> <li><p>valor de denoise do K-Sampler em cada workflow.</p> </li> </ol> </li> <li><p>Na primeira etapa, realizamos experimentos apenas no workflow com LoRA, variando samples e denoise diretamente nesse fluxo, mantendo o workflow de gera\u00e7\u00e3o 3D inalterado.</p> </li> <li><p>Na segunda etapa, mantivemos o workflow do LoRA fixo (com a melhor configura\u00e7\u00e3o identificada na etapa anterior) e passamos a variar os mesmos par\u00e2metros (samples e denoise) no workflow de gera\u00e7\u00e3o de imagem utilizado pelo Image-to-3D.</p> </li> <li><p>Para ambos os workflows (LoRA e Image3D), testamos sistematicamente os seguintes valores de par\u00e2metros:</p> <ul> <li>Samples: 10, 30, 50</li> <li>Denoise: 0.7, 0.85, 1.0</li> </ul> </li> <li><p>Em todos os testes, mantivemos os demais par\u00e2metros dos n\u00f3s (modelo base, LoRA selecionado, prompts e resolu\u00e7\u00e3o) constantes, para isolar o impacto espec\u00edfico da varia\u00e7\u00e3o de samples e denoise na qualidade visual das imagens e na gera\u00e7\u00e3o dos modelos 3D.</p> </li> </ul>"},{"location":"projetos/generative/main/#testes-fazendo-alteracoes-no-workflow-do-lora","title":"Testes fazendo altera\u00e7\u00f5es no workflow do LoRA:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-07-e-steps-30","title":"Denoise = 0.7 e Steps = 30:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-085-e-steps-30","title":"Denoise = 0.85 e Steps = 30:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-10-e-steps-30","title":"Denoise = 1.0 e Steps = 30:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-10-e-steps-50","title":"Denoise = 1.0 e Steps = 50 :\u00b6","text":""},{"location":"projetos/generative/main/#denoise-10-e-steps-10","title":"Denoise = 1.0 e Steps 10:\u00b6","text":""},{"location":"projetos/generative/main/#testes-fazendo-alteracoes-no-workflow-do-image-to-3d","title":"Testes fazendo altera\u00e7\u00f5es no workflow do Image-to-3D:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-07-e-steps-30","title":"Denoise = 0.7 e Steps = 30:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-085-steps-30","title":"Denoise = 0.85 Steps = 30:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-10-e-steps-30","title":"Denoise = 1.0 e Steps = 30:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-10-e-steps-50","title":"Denoise = 1.0 e Steps = 50:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-10-e-steps-10","title":"Denoise = 1.0 e Steps 10:\u00b6","text":""},{"location":"projetos/generative/main/#conclusao-dos-testes","title":"Conclus\u00e3o dos testes\u00b6","text":""},{"location":"projetos/generative/main/#a-partir-das-variacoes-de-parametros-da-para-tirar-alguns-aprendizados-bem-claros","title":"A partir das varia\u00e7\u00f5es de par\u00e2metros, d\u00e1 para tirar alguns aprendizados bem claros:\u00b6","text":""},{"location":"projetos/generative/main/#denoise-07-e-085","title":"Denoise 0.7 e 0.85\u00b6","text":"<p>Com esses valores mais baixos, as imagens perderam for\u00e7a. Os detalhes ficam pobres, partes como m\u00e3os e a arma aparecem deformadas e a composi\u00e7\u00e3o como um todo parece menos fiel ao que o prompt pedia. \u00c9 como se a imagem n\u00e3o tivesse maturado o suficiente.</p>"},{"location":"projetos/generative/main/#denoise-10","title":"Denoise 1.0\u00b6","text":"<p>Nesse n\u00edvel, o modelo realmente \u201cfecha\u201d a ideia. As imagens ficam mais consistentes, com estrutura mais clara e coerente, incluindo como ,por exeplo,  pose, roupa, arma, tudo se encaixa melhor. Mesmo sendo gera\u00e7\u00e3o de IA, o resultado transmite muito mais estabilidade e proximidade de algo plaus\u00edvel.</p>"},{"location":"projetos/generative/main/#steps-10-30-e-50-com-denoise-10","title":"Steps 10, 30 e 50 (com denoise 1.0)\u00b6","text":"<p>Com 10 steps , a imagem parece interrompida no meio; v\u00e1rios trechos saem distorcidos. Com 30 steps , o resultado j\u00e1 \u00e9 s\u00f3lido e utiliz\u00e1vel. Com 50 steps, h\u00e1 um refinamento extra: o rosto melhora, pequenos artefatos desaparecem e a imagem ganha um acabamento mais profissional.</p> <p>Em resumo, para esse conjunto de testes, a combina\u00e7\u00e3o denoise 1.0 com 30 a 50 steps entregou os resultados mais completos, est\u00e1veis e coerentes,  tanto para uso direto em 2D quanto pensando no pipeline para 3D depois.</p>"},{"location":"projetos/mlp-classification/","title":"MLP Classification","text":""},{"location":"projetos/mlp-classification/#classificacao-com-mlp","title":"Classifica\u00e7\u00e3o com MLP","text":""},{"location":"projetos/mlp-classification/#grupo","title":"Grupo","text":"<ol> <li>Eduardo Selber Castanho</li> <li>Henrique Fazzio Badin</li> <li>Lucas Fernando de Souza Lima</li> </ol>"},{"location":"projetos/mlp-classification/#1-dataset-overview","title":"1. Dataset Overview","text":"<p>Dataset: Predict Students' Dropout and Academic Success Fonte: UCI Machine Learning Repository Dom\u00ednio: Educa\u00e7\u00e3o / Ci\u00eancias Sociais Tipo de dados: Tabular (num\u00e9ricos, categ\u00f3ricos e inteiros) Tarefa: Classifica\u00e7\u00e3o multiclasse (3 classes: dropout, enrolled, graduate) Amostras: 4.424 Features: 36 Descri\u00e7\u00e3o: Dados de estudantes de gradua\u00e7\u00e3o em diversas \u00e1reas, contendo informa\u00e7\u00f5es demogr\u00e1ficas, socioecon\u00f4micas e de desempenho acad\u00eamico nos dois primeiros semestres. O objetivo \u00e9 prever a situa\u00e7\u00e3o final do aluno \u2014 evas\u00e3o, matr\u00edcula ativa ou gradua\u00e7\u00e3o.  </p> <p>Motiva\u00e7\u00e3o: O Predict Students' Dropout and Academic Success foi escolhido por ser tamb\u00e9m utilizado em uma competi\u00e7\u00e3o no Kaggle, o que refor\u00e7a sua relev\u00e2ncia e permite compara\u00e7\u00f5es de desempenho entre diferentes abordagens de classifica\u00e7\u00e3o.  </p> <p>Al\u00e9m disso, trata-se de um tema socialmente importante, pois os resultados obtidos podem ser aplicados em escolas, universidades e institui\u00e7\u00f5es educacionais para identificar precocemente alunos com risco de evas\u00e3o e orientar estrat\u00e9gias de apoio acad\u00eamico.  </p> <p>O dataset \u00e9 robusto (mais de 4.000 inst\u00e2ncias e 36 vari\u00e1veis) e, ao mesmo tempo, bem estruturado, com dados limpos e sem valores ausentes ou duplicados, al\u00e9m de descri\u00e7\u00f5es claras das features, o que facilita o desenvolvimento e an\u00e1lise de modelos de aprendizado de m\u00e1quina.  </p>"},{"location":"projetos/mlp-classification/#2-dataset-explanation","title":"2. Dataset Explanation","text":"<p>O conjunto de dados Predict Students\u2019 Dropout and Academic Success cont\u00e9m informa\u00e7\u00f5es acad\u00eamicas, demogr\u00e1ficas e socioecon\u00f4micas de estudantes de cursos de gradua\u00e7\u00e3o, coletadas no momento da matr\u00edcula e ao final dos dois primeiros semestres. O objetivo \u00e9 prever o status final do aluno \u2014 dropout, enrolled ou graduate \u2014 configurando um problema de classifica\u00e7\u00e3o multiclasse.  </p> <p>O dataset possui 36 vari\u00e1veis e 4.424 inst\u00e2ncias, todas sem valores nulos ou duplicados. As features incluem dados pessoais, hist\u00f3rico acad\u00eamico, desempenho em disciplinas e indicadores macroecon\u00f4micos.  </p>"},{"location":"projetos/mlp-classification/#21-feature-description","title":"2.1 Feature Description","text":"Feature Tipo Descri\u00e7\u00e3o Valores / Intervalos Marital Status Integer Estado civil 1\u2013single, 2\u2013married, 3\u2013widower, 4\u2013divorced, 5\u2013facto union, 6\u2013legally separated Application mode Integer Tipo de candidatura ao curso 1\u2013general, 5\u2013special contingent, 17\u20132nd phase, etc. Application order Integer Ordem de prefer\u00eancia da candidatura 0\u20139 Course Integer Curso matriculado V\u00e1rios c\u00f3digos (ex.: 9147\u2013Management, 9500\u2013Nursing) Daytime/evening attendance Integer Turno 1\u2013daytime, 0\u2013evening Previous qualification Integer N\u00edvel de escolaridade anterior 1\u2013secondary, 2\u2013higher ed., etc. Previous qualification (grade) Continuous Nota da qualifica\u00e7\u00e3o anterior 0\u2013200 Nationality Integer Nacionalidade Ex.: 1\u2013Portuguese, 41\u2013Brazilian, 21\u2013Angolan Mother\u2019s qualification Integer Escolaridade da m\u00e3e 1\u2013secondary, 5\u2013doctorate, etc. Father\u2019s qualification Integer Escolaridade do pai 1\u2013secondary, 5\u2013doctorate, etc. Mother\u2019s occupation Integer Ocupa\u00e7\u00e3o da m\u00e3e 0\u2013student, 1\u2013manager, 5\u2013service worker, etc. Father\u2019s occupation Integer Ocupa\u00e7\u00e3o do pai 0\u2013student, 1\u2013manager, 8\u2013technician, etc. Admission grade Continuous Nota de admiss\u00e3o no curso 0\u2013200 Displaced Integer Mora fora da cidade de origem 1\u2013yes, 0\u2013no Educational special needs Integer Necessidades educacionais especiais 1\u2013yes, 0\u2013no Debtor Integer Est\u00e1 em d\u00e9bito financeiro 1\u2013yes, 0\u2013no Tuition fees up to date Integer Mensalidades em dia 1\u2013yes, 0\u2013no Gender Integer Sexo do estudante 1\u2013male, 0\u2013female Scholarship holder Integer Possui bolsa de estudos 1\u2013yes, 0\u2013no Age at enrollment Integer Idade no momento da matr\u00edcula anos International Integer Estudante internacional 1\u2013yes, 0\u2013no Curricular units 1st sem (credited) Integer Disciplinas creditadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (enrolled) Integer Disciplinas matriculadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (evaluations) Integer Avalia\u00e7\u00f5es realizadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (approved) Integer Disciplinas aprovadas (1\u00ba semestre) 0\u2013n Curricular units 1st sem (grade) Integer M\u00e9dia de notas (1\u00ba semestre) 0\u201320 Curricular units 1st sem (without evaluations) Integer Disciplinas sem avalia\u00e7\u00e3o (1\u00ba semestre) 0\u2013n Curricular units 2nd sem (credited) Integer Disciplinas creditadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (enrolled) Integer Disciplinas matriculadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (evaluations) Integer Avalia\u00e7\u00f5es realizadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (approved) Integer Disciplinas aprovadas (2\u00ba semestre) 0\u2013n Curricular units 2nd sem (grade) Integer M\u00e9dia de notas (2\u00ba semestre) 0\u201320 Curricular units 2nd sem (without evaluations) Integer Disciplinas sem avalia\u00e7\u00e3o (2\u00ba semestre) 0\u2013n Unemployment rate Continuous Taxa de desemprego (%) valor cont\u00ednuo Inflation rate Continuous Taxa de infla\u00e7\u00e3o (%) valor cont\u00ednuo GDP Continuous Produto Interno Bruto valor cont\u00ednuo Target Categorical Situa\u00e7\u00e3o final do aluno Dropout, Enrolled, Graduate"},{"location":"projetos/mlp-classification/#22-target-variable","title":"2.2 Target Variable","text":"<p>A vari\u00e1vel Target representa a situa\u00e7\u00e3o final do estudante ao t\u00e9rmino do curso:</p> <ul> <li>Dropout (0): o aluno abandonou o curso  </li> <li>Enrolled (1): o aluno ainda est\u00e1 matriculado  </li> <li>Graduate (2): o aluno concluiu o curso com sucesso</li> </ul>"},{"location":"projetos/mlp-classification/#23-data-issues","title":"2.3 Data Issues","text":"<p>O dataset j\u00e1 foi pr\u00e9-processado e limpo pelos autores originais, n\u00e3o apresentando:</p> <ul> <li>Valores ausentes (<code>NaN</code>);</li> <li>Duplicatas;</li> <li>Outliers sem explica\u00e7\u00e3o evidente.  </li> </ul>"},{"location":"projetos/mlp-classification/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization","text":"<p>A etapa de limpeza e normaliza\u00e7\u00e3o foi realizada para garantir a qualidade dos dados e preparar o conjunto para o treinamento da rede MLP. O processo incluiu inspe\u00e7\u00e3o inicial, remo\u00e7\u00e3o de outliers, codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas, normaliza\u00e7\u00e3o num\u00e9rica e an\u00e1lise explorat\u00f3ria com PCA (Principal Component Analysis).</p>"},{"location":"projetos/mlp-classification/#31-initial-inspection","title":"3.1 Initial Inspection","text":"<p>Ap\u00f3s o carregamento do dataset (<code>train.csv</code>), foi feita uma an\u00e1lise explorat\u00f3ria inicial com <code>pandas</code> e <code>matplotlib</code> para verificar estrutura, tipos de dados e poss\u00edveis problemas de consist\u00eancia.</p> <pre><code>df.info()\ndf.isnull().sum()\ndf.describe().transpose()\ndf.hist(bins=30, figsize=(20, 15))\n</code></pre> <ul> <li>Nenhum valor nulo foi encontrado.</li> <li>N\u00e3o h\u00e1 colunas duplicadas.</li> <li>As distribui\u00e7\u00f5es mostram amplitudes diferentes entre vari\u00e1veis, o que justifica a posterior normaliza\u00e7\u00e3o.</li> </ul> <p>Figura 1 \u2014 Distribui\u00e7\u00e3o inicial das features</p> <p></p>"},{"location":"projetos/mlp-classification/#32-outlier-detection-and-removal","title":"3.2 Outlier Detection and Removal","text":"<p>Foram definidos limites manuais de plausibilidade (bounds) para cada vari\u00e1vel quantitativa, baseados em conhecimento de dom\u00ednio e na distribui\u00e7\u00e3o dos dados. A fun\u00e7\u00e3o <code>remove_outliers_by_bounds()</code> filtrou linhas fora desses intervalos.</p> <pre><code>df_sem_outliers = remove_outliers_by_bounds(df)\n</code></pre>"},{"location":"projetos/mlp-classification/#principais-resultados","title":"Principais resultados:","text":"Vari\u00e1vel Intervalo Mantido Linhas Removidas Previous qualification (grade) [80, 180] 13 Admission grade [90, 180] 16 Age at enrollment [16, 60] 33 Curricular units 1st sem (approved) [0, 15] 108 Curricular units 1st sem (without evaluations) [0, 4] 127 Curricular units 2nd sem (without evaluations) [0, 1] 1.011 Inflation rate [-1.5, 3.5] 7.327 GDP [-4.5, 3.5] 5.082 <p>Resumo final: 76.518 \u2192 62.502 linhas ap\u00f3s limpeza.</p> <p>Figura 2 \u2014 Distribui\u00e7\u00e3o ap\u00f3s remo\u00e7\u00e3o de outliers</p> <p></p> <p>A filtragem removeu registros com valores extremos (principalmente em indicadores macroecon\u00f4micos e notas m\u00e9dias), reduzindo ru\u00eddos sem comprometer o volume de dados.</p>"},{"location":"projetos/mlp-classification/#33-feature-encoding","title":"3.3 Feature Encoding","text":"<p>Para transformar vari\u00e1veis categ\u00f3ricas em num\u00e9ricas, foi aplicada codifica\u00e7\u00e3o one-hot com <code>pandas.get_dummies()</code>, preservando todas as categorias:</p> <pre><code>df_encoded = pd.get_dummies(df, columns=[\n    \"Nacionality\", \"Marital status\", \"Application mode\", \"Course\",\n    \"Previous qualification\", \"Mother's qualification\", \"Father's qualification\",\n    \"Mother's occupation\", \"Father's occupation\"\n])\n</code></pre> <ul> <li>As novas colunas (<code>uint8</code>) representaram corretamente cada categoria.</li> <li> <p>O <code>Target</code> foi mapeado para valores num\u00e9ricos:</p> </li> <li> <p><code>Graduate = 1.0</code></p> </li> <li><code>Dropout = 0.5</code></li> <li><code>Enrolled = 0.0</code></li> </ul>"},{"location":"projetos/mlp-classification/#34-normalization","title":"3.4 Normalization","text":"<p>Como as features possuem escalas diferentes (por exemplo, notas entre 0\u2013200 e idades entre 16\u201360), aplicou-se a normaliza\u00e7\u00e3o Min\u2013Max, escalando todos os valores para o intervalo [0, 1]:</p> <pre><code>from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_scaled[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n</code></pre> <p>Essa transforma\u00e7\u00e3o assegura que todas as vari\u00e1veis tenham o mesmo peso relativo no c\u00e1lculo dos gradientes durante o treinamento do MLP.</p>"},{"location":"projetos/mlp-classification/#35-dimensionality-reduction-pca","title":"3.5 Dimensionality Reduction (PCA)","text":"<p>Devido ao grande n\u00famero de vari\u00e1veis presentes no dataset \u2014 ap\u00f3s o one-hot encoding havia centenas de features num\u00e9ricas \u2014 aplicamos uma An\u00e1lise de Componentes Principais (PCA) com o objetivo de reduzir a dimensionalidade dos dados e facilitar a visualiza\u00e7\u00e3o da separabilidade entre as classes.</p> <p>A redu\u00e7\u00e3o de dimensionalidade \u00e9 \u00fatil neste caso porque: - Diminui a complexidade computacional para an\u00e1lises explorat\u00f3rias. - Permite observar padr\u00f5es e agrupamentos de classes em um espa\u00e7o bidimensional. - Ajuda a verificar se h\u00e1 algum grau de separa\u00e7\u00e3o natural entre as classes antes do treinamento do MLP.</p> <p>O PCA foi aplicado sobre as colunas num\u00e9ricas normalizadas, extraindo os dois primeiros componentes principais:</p> <pre><code>from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca_result = pca.fit_transform(df_scaled[numeric_cols])\n</code></pre> <p>Os resultados mostraram que:</p> <ul> <li>PC1: explica aproximadamente 34% da vari\u00e2ncia total</li> <li>PC2: explica aproximadamente 21% da vari\u00e2ncia</li> <li>Total: cerca de 55% da vari\u00e2ncia foi capturada pelos dois primeiros componentes</li> </ul> <p>Isso indica que, embora a maior parte da informa\u00e7\u00e3o ainda esteja distribu\u00edda entre diversas dimens\u00f5es, \u00e9 poss\u00edvel visualizar parte da estrutura dos dados em duas dimens\u00f5es.</p> <p>A figura abaixo mostra a proje\u00e7\u00e3o dos dados no plano formado pelos dois primeiros componentes principais, com as classes representadas por cores diferentes.</p> <p>Figura 3 \u2014 PCA (PC1 vs PC2) colorido por classe</p> <p></p>"},{"location":"projetos/mlp-classification/#36-summary","title":"3.6 Summary","text":"<ul> <li>Nenhum dado ausente ou duplicado encontrado.</li> <li>Outliers removidos com base em limites definidos manualmente.</li> <li>Vari\u00e1veis categ\u00f3ricas convertidas por one-hot encoding.</li> <li>Normaliza\u00e7\u00e3o Min\u2013Max aplicada a todas as vari\u00e1veis num\u00e9ricas.</li> </ul>"},{"location":"projetos/mlp-classification/#4-mlp-implementation","title":"4. MLP Implementation","text":"<p>Para a implementa\u00e7\u00e3o do MLP consideramos decis\u00f5es de design sobre arquitetura, fun\u00e7\u00f5es de ativa\u00e7\u00e3o, fun\u00e7\u00e3o de perda, inicializa\u00e7\u00e3o de pesos e hiperpar\u00e2metros. A implementa\u00e7\u00e3o foi feita em NumPy para refor\u00e7ar a compreens\u00e3o das opera\u00e7\u00f5es fundamentais (forward, loss, backprop e atualiza\u00e7\u00e3o de pesos). Em seguida detalhamos cada componente.</p>"},{"location":"projetos/mlp-classification/#41-network-architecture","title":"4.1 Network Architecture","text":"<p>Optou-se por um MLP com duas camadas ocultas:</p> <ul> <li>Input: dimens\u00e3o <code>D</code> (n\u00famero de features ap\u00f3s one-hot + normaliza\u00e7\u00e3o).</li> <li>Hidden layer 1: <code>H1 = 64</code> neur\u00f4nios, ativa\u00e7\u00e3o <code>tanh</code>.</li> <li>Hidden layer 2: <code>H2 = 32</code> neur\u00f4nios, ativa\u00e7\u00e3o <code>tanh</code>.</li> <li>Output: <code>K</code> neur\u00f4nios (n\u00famero de classes), ativa\u00e7\u00e3o <code>softmax</code>.</li> </ul> <p>Justificativa curta:</p> <ul> <li>Duas camadas ocultas capturam n\u00e3o linearidades mais complexas que uma \u00fanica camada sem tornar a rede excessivamente profunda.</li> <li>H1/H2 escolhidos por experimenta\u00e7\u00e3o emp\u00edrica: aumentos significativos na largura n\u00e3o trouxeram ganhos relevantes, apenas custo computacional.</li> </ul>"},{"location":"projetos/mlp-classification/#42-activation-functions","title":"4.2 Activation Functions","text":"<ul> <li> <p>Camadas ocultas: <code>tanh</code></p> </li> <li> <p>Vantagem: sa\u00edda centrada em zero, derivada simples (<code>1 - tanh^2</code>) e bom comportamento com inicializa\u00e7\u00f5es escalonadas.</p> </li> <li>Observa\u00e7\u00e3o: <code>ReLU</code> pode acelerar converg\u00eancia, mas neste trabalho mantivemos <code>tanh</code> por estabilidade e por corresponder ao material do curso.</li> <li> <p>Camada de sa\u00edda: <code>softmax</code></p> </li> <li> <p>Produz probabilidades normalizadas por amostra, adequada para cross-entropy multiclasses.</p> </li> </ul>"},{"location":"projetos/mlp-classification/#43-loss-function","title":"4.3 Loss Function","text":"<ul> <li>Cross-entropy (categorical) combinada com regulariza\u00e7\u00e3o L2 aplicada a todos os pesos:</li> </ul> \\[ \\text{loss} = -\\frac{1}{B}\\sum_{i=1}^{B}\\sum_{k=1}^{K} y_{ik}\\log(\\hat{y}_{ik} + \\varepsilon) + \\lambda \\sum_{\\ell} \\|W^{(\\ell)}\\|^2 \\] <ul> <li><code>B</code> \u00e9 o tamanho do mini-batch.</li> <li> <p>A combina\u00e7\u00e3o cross-entropy + softmax simplifica a derivada do output para <code>(yhat - y_onehot)/B</code>, o que facilita o c\u00e1lculo de gradientes.</p> </li> <li> <p>Motiva\u00e7\u00e3o: cross-entropy penaliza previs\u00f5es confiantes e erradas de forma mais forte que MSE, sendo padr\u00e3o em classifica\u00e7\u00e3o probabil\u00edstica.</p> </li> </ul>"},{"location":"projetos/mlp-classification/#44-hyperparameters","title":"4.4 Hyperparameters","text":"<p>Hiperpar\u00e2metros selecionados (valores usados nos experimentos):</p> <ul> <li> <p>Learning Rate (lr): <code>0.005</code></p> </li> <li> <p>Testes com 0.001, 0.005 e 0.01 indicaram que 0.005 entregou bom equil\u00edbrio entre velocidade e estabilidade com <code>tanh</code>.</p> </li> <li> <p>Epochs (m\u00e1x): <code>500</code></p> </li> <li> <p>Limite superior; o treinamento \u00e9 interrompido via early stopping quando a valida\u00e7\u00e3o n\u00e3o melhora.</p> </li> <li> <p>Batch size: <code>64</code></p> </li> <li> <p>Atualiza\u00e7\u00e3o mais frequente que <code>128</code>, com bom trade-off entre estabilidade do gradiente e varia\u00e7\u00e3o estoc\u00e1stica \u00fatil.</p> </li> <li> <p>Regularization (L2): <code>lambda_l2 = 1e-4</code></p> </li> <li> <p>Penaliza\u00e7\u00e3o leve que reduz overfitting sem prejudicar aprendizagem.</p> </li> <li>Patience (early stopping): <code>15</code> \u00e9pocas sem melhora em <code>val_loss</code>.</li> <li> <p>Inicializa\u00e7\u00e3o de pesos: normal padr\u00e3o com escala <code>1/sqrt(fan_in)</code>:</p> </li> <li> <p>ex.: <code>W = rng.normal(0,1,(out,in)) / sqrt(in)</code> \u2014 reduz satura\u00e7\u00e3o inicial e ajuda na estabilidade do treino.</p> </li> <li>Seed / reproducibility: <code>np.random.default_rng(42)</code> e <code>random_state=42</code> nos splits.</li> </ul> <p>Ajustes e recomenda\u00e7\u00f5es:</p> <ul> <li>Para acelerar converg\u00eancia pode-se testar <code>ReLU + Adam</code> (optimizers modernos) e <code>batch normalization</code>.</li> <li>Para lidar com desbalanceamento usar <code>class_weight</code> no loss ou t\u00e9cnicas de oversampling (SMOTE).</li> </ul>"},{"location":"projetos/mlp-classification/#45-implementation-details","title":"4.5 Implementation Details","text":"<p>A implementa\u00e7\u00e3o do MLP foi feita inteiramente em NumPy, permitindo controle direto sobre cada etapa \u2014 forward pass, c\u00e1lculo da fun\u00e7\u00e3o de perda, retropropaga\u00e7\u00e3o e atualiza\u00e7\u00e3o dos par\u00e2metros. Essa abordagem possibilita compreender o fluxo interno de dados e gradientes, sem a abstra\u00e7\u00e3o de frameworks como PyTorch ou TensorFlow.</p> <p>Para refer\u00eancia, as etapas de treinamento seguiram o fluxo do material de Numerical Simulation do curso, dividido em:</p> <ol> <li>Forward Pass: os dados s\u00e3o propagados camada a camada;</li> <li>Loss Calculation: c\u00e1lculo da cross-entropy e penaliza\u00e7\u00e3o L2;</li> <li>Backward Pass: propaga\u00e7\u00e3o reversa dos gradientes para ajustar pesos e vieses.</li> </ol> <pre><code># Forward Pass (propaga\u00e7\u00e3o)\nz1 = xb @ W1.T + b1; h1 = tanh(z1)\nz2 = h1 @ W2.T + b2; h2 = tanh(z2)\nz3 = h2 @ W3.T + b3; yhat = softmax(z3)\n\n# C\u00e1lculo da Loss (cross-entropy + L2)\nloss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))\nl2_term = lambda_l2 * (np.sum(W1*W1) + np.sum(W2*W2) + np.sum(W3*W3))\nloss = loss_ce + l2_term\n\n# Backpropagation\nd3 = (yhat - yb) / B\ngW3 = d3.T @ h2 + 2*lambda_l2*W3\ngb3 = d3.sum(axis=0)\n\ndh2 = d3 @ W3\ndz2 = dh2 * (1 - h2**2)\ngW2 = dz2.T @ h1 + 2*lambda_l2*W2\ngb2 = dz2.sum(axis=0)\n\ndh1 = dz2 @ W2\ndz1 = dh1 * (1 - h1**2)\ngW1 = dz1.T @ xb + 2*lambda_l2*W1\ngb1 = dz1.sum(axis=0)\n\n# Atualiza\u00e7\u00e3o dos par\u00e2metros\nW3 -= lr * gW3; b3 -= lr * gb3\nW2 -= lr * gW2; b2 -= lr * gb2\nW1 -= lr * gW1; b1 -= lr * gb1\n</code></pre> <p>Durante o treinamento, foram adotadas tr\u00eas pr\u00e1ticas fundamentais:</p> <ul> <li>Mini-Batch Training: processa subconjuntos de amostras, aumentando a efici\u00eancia e introduzindo ru\u00eddo estoc\u00e1stico que ajuda na generaliza\u00e7\u00e3o.</li> <li>Regulariza\u00e7\u00e3o L2: evita overfitting penalizando pesos muito grandes.</li> <li>Early Stopping: interrompe o treino se a perda de valida\u00e7\u00e3o (<code>val_loss</code>) n\u00e3o melhorar ap\u00f3s 15 \u00e9pocas consecutivas, preservando o modelo \u00f3timo.</li> </ul>"},{"location":"projetos/mlp-classification/#monitoramento-de-metricas","title":"Monitoramento de m\u00e9tricas","text":"<p>Durante as \u00e9pocas, foram registrados vetores de perda e acur\u00e1cia para treino e valida\u00e7\u00e3o:</p> <ul> <li><code>train_losses</code>, <code>val_losses</code></li> <li><code>train_accs</code>, <code>val_accs</code></li> </ul> <p>Exemplo dos logs registrados:</p> <pre><code>\u00c9poca  10 | TrainLoss 0.5499 | ValLoss 0.5394 | TrainAcc 0.793 | ValAcc 0.791\n\u00c9poca 200 | TrainLoss 0.4619 | ValLoss 0.4555 | TrainAcc 0.826 | ValAcc 0.823\nEarly stopping (\u00e9poca 219) melhor val_loss=0.4528\n</code></pre> <p>Esses logs mostram uma converg\u00eancia suave, com perda diminuindo at\u00e9 a \u00e9poca ~200, quando a valida\u00e7\u00e3o estabiliza, demonstrando equil\u00edbrio entre aprendizado e generaliza\u00e7\u00e3o.</p>"},{"location":"projetos/mlp-classification/#geracao-das-curvas-de-aprendizado","title":"Gera\u00e7\u00e3o das curvas de aprendizado","text":"<p>As curvas de loss e acur\u00e1cia foram plotadas para inspe\u00e7\u00e3o visual da converg\u00eancia:</p> <pre><code>plt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Loss\"); plt.title(\"Curva de Loss\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(train_accs, label=\"Train Acc\")\nplt.plot(val_accs, label=\"Val Acc\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\"); plt.title(\"Curva de Acur\u00e1cia\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Esses gr\u00e1ficos permitem verificar se h\u00e1 overfitting (diverg\u00eancia entre treino e valida\u00e7\u00e3o) ou underfitting (valores de perda altos e est\u00e1veis).</p>"},{"location":"projetos/mlp-classification/#avaliacao-e-checkpoint","title":"Avalia\u00e7\u00e3o e checkpoint","text":"<p>Ao final do treino:</p> <ul> <li>O modelo \u00e9 restaurado para os pesos do melhor checkpoint (menor <code>val_loss</code>);</li> <li>Avaliado no conjunto de teste com m\u00e9tricas: acur\u00e1cia, precis\u00e3o, recall e F1-score;</li> <li>Uma matriz de confus\u00e3o \u00e9 gerada para an\u00e1lise dos erros;</li> <li>O decision boundary \u00e9 visualizado no espa\u00e7o 2D via PCA.</li> </ul>"},{"location":"projetos/mlp-classification/#5-model-training","title":"5. Model Training","text":""},{"location":"projetos/mlp-classification/#51-training-setup","title":"5.1 Training Setup","text":"<ul> <li>Dados j\u00e1 codificados e escalados (MinMax).</li> <li>Split feito com <code>stratify</code> para preservar propor\u00e7\u00f5es de classe (70% train / 15% val / 15% test).</li> <li>Shuffle por \u00e9poca com <code>rng.permutation</code>.</li> </ul>"},{"location":"projetos/mlp-classification/#52-training-loop-resumo","title":"5.2 Training Loop (resumo)","text":"<ul> <li> <p>Para cada \u00e9poca:</p> </li> <li> <p>embaralha treino;</p> </li> <li>itera por mini-batches;</li> <li>forward \u2192 calcula <code>loss</code> (+ L2) \u2192 backward \u2192 atualiza par\u00e2metros;</li> <li>ao fim da \u00e9poca calcula <code>train_loss</code>, <code>val_loss</code>, <code>train_acc</code>, <code>val_acc</code>.</li> <li>Early stopping: interrompe se <code>val_loss</code> n\u00e3o melhorar por <code>patience</code> \u00e9pocas.</li> </ul> <p>Trechos chave (exemplo do forward, loss e update):</p> <pre><code># forward (mini-batch)\nz1 = xb @ W1.T + b1; h1 = tanh(z1)\nz2 = h1 @ W2.T + b2; h2 = tanh(z2)\nz3 = h2 @ W3.T + b3; yhat = softmax(z3)\n\n# loss CE + L2\nloss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))\nl2_term = lambda_l2 * (np.sum(W1*W1) + ...)\nloss = loss_ce + l2_term\n\n# backward (resumo)\nd3 = (yhat - yb) / B\ngW3 = d3.T @ h2 + 2*lambda_l2*W3\n... # calcula gW2,gW1 e gb*\n\n# update\nW3 -= lr * gW3\n...\n</code></pre>"},{"location":"projetos/mlp-classification/#53-training-observations-logs","title":"5.3 Training Observations (logs)","text":"<p>Trechos de log produzidos durante treinamento:</p> <pre><code>\u00c9poca   1 | TrainLoss 0.9766 | ValLoss 0.8791 | TrainAcc 0.654 | ValAcc 0.653\n\u00c9poca  10 | TrainLoss 0.5499 | ValLoss 0.5394 | TrainAcc 0.793 | ValAcc 0.791\n...\n\u00c9poca 210 | TrainLoss 0.4615 | ValLoss 0.4534 | TrainAcc 0.825 | ValAcc 0.823\nEarly stopping (\u00e9poca 219) melhor val_loss=0.4528\n</code></pre> <ul> <li>Observa-se r\u00e1pida converg\u00eancia nas primeiras 50 \u00e9pocas; depois estabiliza\u00e7\u00e3o com pequenas melhorias.</li> <li>Early stopping ao redor de ~219 \u00e9pocas.</li> </ul>"},{"location":"projetos/mlp-classification/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy","text":""},{"location":"projetos/mlp-classification/#61-data-split","title":"6.1 Data Split","text":"<ul> <li><code>train / val / test = 70% / 15% / 15%</code> com stratify para preservar distribui\u00e7\u00e3o de classes.</li> </ul>"},{"location":"projetos/mlp-classification/#62-validation-strategy","title":"6.2 Validation Strategy","text":"<ul> <li>Valida\u00e7\u00e3o simples (conjunto de valida\u00e7\u00e3o dedicado) usada para sele\u00e7\u00e3o de hiperpar\u00e2metros e early stopping.</li> </ul>"},{"location":"projetos/mlp-classification/#63-reproducibility","title":"6.3 Reproducibility","text":"<ul> <li>Seed fixada via <code>np.random.default_rng(42)</code> e <code>random_state=42</code> nos <code>train_test_split</code>.</li> </ul>"},{"location":"projetos/mlp-classification/#64-overfitting-prevention","title":"6.4 Overfitting Prevention","text":"<ul> <li>Regulariza\u00e7\u00e3o L2 em todos os pesos.</li> <li>Early stopping baseado em <code>val_loss</code>.</li> <li>Batch training (mini-batch) fornece ru\u00eddo \u00fatil ao otimizar.</li> </ul>"},{"location":"projetos/mlp-classification/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization","text":""},{"location":"projetos/mlp-classification/#71-loss-and-accuracy-curves","title":"7.1 Loss and Accuracy Curves","text":"<p>Figura: Curva de Loss e Acur\u00e1cia (treino vs valida\u00e7\u00e3o).</p> <p></p>"},{"location":"projetos/mlp-classification/#72-analysis-of-learning-behavior","title":"7.2 Analysis of Learning Behavior","text":"<ul> <li>Treino e valida\u00e7\u00e3o converge pr\u00f3ximos e sem diverg\u00eancia acentuada \u2192 overfitting limitado gra\u00e7as a L2 + early stopping.</li> <li>Acur\u00e1cia de valida\u00e7\u00e3o estabilizou ~0.823, indicando bom ajuste do modelo.</li> </ul>"},{"location":"projetos/mlp-classification/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":""},{"location":"projetos/mlp-classification/#81-test-metrics-resultado-final","title":"8.1 Test Metrics (resultado final)","text":"<p>Resultados calculados no conjunto de teste:</p> <pre><code>M\u00c9TRICAS TESTE\nAcur\u00e1cia : 0.8195\nPrecis\u00e3o : 0.7897\nRecall   : 0.7690\nF1-score : 0.7771\n</code></pre>"},{"location":"projetos/mlp-classification/#82-baseline-majority-class","title":"8.2 Baseline (majority class)","text":"<p>Classe majorit\u00e1ria no teste: <code>2</code> Baseline (predizer sempre a classe majorit\u00e1ria):</p> <pre><code>M\u00c9TRICAS BASELINE (Majority Class)\nAcur\u00e1cia : 0.4708\nPrecis\u00e3o : 0.1569\nRecall   : 0.3333\nF1-score : 0.2134\n</code></pre> <p>Compara\u00e7\u00e3o: o MLP supera claramente o baseline em todas as m\u00e9tricas (ex.: acur\u00e1cia 0.8195 vs 0.4708).</p>"},{"location":"projetos/mlp-classification/#83-confusion-matrix","title":"8.3 Confusion Matrix","text":"<p>Figura: Matriz de Confus\u00e3o (Teste):</p> <p></p> <p>Classe \"Graduate (1.0)\" O modelo obteve o melhor desempenho nesta classe, com 4088 acertos e poucos erros de confus\u00e3o. Representa tamb\u00e9m a classe majorit\u00e1ria, o que explica a facilidade de identifica\u00e7\u00e3o. Poucos graduados foram confundidos como \u201cDropout\u201d (242) ou \u201cEnrolled\u201d (84).</p> <p>Classe \"Enrolled (0.5)\" A segunda melhor classe em termos de acerto (2568 corretos). Alguns alunos ainda matriculados foram incorretamente classificados como \u201cDropout\u201d (341) \u2014 possivelmente devido a perfis de desempenho inicial similares. Menor confus\u00e3o com \u201cGraduate\u201d (205) indica que o modelo diferencia razoavelmente alunos ativos dos formados.</p> <p>Classe \"Dropout (0.0)\" A classe mais problem\u00e1tica: apenas 1028 acertos contra 820 erros (211+609). Quase 600 alunos que abandonaram o curso foram classificados como \u201cGraduate\u201d \u2014 mostrando que o modelo tem dificuldade em detectar evas\u00e3o, o que \u00e9 cr\u00edtico neste tipo de aplica\u00e7\u00e3o. Essa confus\u00e3o indica que o modelo tende a superestimar o sucesso acad\u00eamico, possivelmente por conta do desbalanceamento do dataset.</p>"},{"location":"projetos/mlp-classification/#84-decision-boundary-pca-2d","title":"8.4 Decision Boundary (PCA 2D)","text":"<p>Figura: Decis\u00e3o no espa\u00e7o PCA (PC1 \u00d7 PC2) com pontos corretos/errados do conjunto de teste.</p> <p></p>"},{"location":"projetos/mlp-classification/#9-conclusion","title":"9. Conclusion","text":""},{"location":"projetos/mlp-classification/#91-key-findings","title":"9.1 Key Findings","text":"<ul> <li>MLP implementado do zero (NumPy) atingiu Acur\u00e1cia = 0.8195 e F1 \u2248 0.7771 no conjunto de teste, superando largamente o baseline majorit\u00e1rio.</li> <li>PCA mostrou separa\u00e7\u00e3o parcial entre classes, justificando uso de modelo n\u00e3o-linear (MLP).</li> </ul>"},{"location":"projetos/mlp-classification/#92-limitations","title":"9.2 Limitations","text":"<ul> <li>Desbalanceamento de classes requer an\u00e1lise adicional (class weighting, oversampling/undersampling).</li> <li>Remo\u00e7\u00e3o de outliers foi feita por limites manuais \u2014 abordagem autom\u00e1tica (IQR, isolation forest) poderia ser comparada.</li> <li>Implementa\u00e7\u00e3o atual usa <code>tanh</code>; testar <code>ReLU</code> + batchnorm pode acelerar/aperfei\u00e7oar treino.</li> </ul>"},{"location":"projetos/mlp-classification/#10-referencias","title":"10. Refer\u00eancias","text":"<ol> <li> <p>Material de apoio \u2014 Artificial Neural Networks and Deep Learning https://insper.github.io/ann-dl/</p> </li> <li> <p>Kaggle \u2014 Academic Success Classifier Competition https://www.kaggle.com/competitions/academic-success-classifier/data</p> </li> <li> <p>UCI Machine Learning Repository \u2014 Predict Students\u2019 Dropout and Academic Success https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success</p> </li> </ol>"},{"location":"projetos/mlp-classification/#appendix-codigo-de-treino-e-avaliacao","title":"Appendix \u2014 C\u00f3digo de Treino e Avalia\u00e7\u00e3o","text":"<p>Arquivo Jupyter implementando o MLP .</p>"},{"location":"projetos/mlp-classification/main/","title":"Main","text":"In\u00a0[13]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Caminho para o seu ficheiro CSV\ncaminho_ficheiro = 'data/train.csv' \n\n# Carregar o ficheiro CSV para um DataFrame\ndf = pd.read_csv(caminho_ficheiro)\n\ndf.head()\n</pre> import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import pandas as pd  # Caminho para o seu ficheiro CSV caminho_ficheiro = 'data/train.csv'   # Carregar o ficheiro CSV para um DataFrame df = pd.read_csv(caminho_ficheiro)  df.head() Out[13]: id Marital status Application mode Application order Course Daytime/evening attendance Previous qualification Previous qualification (grade) Nacionality Mother's qualification ... Curricular units 2nd sem (credited) Curricular units 2nd sem (enrolled) Curricular units 2nd sem (evaluations) Curricular units 2nd sem (approved) Curricular units 2nd sem (grade) Curricular units 2nd sem (without evaluations) Unemployment rate Inflation rate GDP Target 0 0 1 1 1 9238 1 1 126.0 1 1 ... 0 6 7 6 12.428571 0 11.1 0.6 2.02 Graduate 1 1 1 17 1 9238 1 1 125.0 1 19 ... 0 6 9 0 0.000000 0 11.1 0.6 2.02 Dropout 2 2 1 17 2 9254 1 1 137.0 1 3 ... 0 6 0 0 0.000000 0 16.2 0.3 -0.92 Dropout 3 3 1 1 3 9500 1 1 131.0 1 19 ... 0 8 11 7 12.820000 0 11.1 0.6 2.02 Enrolled 4 4 1 1 2 9500 1 1 132.0 1 19 ... 0 7 12 6 12.933333 0 7.6 2.6 0.32 Graduate <p>5 rows \u00d7 38 columns</p> In\u00a0[14]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 76518 entries, 0 to 76517\nData columns (total 38 columns):\n #   Column                                          Non-Null Count  Dtype  \n---  ------                                          --------------  -----  \n 0   id                                              76518 non-null  int64  \n 1   Marital status                                  76518 non-null  int64  \n 2   Application mode                                76518 non-null  int64  \n 3   Application order                               76518 non-null  int64  \n 4   Course                                          76518 non-null  int64  \n 5   Daytime/evening attendance                      76518 non-null  int64  \n 6   Previous qualification                          76518 non-null  int64  \n 7   Previous qualification (grade)                  76518 non-null  float64\n 8   Nacionality                                     76518 non-null  int64  \n 9   Mother's qualification                          76518 non-null  int64  \n 10  Father's qualification                          76518 non-null  int64  \n 11  Mother's occupation                             76518 non-null  int64  \n 12  Father's occupation                             76518 non-null  int64  \n 13  Admission grade                                 76518 non-null  float64\n 14  Displaced                                       76518 non-null  int64  \n 15  Educational special needs                       76518 non-null  int64  \n 16  Debtor                                          76518 non-null  int64  \n 17  Tuition fees up to date                         76518 non-null  int64  \n 18  Gender                                          76518 non-null  int64  \n 19  Scholarship holder                              76518 non-null  int64  \n 20  Age at enrollment                               76518 non-null  int64  \n 21  International                                   76518 non-null  int64  \n 22  Curricular units 1st sem (credited)             76518 non-null  int64  \n 23  Curricular units 1st sem (enrolled)             76518 non-null  int64  \n 24  Curricular units 1st sem (evaluations)          76518 non-null  int64  \n 25  Curricular units 1st sem (approved)             76518 non-null  int64  \n 26  Curricular units 1st sem (grade)                76518 non-null  float64\n 27  Curricular units 1st sem (without evaluations)  76518 non-null  int64  \n 28  Curricular units 2nd sem (credited)             76518 non-null  int64  \n 29  Curricular units 2nd sem (enrolled)             76518 non-null  int64  \n 30  Curricular units 2nd sem (evaluations)          76518 non-null  int64  \n 31  Curricular units 2nd sem (approved)             76518 non-null  int64  \n 32  Curricular units 2nd sem (grade)                76518 non-null  float64\n 33  Curricular units 2nd sem (without evaluations)  76518 non-null  int64  \n 34  Unemployment rate                               76518 non-null  float64\n 35  Inflation rate                                  76518 non-null  float64\n 36  GDP                                             76518 non-null  float64\n 37  Target                                          76518 non-null  object \ndtypes: float64(7), int64(30), object(1)\nmemory usage: 22.2+ MB\n</pre> In\u00a0[15]: Copied! <pre>df.isnull().sum()\n</pre> df.isnull().sum() Out[15]: <pre>id                                                0\nMarital status                                    0\nApplication mode                                  0\nApplication order                                 0\nCourse                                            0\nDaytime/evening attendance                        0\nPrevious qualification                            0\nPrevious qualification (grade)                    0\nNacionality                                       0\nMother's qualification                            0\nFather's qualification                            0\nMother's occupation                               0\nFather's occupation                               0\nAdmission grade                                   0\nDisplaced                                         0\nEducational special needs                         0\nDebtor                                            0\nTuition fees up to date                           0\nGender                                            0\nScholarship holder                                0\nAge at enrollment                                 0\nInternational                                     0\nCurricular units 1st sem (credited)               0\nCurricular units 1st sem (enrolled)               0\nCurricular units 1st sem (evaluations)            0\nCurricular units 1st sem (approved)               0\nCurricular units 1st sem (grade)                  0\nCurricular units 1st sem (without evaluations)    0\nCurricular units 2nd sem (credited)               0\nCurricular units 2nd sem (enrolled)               0\nCurricular units 2nd sem (evaluations)            0\nCurricular units 2nd sem (approved)               0\nCurricular units 2nd sem (grade)                  0\nCurricular units 2nd sem (without evaluations)    0\nUnemployment rate                                 0\nInflation rate                                    0\nGDP                                               0\nTarget                                            0\ndtype: int64</pre> In\u00a0[16]: Copied! <pre>df.describe().transpose()\n</pre> df.describe().transpose() Out[16]: count mean std min 25% 50% 75% max id 76518.0 38258.500000 22088.988286 0.00 19129.250000 38258.500000 57387.750000 76517.000 Marital status 76518.0 1.111934 0.441669 1.00 1.000000 1.000000 1.000000 6.000 Application mode 76518.0 16.054419 16.682337 1.00 1.000000 17.000000 39.000000 53.000 Application order 76518.0 1.644410 1.229645 0.00 1.000000 1.000000 2.000000 9.000 Course 76518.0 9001.286377 1803.438531 33.00 9119.000000 9254.000000 9670.000000 9991.000 Daytime/evening attendance 76518.0 0.915314 0.278416 0.00 1.000000 1.000000 1.000000 1.000 Previous qualification 76518.0 3.658760 8.623774 1.00 1.000000 1.000000 1.000000 43.000 Previous qualification (grade) 76518.0 132.378766 10.995328 95.00 125.000000 133.100000 140.000000 190.000 Nacionality 76518.0 1.226600 3.392183 1.00 1.000000 1.000000 1.000000 109.000 Mother's qualification 76518.0 19.837633 15.399456 1.00 1.000000 19.000000 37.000000 44.000 Father's qualification 76518.0 23.425076 14.921164 1.00 4.000000 19.000000 37.000000 44.000 Mother's occupation 76518.0 8.583196 17.471591 0.00 4.000000 7.000000 9.000000 194.000 Father's occupation 76518.0 8.882172 16.803940 0.00 5.000000 7.000000 9.000000 195.000 Admission grade 76518.0 125.363971 12.562328 95.00 118.000000 124.600000 132.000000 190.000 Displaced 76518.0 0.569265 0.495182 0.00 0.000000 1.000000 1.000000 1.000 Educational special needs 76518.0 0.003738 0.061023 0.00 0.000000 0.000000 0.000000 1.000 Debtor 76518.0 0.071382 0.257463 0.00 0.000000 0.000000 0.000000 1.000 Tuition fees up to date 76518.0 0.893646 0.308292 0.00 1.000000 1.000000 1.000000 1.000 Gender 76518.0 0.315821 0.464845 0.00 0.000000 0.000000 1.000000 1.000 Scholarship holder 76518.0 0.247393 0.431500 0.00 0.000000 0.000000 0.000000 1.000 Age at enrollment 76518.0 22.278653 6.889241 17.00 18.000000 19.000000 23.000000 70.000 International 76518.0 0.006626 0.081130 0.00 0.000000 0.000000 0.000000 1.000 Curricular units 1st sem (credited) 76518.0 0.188871 1.175296 0.00 0.000000 0.000000 0.000000 20.000 Curricular units 1st sem (enrolled) 76518.0 5.891516 1.671776 0.00 5.000000 6.000000 6.000000 26.000 Curricular units 1st sem (evaluations) 76518.0 7.352362 3.508292 0.00 6.000000 7.000000 9.000000 45.000 Curricular units 1st sem (approved) 76518.0 4.178520 2.687995 0.00 2.000000 5.000000 6.000000 26.000 Curricular units 1st sem (grade) 76518.0 9.995862 5.264224 0.00 10.666667 12.166667 13.314286 18.875 Curricular units 1st sem (without evaluations) 76518.0 0.057960 0.408490 0.00 0.000000 0.000000 0.000000 12.000 Curricular units 2nd sem (credited) 76518.0 0.137053 0.933830 0.00 0.000000 0.000000 0.000000 19.000 Curricular units 2nd sem (enrolled) 76518.0 5.933414 1.627182 0.00 5.000000 6.000000 6.000000 23.000 Curricular units 2nd sem (evaluations) 76518.0 7.234468 3.503040 0.00 6.000000 7.000000 9.000000 33.000 Curricular units 2nd sem (approved) 76518.0 4.007201 2.772956 0.00 1.000000 5.000000 6.000000 20.000 Curricular units 2nd sem (grade) 76518.0 9.626085 5.546035 0.00 10.000000 12.142857 13.244048 18.000 Curricular units 2nd sem (without evaluations) 76518.0 0.062443 0.462107 0.00 0.000000 0.000000 0.000000 12.000 Unemployment rate 76518.0 11.520340 2.653375 7.60 9.400000 11.100000 12.700000 16.200 Inflation rate 76518.0 1.228218 1.398816 -0.80 0.300000 1.400000 2.600000 3.700 GDP 76518.0 -0.080921 2.251382 -4.06 -1.700000 0.320000 1.790000 3.510 In\u00a0[17]: Copied! <pre>df.hist(bins=30, figsize=(20, 15), layout=(7, 6))\nplt.tight_layout()\nplt.show()\n</pre> df.hist(bins=30, figsize=(20, 15), layout=(7, 6)) plt.tight_layout() plt.show() In\u00a0[18]: Copied! <pre>import pandas as pd\n\nBOUNDS = {\n    # Notas e idades\n    \"Previous qualification (grade)\": (80, 180),\n    \"Admission grade\": (90, 180),\n    \"Age at enrollment\": (16, 60),\n\n    # 1\u00ba semestre\n    \"Curricular units 1st sem (evaluations)\": (0, 40),\n    \"Curricular units 1st sem (approved)\": (0, 15),\n    \"Curricular units 1st sem (grade)\": (0, 17),         \n    \"Curricular units 1st sem (credited)\": (0, 10),\n    \"Curricular units 1st sem (enrolled)\": (0, 15),\n    \"Curricular units 1st sem (without evaluations)\": (0, 4),\n\n    # 2\u00ba semestre\n    \"Curricular units 2nd sem (evaluations)\": (0, 20),\n    \"Curricular units 2nd sem (approved)\": (0, 15),\n    \"Curricular units 2nd sem (grade)\": (0, 20),         \n    \"Curricular units 2nd sem (credited)\": (0,10),\n    \"Curricular units 2nd sem (enrolled)\": (0, 15),\n    \"Curricular units 2nd sem (without evaluations)\": (0, 1),\n\n    # Macro (pela figura)\n    \"Unemployment rate\": (0, 17),\n    \"Inflation rate\": (-1.5, 3.5),\n    \"GDP\": (-4.5, 3.5),\n}\n\ndef remove_outliers_by_bounds(df: pd.DataFrame, bounds: dict = BOUNDS) -&gt; pd.DataFrame:\n    \n    df_clean = df.copy()\n    total_before = len(df_clean)\n\n    for col, (lo, hi) in bounds.items():\n        if col in df_clean.columns:\n            before = len(df_clean)\n            df_clean = df_clean[df_clean[col].between(lo, hi)]\n            removed = before - len(df_clean)\n            print(f\"\u2705 {col}: mantido [{lo}, {hi}] \u2014 removidos {removed}\")\n        else:\n            print(f\"\u2022 {col}: n\u00e3o encontrada no DataFrame (ignorada).\")\n\n    print(f\"\\nResumo: {total_before} \u2192 {len(df_clean)} linhas ap\u00f3s limpeza.\")\n    return df_clean\n\ndf_sem_outliers = remove_outliers_by_bounds(df)\ndf_sem_outliers.hist(bins=30, figsize=(20, 15), layout=(7, 6))\nplt.tight_layout()\nplt.show()\ndf = df_sem_outliers.copy()\n</pre> import pandas as pd  BOUNDS = {     # Notas e idades     \"Previous qualification (grade)\": (80, 180),     \"Admission grade\": (90, 180),     \"Age at enrollment\": (16, 60),      # 1\u00ba semestre     \"Curricular units 1st sem (evaluations)\": (0, 40),     \"Curricular units 1st sem (approved)\": (0, 15),     \"Curricular units 1st sem (grade)\": (0, 17),              \"Curricular units 1st sem (credited)\": (0, 10),     \"Curricular units 1st sem (enrolled)\": (0, 15),     \"Curricular units 1st sem (without evaluations)\": (0, 4),      # 2\u00ba semestre     \"Curricular units 2nd sem (evaluations)\": (0, 20),     \"Curricular units 2nd sem (approved)\": (0, 15),     \"Curricular units 2nd sem (grade)\": (0, 20),              \"Curricular units 2nd sem (credited)\": (0,10),     \"Curricular units 2nd sem (enrolled)\": (0, 15),     \"Curricular units 2nd sem (without evaluations)\": (0, 1),      # Macro (pela figura)     \"Unemployment rate\": (0, 17),     \"Inflation rate\": (-1.5, 3.5),     \"GDP\": (-4.5, 3.5), }  def remove_outliers_by_bounds(df: pd.DataFrame, bounds: dict = BOUNDS) -&gt; pd.DataFrame:          df_clean = df.copy()     total_before = len(df_clean)      for col, (lo, hi) in bounds.items():         if col in df_clean.columns:             before = len(df_clean)             df_clean = df_clean[df_clean[col].between(lo, hi)]             removed = before - len(df_clean)             print(f\"\u2705 {col}: mantido [{lo}, {hi}] \u2014 removidos {removed}\")         else:             print(f\"\u2022 {col}: n\u00e3o encontrada no DataFrame (ignorada).\")      print(f\"\\nResumo: {total_before} \u2192 {len(df_clean)} linhas ap\u00f3s limpeza.\")     return df_clean  df_sem_outliers = remove_outliers_by_bounds(df) df_sem_outliers.hist(bins=30, figsize=(20, 15), layout=(7, 6)) plt.tight_layout() plt.show() df = df_sem_outliers.copy() <pre>\u2705 Previous qualification (grade): mantido [80, 180] \u2014 removidos 13\n\u2705 Admission grade: mantido [90, 180] \u2014 removidos 16\n\u2705 Age at enrollment: mantido [16, 60] \u2014 removidos 33\n\u2705 Curricular units 1st sem (evaluations): mantido [0, 40] \u2014 removidos 3\n\u2705 Curricular units 1st sem (approved): mantido [0, 15] \u2014 removidos 108\n\u2705 Curricular units 1st sem (grade): mantido [0, 17] \u2014 removidos 27\n\u2705 Curricular units 1st sem (credited): mantido [0, 10] \u2014 removidos 151\n\u2705 Curricular units 1st sem (enrolled): mantido [0, 15] \u2014 removidos 25\n\u2705 Curricular units 1st sem (without evaluations): mantido [0, 4] \u2014 removidos 127\n\u2705 Curricular units 2nd sem (evaluations): mantido [0, 20] \u2014 removidos 72\n\u2705 Curricular units 2nd sem (approved): mantido [0, 15] \u2014 removidos 2\n\u2705 Curricular units 2nd sem (grade): mantido [0, 20] \u2014 removidos 0\n\u2705 Curricular units 2nd sem (credited): mantido [0, 10] \u2014 removidos 19\n\u2705 Curricular units 2nd sem (enrolled): mantido [0, 15] \u2014 removidos 0\n\u2705 Curricular units 2nd sem (without evaluations): mantido [0, 1] \u2014 removidos 1011\n\u2705 Unemployment rate: mantido [0, 17] \u2014 removidos 0\n\u2705 Inflation rate: mantido [-1.5, 3.5] \u2014 removidos 7327\n\u2705 GDP: mantido [-4.5, 3.5] \u2014 removidos 5082\n\nResumo: 76518 \u2192 62502 linhas ap\u00f3s limpeza.\n</pre> In\u00a0[19]: Copied! <pre>from sklearn.feature_selection import f_classif\nfrom sklearn.preprocessing import LabelEncoder\n\ntarget = \"Target\"\n\n# Converter o target categ\u00f3rico para n\u00fameros\nle = LabelEncoder()\ny = le.fit_transform(df[target])\n\n# Selecionar apenas as features num\u00e9ricas\nX = df.select_dtypes(include=[\"int64\", \"float64\"])\n\n# Calcular o valor F e o p-valor\nf_values, p_values = f_classif(X, y)\n\n# Montar um DataFrame com resultados\nanova_results = pd.DataFrame({\n    \"Feature\": X.columns,\n    \"F_value\": f_values,\n    \"p_value\": p_values\n}).sort_values(\"F_value\", ascending=False)\n\nprint(anova_results)\n</pre> from sklearn.feature_selection import f_classif from sklearn.preprocessing import LabelEncoder  target = \"Target\"  # Converter o target categ\u00f3rico para n\u00fameros le = LabelEncoder() y = le.fit_transform(df[target])  # Selecionar apenas as features num\u00e9ricas X = df.select_dtypes(include=[\"int64\", \"float64\"])  # Calcular o valor F e o p-valor f_values, p_values = f_classif(X, y)  # Montar um DataFrame com resultados anova_results = pd.DataFrame({     \"Feature\": X.columns,     \"F_value\": f_values,     \"p_value\": p_values }).sort_values(\"F_value\", ascending=False)  print(anova_results) <pre>                                           Feature       F_value  \\\n31             Curricular units 2nd sem (approved)  55012.368720   \n32                Curricular units 2nd sem (grade)  41227.809404   \n25             Curricular units 1st sem (approved)  41152.259196   \n26                Curricular units 1st sem (grade)  28643.276209   \n17                         Tuition fees up to date   7447.540333   \n19                              Scholarship holder   6091.296799   \n30          Curricular units 2nd sem (evaluations)   4808.070382   \n18                                          Gender   3937.830231   \n20                               Age at enrollment   3904.168074   \n2                                 Application mode   3883.817076   \n24          Curricular units 1st sem (evaluations)   3210.950614   \n29             Curricular units 2nd sem (enrolled)   3015.335349   \n23             Curricular units 1st sem (enrolled)   2551.490819   \n16                                          Debtor   2181.472215   \n13                                 Admission grade   1071.274536   \n4                                           Course   1025.733706   \n7                   Previous qualification (grade)    771.824039   \n14                                       Displaced    632.950767   \n1                                   Marital status    571.976474   \n3                                Application order    555.711913   \n5                       Daytime/evening attendance    481.899373   \n36                                             GDP    337.796318   \n6                           Previous qualification    296.417955   \n9                           Mother's qualification    266.239332   \n34                               Unemployment rate    173.607605   \n27  Curricular units 1st sem (without evaluations)    128.079142   \n11                             Mother's occupation    114.305871   \n35                                  Inflation rate     91.890424   \n12                             Father's occupation     72.934625   \n33  Curricular units 2nd sem (without evaluations)     70.150994   \n28             Curricular units 2nd sem (credited)     26.861119   \n22             Curricular units 1st sem (credited)     16.989620   \n10                          Father's qualification      4.103516   \n8                                      Nacionality      2.520930   \n21                                   International      1.757546   \n15                       Educational special needs      0.961615   \n0                                               id      0.015520   \n\n          p_value  \n31   0.000000e+00  \n32   0.000000e+00  \n25   0.000000e+00  \n26   0.000000e+00  \n17   0.000000e+00  \n19   0.000000e+00  \n30   0.000000e+00  \n18   0.000000e+00  \n20   0.000000e+00  \n2    0.000000e+00  \n24   0.000000e+00  \n29   0.000000e+00  \n23   0.000000e+00  \n16   0.000000e+00  \n13   0.000000e+00  \n4    0.000000e+00  \n7    0.000000e+00  \n14  7.241510e-273  \n1   6.914569e-247  \n3   6.000432e-240  \n5   2.046752e-208  \n36  1.213836e-146  \n6   7.482261e-129  \n9   7.303037e-116  \n34   6.483917e-76  \n27   3.087560e-56  \n11   2.806482e-50  \n35   1.415976e-40  \n12   2.300390e-32  \n33   3.697904e-31  \n28   2.184616e-12  \n22   4.202492e-08  \n10   1.651895e-02  \n8    8.039297e-02  \n21   1.724761e-01  \n15   3.822805e-01  \n0    9.845999e-01  \n</pre> In\u00a0[20]: Copied! <pre>from sklearn.preprocessing import MinMaxScaler\n\ntarget = \"Target\"\n\ndf_encoded = pd.get_dummies(df, columns=[\"Nacionality\",\"Marital status\",\"Application mode\",\"Course\", \"Previous qualification\", \"Mother's qualification\",\"Father's qualification\",\"Mother's occupation\",\"Father's occupation\" ], drop_first=False)  \nnumeric_cols = df_encoded.select_dtypes(include=['int64', 'float64', 'uint8','boolean']).columns\n\nscaler = MinMaxScaler()\ndf_scaled = df_encoded.copy()\n\ndf_scaled[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])\n\ntarget_map = {\"Graduate\": 1, \"Dropout\": 0.5, \"Enrolled\": 0}\ndf_scaled['Target'] = df_scaled['Target'].map(target_map)\n\n\nprint(\"Dataset com One-Hot Encoding e Min-Max Normalization:\")\ndf_scaled.head()\n</pre> from sklearn.preprocessing import MinMaxScaler  target = \"Target\"  df_encoded = pd.get_dummies(df, columns=[\"Nacionality\",\"Marital status\",\"Application mode\",\"Course\", \"Previous qualification\", \"Mother's qualification\",\"Father's qualification\",\"Mother's occupation\",\"Father's occupation\" ], drop_first=False)   numeric_cols = df_encoded.select_dtypes(include=['int64', 'float64', 'uint8','boolean']).columns  scaler = MinMaxScaler() df_scaled = df_encoded.copy()  df_scaled[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])  target_map = {\"Graduate\": 1, \"Dropout\": 0.5, \"Enrolled\": 0} df_scaled['Target'] = df_scaled['Target'].map(target_map)   print(\"Dataset com One-Hot Encoding e Min-Max Normalization:\") df_scaled.head() <pre>Dataset com One-Hot Encoding e Min-Max Normalization:\n</pre> Out[20]: id Application order Daytime/evening attendance Previous qualification (grade) Admission grade Displaced Educational special needs Debtor Tuition fees up to date Gender ... Father's occupation_171 Father's occupation_172 Father's occupation_174 Father's occupation_175 Father's occupation_181 Father's occupation_183 Father's occupation_192 Father's occupation_193 Father's occupation_194 Father's occupation_195 0 0.000000 0.111111 1.0 0.364706 0.324706 0.0 0.0 0.0 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 0.000013 0.111111 1.0 0.352941 0.291765 1.0 0.0 0.0 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 0.000026 0.222222 1.0 0.494118 0.584706 0.0 0.0 0.0 1.0 1.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3 0.000039 0.333333 1.0 0.423529 0.365882 1.0 0.0 0.0 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 4 0.000052 0.222222 1.0 0.435294 0.295294 1.0 0.0 0.0 1.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 <p>5 rows \u00d7 269 columns</p> In\u00a0[21]: Copied! <pre>from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nn_components = 2\npca = PCA(n_components=n_components)\npca_result = pca.fit_transform(df_scaled[numeric_cols])\n\n\ndf_pca = pd.DataFrame(pca_result, columns=[f\"PC{i+1}\" for i in range(n_components)])\n\n\ndf_pca[target] = df[target]\n\nprint(\"\ud83d\udd39 Vari\u00e2ncia explicada por cada componente:\")\nfor i, var in enumerate(pca.explained_variance_ratio_):\n    print(f\"PC{i+1}: {var:.2%}\")\nprint(f\"\ud83d\udd39 Vari\u00e2ncia total explicada: {pca.explained_variance_ratio_.sum():.2%}\")\n\n\nplt.figure(figsize=(8,6))\nfor classe in df[target].unique():\n    subset = df_pca[df_pca[target] == classe]\n    plt.scatter(subset[\"PC1\"], subset[\"PC2\"], label=str(classe))\n\nplt.title(\"PCA - Proje\u00e7\u00e3o dos Dados (PC1 vs PC2)\")\nplt.xlabel(\"Componente Principal 1\")\nplt.ylabel(\"Componente Principal 2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n</pre> from sklearn.decomposition import PCA import matplotlib.pyplot as plt  n_components = 2 pca = PCA(n_components=n_components) pca_result = pca.fit_transform(df_scaled[numeric_cols])   df_pca = pd.DataFrame(pca_result, columns=[f\"PC{i+1}\" for i in range(n_components)])   df_pca[target] = df[target]  print(\"\ud83d\udd39 Vari\u00e2ncia explicada por cada componente:\") for i, var in enumerate(pca.explained_variance_ratio_):     print(f\"PC{i+1}: {var:.2%}\") print(f\"\ud83d\udd39 Vari\u00e2ncia total explicada: {pca.explained_variance_ratio_.sum():.2%}\")   plt.figure(figsize=(8,6)) for classe in df[target].unique():     subset = df_pca[df_pca[target] == classe]     plt.scatter(subset[\"PC1\"], subset[\"PC2\"], label=str(classe))  plt.title(\"PCA - Proje\u00e7\u00e3o dos Dados (PC1 vs PC2)\") plt.xlabel(\"Componente Principal 1\") plt.ylabel(\"Componente Principal 2\") plt.legend() plt.grid(True) plt.show() <pre>\ud83d\udd39 Vari\u00e2ncia explicada por cada componente:\nPC1: 10.69%\nPC2: 7.68%\n\ud83d\udd39 Vari\u00e2ncia total explicada: 18.37%\n</pre> In\u00a0[22]: Copied! <pre>df_scaled.isnull().sum()\n</pre> df_scaled.isnull().sum() Out[22]: <pre>id                                0\nApplication order                 0\nDaytime/evening attendance        0\nPrevious qualification (grade)    0\nAdmission grade                   0\n                                 ..\nFather's occupation_183           0\nFather's occupation_192           0\nFather's occupation_193           0\nFather's occupation_194           0\nFather's occupation_195           0\nLength: 269, dtype: int64</pre> In\u00a0[23]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n# ------------------------------------------------------------------\n# 0) Partindo de df_scaled: features normalizadas [0,1] + coluna \"Target\" {0, 0.5, 1}\n# ------------------------------------------------------------------\n\n# (a) mapear r\u00f3tulos reais -&gt; \u00edndices 0..K-1\nclasses = np.sort(df_scaled[\"Target\"].unique())             \nclass_to_idx = {c: i for i, c in enumerate(classes)}        \nidx_to_class = {i: c for c, i in class_to_idx.items()}\n\nY = df_scaled[\"Target\"].map(class_to_idx).to_numpy(dtype=int)\nX = df_scaled.drop(columns=\"Target\").to_numpy(dtype=float)\n\n# ------------------------------------------------------------------\n# 1) SPLIT 70/15/15 (train / val / test)\n# ------------------------------------------------------------------\nTEST_RATIO = 0.15\nVAL_RATIO  = 0.15\nVAL_REL = VAL_RATIO / (1.0 - TEST_RATIO)  \n\nX_rem, X_test, y_rem, y_test = train_test_split(\n    X, Y, test_size=TEST_RATIO, random_state=42, stratify=Y\n)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_rem, y_rem, test_size=VAL_REL, random_state=42, stratify=y_rem\n)\n\nN, D = X_train.shape\nnum_classes = len(classes)\n\ndef one_hot(y, K):\n    eye = np.eye(K, dtype=float)\n    return eye[y]  \n\n# ------------------------------------------------------------------\n# 2) MLP: inicializa\u00e7\u00e3o\n# ------------------------------------------------------------------\nrng = np.random.default_rng(42)\nH1, H2 = 64, 32\n\nW1 = rng.normal(0, 1, (H1, D)) / np.sqrt(D)\nb1 = np.zeros(H1)\n\nW2 = rng.normal(0, 1, (H2, H1)) / np.sqrt(H1)\nb2 = np.zeros(H2)\n\nW3 = rng.normal(0, 1, (num_classes, H2)) / np.sqrt(H2)\nb3 = np.zeros(num_classes)\n\ndef tanh(x): \n    return np.tanh(x)\n\ndef softmax(x):\n    x = x - x.max(axis=1, keepdims=True)\n    e = np.exp(x)\n    return e / e.sum(axis=1, keepdims=True)\n\ndef forward(X):\n    h1 = tanh(X @ W1.T + b1)\n    h2 = tanh(h1 @ W2.T + b2)\n    yhat = softmax(h2 @ W3.T + b3)\n    return yhat\n\n# ------------------------------------------------------------------\n# 3) Treino com mini-batch + Early Stopping + L2 + logs\n# ------------------------------------------------------------------\nlr = 0.005\nepochs = 500\nbatch = 128\nlambda_l2 = 1e-4\n\ntrain_losses = []\nval_losses = []\ntrain_accs = []\nval_accs = []\n\npatience = 15\nbest_val = np.inf\nbad_epochs = 0\n\nbest_W1, best_b1 = W1.copy(), b1.copy()\nbest_W2, best_b2 = W2.copy(), b2.copy()\nbest_W3, best_b3 = W3.copy(), b3.copy()\n\nfor ep in range(1, epochs + 1):\n    idx = rng.permutation(N)\n    X_train = X_train[idx]\n    y_train = y_train[idx]\n    Y_oh = one_hot(y_train, num_classes)\n\n    # ---- treino (mini-batch)\n    epoch_loss = 0\n    for i in range(0, N, batch):\n        xb = X_train[i:i+batch]\n        yb = Y_oh[i:i+batch]\n        B  = xb.shape[0]\n\n        # ================================= Forward Pass se\u00e7\u00e3o Numerical Simulation =================================\n        z1 = xb @ W1.T + b1;  h1 = tanh(z1)\n        z2 = h1 @ W2.T + b2;  h2 = tanh(z2)\n        z3 = h2 @ W3.T + b3;  yhat = softmax(z3)\n        # ===========================================================================================================\n\n        # ================================= Loss Calculation se\u00e7\u00e3o Numerical Simulation + L2 =============================\n        loss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))\n        l2_term = lambda_l2 * (np.sum(W1*W1) + np.sum(W2*W2) + np.sum(W3*W3))\n        loss = loss_ce + l2_term\n        epoch_loss += loss * B\n        # ===========================================================================================================\n\n        # ================================= Backward Pass se\u00e7\u00e3o Numerical Simulation ================================\n        d3  = (yhat - yb) / B # Output layer gradients: dL/dz3 com a fun\u00e7\u00e3o softmax e cross-entropy (A fun\u00e7\u00e3o de Loss est\u00e1 sendo usada aqui)\n        gW3 = d3.T @ h2 + 2*lambda_l2*W3 # Weight gradients: dL/dW3\n        gb3 = d3.sum(axis=0) # Bias gradients: dL/db3\n\n        dh2 = d3 @ W3 # dL/dh2\n        dz2 = dh2 * (1 - h2**2) # Output layer gradients: dL/dz2 = dL/dh2 \u22c5 (1 - tanh(z2)^2) com a fun\u00e7\u00e3o tanh\n        gW2 = dz2.T @ h1 + 2*lambda_l2*W2 # Weight gradients: dL/dW2\n        gb2 = dz2.sum(axis=0) # Bias gradients: dL/db2\n\n        dh1 = dz2 @ W2 # dL/dh1\n        dz1 = dh1 * (1 - h1**2) # dL/dz1 = dL/dh1 \u22c5 (1 - tanh(z1)^2) com a fun\u00e7\u00e3o tanh\n        gW1 = dz1.T @ xb + 2*lambda_l2*W1 # Weight gradients: dL/dW1\n        gb1 = dz1.sum(axis=0) # Bias gradients: dL/db1\n\n        # update dos par\u00e2metros\n        W3 -= lr * gW3\n        b3 -= lr * gb3\n        W2 -= lr * gW2\n        b2 -= lr * gb2\n        W1 -= lr * gW1\n        b1 -= lr * gb1\n        # ===========================================================================================================\n\n    epoch_loss /= N\n    yhat_train = forward(X_train)\n    train_acc = (np.argmax(yhat_train, axis=1) == y_train).mean()\n\n    # valida\u00e7\u00e3o\n    yhat_val = forward(X_val)\n    Y_val_oh = one_hot(y_val, num_classes)\n    val_loss = -np.mean(np.sum(Y_val_oh * np.log(yhat_val + 1e-9), axis=1))\n    val_acc = (np.argmax(yhat_val, axis=1) == y_val).mean()\n\n    train_losses.append(epoch_loss)\n    val_losses.append(val_loss)\n    train_accs.append(train_acc)\n    val_accs.append(val_acc)\n\n    improved = val_loss &lt; best_val - 1e-6\n    if improved:\n        best_val = val_loss\n        bad_epochs = 0\n        best_W1, best_b1 = W1.copy(), b1.copy()\n        best_W2, best_b2 = W2.copy(), b2.copy()\n        best_W3, best_b3 = W3.copy(), b3.copy()\n    else:\n        bad_epochs += 1\n        if bad_epochs &gt;= patience:\n            print(f\"\u23f9\ufe0f Early stopping (\u00e9poca {ep}) melhor val_loss={best_val:.4f}\")\n            break\n\n    if ep % 10 == 0 or ep == 1:\n        print(f\"\u00c9poca {ep:3d} | TrainLoss {epoch_loss:.4f} | ValLoss {val_loss:.4f} | TrainAcc {train_acc:.3f} | ValAcc {val_acc:.3f}\")\n\n# restaura melhor checkpoint\nW1, b1 = best_W1, best_b1\nW2, b2 = best_W2, best_b2\nW3, b3 = best_W3, best_b3\n\n# ------------------------------------------------------------------\n# 4) Curvas de erro e acur\u00e1cia\n# ------------------------------------------------------------------\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Loss\"); plt.title(\"Curva de Loss\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(train_accs, label=\"Train Acc\")\nplt.plot(val_accs, label=\"Val Acc\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\"); plt.title(\"Curva de Acur\u00e1cia\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# ------------------------------------------------------------------\n# 5) Avalia\u00e7\u00e3o FINAL no TESTE\n# ------------------------------------------------------------------\n\n\nyhat_test = forward(X_test)\nypred_test = np.argmax(yhat_test, axis=1)\n\nacc_test = accuracy_score(y_test, ypred_test)\nprec_test = precision_score(y_test, ypred_test, average='macro')\nrec_test = recall_score(y_test, ypred_test, average='macro')\nf1_test = f1_score(y_test, ypred_test, average='macro')\n\nprint(\"\\n\ud83d\udcca M\u00c9TRICAS TESTE\")\nprint(f\"Acur\u00e1cia : {acc_test:.4f}\")\nprint(f\"Precis\u00e3o : {prec_test:.4f}\")\nprint(f\"Recall   : {rec_test:.4f}\")\nprint(f\"F1-score : {f1_test:.4f}\")\n\n# ------------------------------------------------------------------\n# 6) Matriz de Confus\u00e3o\n# ------------------------------------------------------------------\ncm = confusion_matrix(y_test, ypred_test)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\ndisp.plot(cmap=\"Blues\")\nplt.title(\"Matriz de Confus\u00e3o (Teste)\")\nplt.show()\n\n# ------------------------------------------------------------------\n# 7) Decision Boundary (PCA 2D) \u2014 APENAS TESTE (mant\u00e9m o gr\u00e1fico desejado)\n# ------------------------------------------------------------------\nfrom matplotlib.patches import Patch\n\n# PCA \u00e9 ajustado no TRAIN e aplicado no TESTE\npca = PCA(n_components=2, random_state=42)\nX_train_p = pca.fit_transform(X_train)\nX_test_p  = pca.transform(X_test)\n\n# grade no espa\u00e7o PCA\nx_min, x_max = X_train_p[:, 0].min() - 0.2, X_train_p[:, 0].max() + 0.2\ny_min, y_max = X_train_p[:, 1].min() - 0.2, X_train_p[:, 1].max() + 0.2\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 350),\n    np.linspace(y_min, y_max, 350),\n)\ngrid_pca  = np.c_[xx.ravel(), yy.ravel()]\ngrid_full = pca.inverse_transform(grid_pca)\n\n# predi\u00e7\u00e3o da malha e do teste\ny_grid     = forward(grid_full)\ny_grid_cls = np.argmax(y_grid, axis=1).reshape(xx.shape)\n\nyhat_test   = forward(X_test)\ny_pred_test = np.argmax(yhat_test, axis=1)\ncorrect     = (y_pred_test == y_test)\nerrors      = ~correct\n\nplt.figure(figsize=(8, 6))\ncmap = plt.cm.Set1 if num_classes &lt;= 9 else plt.cm.tab20\n\n# fronteira\nplt.contourf(\n    xx, yy, y_grid_cls,\n    levels=np.arange(num_classes+1)-0.5, alpha=0.30, cmap=cmap, antialiased=True\n)\n\n# pontos do TESTE:\n# - bolinha (o) para corretos\n# - X para errados\nplt.scatter(\n    X_test_p[correct, 0], X_test_p[correct, 1],\n    c=y_test[correct], cmap=cmap, edgecolors=\"k\",\n    s=35, marker=\"o\", label=\"Corretos\"\n)\nplt.scatter(\n    X_test_p[errors, 0], X_test_p[errors, 1],\n    c=y_test[errors], cmap=cmap, edgecolors=\"k\",\n    s=60, marker=\"x\", label=\"Errados\"\n)\n\n# legenda com classes + corretos/errados\nclass_handles = [\n    Patch(color=cmap(i / max(3, num_classes)), label=f\"Classe {classes[i]}\")\n    for i in range(num_classes)\n]\nplt.legend(handles=class_handles + [\n    plt.Line2D([0], [0], marker=\"o\", color=\"w\",\n               markerfacecolor=\"gray\", markeredgecolor=\"k\",\n               label=\"Corretos\", markersize=8),\n    plt.Line2D([0], [0], marker=\"x\", color=\"k\", label=\"Errados\", markersize=8)\n], loc=\"upper right\")\n\nplt.title(f\"Decision Boundary (PCA 2D)\\nAcur\u00e1cia teste: {accuracy_score(y_test, y_pred_test):.4f}\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.tight_layout()\nplt.show()\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay  # ------------------------------------------------------------------ # 0) Partindo de df_scaled: features normalizadas [0,1] + coluna \"Target\" {0, 0.5, 1} # ------------------------------------------------------------------  # (a) mapear r\u00f3tulos reais -&gt; \u00edndices 0..K-1 classes = np.sort(df_scaled[\"Target\"].unique())              class_to_idx = {c: i for i, c in enumerate(classes)}         idx_to_class = {i: c for c, i in class_to_idx.items()}  Y = df_scaled[\"Target\"].map(class_to_idx).to_numpy(dtype=int) X = df_scaled.drop(columns=\"Target\").to_numpy(dtype=float)  # ------------------------------------------------------------------ # 1) SPLIT 70/15/15 (train / val / test) # ------------------------------------------------------------------ TEST_RATIO = 0.15 VAL_RATIO  = 0.15 VAL_REL = VAL_RATIO / (1.0 - TEST_RATIO)    X_rem, X_test, y_rem, y_test = train_test_split(     X, Y, test_size=TEST_RATIO, random_state=42, stratify=Y ) X_train, X_val, y_train, y_val = train_test_split(     X_rem, y_rem, test_size=VAL_REL, random_state=42, stratify=y_rem )  N, D = X_train.shape num_classes = len(classes)  def one_hot(y, K):     eye = np.eye(K, dtype=float)     return eye[y]    # ------------------------------------------------------------------ # 2) MLP: inicializa\u00e7\u00e3o # ------------------------------------------------------------------ rng = np.random.default_rng(42) H1, H2 = 64, 32  W1 = rng.normal(0, 1, (H1, D)) / np.sqrt(D) b1 = np.zeros(H1)  W2 = rng.normal(0, 1, (H2, H1)) / np.sqrt(H1) b2 = np.zeros(H2)  W3 = rng.normal(0, 1, (num_classes, H2)) / np.sqrt(H2) b3 = np.zeros(num_classes)  def tanh(x):      return np.tanh(x)  def softmax(x):     x = x - x.max(axis=1, keepdims=True)     e = np.exp(x)     return e / e.sum(axis=1, keepdims=True)  def forward(X):     h1 = tanh(X @ W1.T + b1)     h2 = tanh(h1 @ W2.T + b2)     yhat = softmax(h2 @ W3.T + b3)     return yhat  # ------------------------------------------------------------------ # 3) Treino com mini-batch + Early Stopping + L2 + logs # ------------------------------------------------------------------ lr = 0.005 epochs = 500 batch = 128 lambda_l2 = 1e-4  train_losses = [] val_losses = [] train_accs = [] val_accs = []  patience = 15 best_val = np.inf bad_epochs = 0  best_W1, best_b1 = W1.copy(), b1.copy() best_W2, best_b2 = W2.copy(), b2.copy() best_W3, best_b3 = W3.copy(), b3.copy()  for ep in range(1, epochs + 1):     idx = rng.permutation(N)     X_train = X_train[idx]     y_train = y_train[idx]     Y_oh = one_hot(y_train, num_classes)      # ---- treino (mini-batch)     epoch_loss = 0     for i in range(0, N, batch):         xb = X_train[i:i+batch]         yb = Y_oh[i:i+batch]         B  = xb.shape[0]          # ================================= Forward Pass se\u00e7\u00e3o Numerical Simulation =================================         z1 = xb @ W1.T + b1;  h1 = tanh(z1)         z2 = h1 @ W2.T + b2;  h2 = tanh(z2)         z3 = h2 @ W3.T + b3;  yhat = softmax(z3)         # ===========================================================================================================          # ================================= Loss Calculation se\u00e7\u00e3o Numerical Simulation + L2 =============================         loss_ce = -np.mean(np.sum(yb * np.log(yhat + 1e-9), axis=1))         l2_term = lambda_l2 * (np.sum(W1*W1) + np.sum(W2*W2) + np.sum(W3*W3))         loss = loss_ce + l2_term         epoch_loss += loss * B         # ===========================================================================================================          # ================================= Backward Pass se\u00e7\u00e3o Numerical Simulation ================================         d3  = (yhat - yb) / B # Output layer gradients: dL/dz3 com a fun\u00e7\u00e3o softmax e cross-entropy (A fun\u00e7\u00e3o de Loss est\u00e1 sendo usada aqui)         gW3 = d3.T @ h2 + 2*lambda_l2*W3 # Weight gradients: dL/dW3         gb3 = d3.sum(axis=0) # Bias gradients: dL/db3          dh2 = d3 @ W3 # dL/dh2         dz2 = dh2 * (1 - h2**2) # Output layer gradients: dL/dz2 = dL/dh2 \u22c5 (1 - tanh(z2)^2) com a fun\u00e7\u00e3o tanh         gW2 = dz2.T @ h1 + 2*lambda_l2*W2 # Weight gradients: dL/dW2         gb2 = dz2.sum(axis=0) # Bias gradients: dL/db2          dh1 = dz2 @ W2 # dL/dh1         dz1 = dh1 * (1 - h1**2) # dL/dz1 = dL/dh1 \u22c5 (1 - tanh(z1)^2) com a fun\u00e7\u00e3o tanh         gW1 = dz1.T @ xb + 2*lambda_l2*W1 # Weight gradients: dL/dW1         gb1 = dz1.sum(axis=0) # Bias gradients: dL/db1          # update dos par\u00e2metros         W3 -= lr * gW3         b3 -= lr * gb3         W2 -= lr * gW2         b2 -= lr * gb2         W1 -= lr * gW1         b1 -= lr * gb1         # ===========================================================================================================      epoch_loss /= N     yhat_train = forward(X_train)     train_acc = (np.argmax(yhat_train, axis=1) == y_train).mean()      # valida\u00e7\u00e3o     yhat_val = forward(X_val)     Y_val_oh = one_hot(y_val, num_classes)     val_loss = -np.mean(np.sum(Y_val_oh * np.log(yhat_val + 1e-9), axis=1))     val_acc = (np.argmax(yhat_val, axis=1) == y_val).mean()      train_losses.append(epoch_loss)     val_losses.append(val_loss)     train_accs.append(train_acc)     val_accs.append(val_acc)      improved = val_loss &lt; best_val - 1e-6     if improved:         best_val = val_loss         bad_epochs = 0         best_W1, best_b1 = W1.copy(), b1.copy()         best_W2, best_b2 = W2.copy(), b2.copy()         best_W3, best_b3 = W3.copy(), b3.copy()     else:         bad_epochs += 1         if bad_epochs &gt;= patience:             print(f\"\u23f9\ufe0f Early stopping (\u00e9poca {ep}) melhor val_loss={best_val:.4f}\")             break      if ep % 10 == 0 or ep == 1:         print(f\"\u00c9poca {ep:3d} | TrainLoss {epoch_loss:.4f} | ValLoss {val_loss:.4f} | TrainAcc {train_acc:.3f} | ValAcc {val_acc:.3f}\")  # restaura melhor checkpoint W1, b1 = best_W1, best_b1 W2, b2 = best_W2, best_b2 W3, b3 = best_W3, best_b3  # ------------------------------------------------------------------ # 4) Curvas de erro e acur\u00e1cia # ------------------------------------------------------------------ plt.figure(figsize=(10,4)) plt.subplot(1,2,1) plt.plot(train_losses, label=\"Train Loss\") plt.plot(val_losses, label=\"Val Loss\") plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Loss\"); plt.title(\"Curva de Loss\") plt.legend()  plt.subplot(1,2,2) plt.plot(train_accs, label=\"Train Acc\") plt.plot(val_accs, label=\"Val Acc\") plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\"); plt.title(\"Curva de Acur\u00e1cia\") plt.legend() plt.tight_layout() plt.show()  # ------------------------------------------------------------------ # 5) Avalia\u00e7\u00e3o FINAL no TESTE # ------------------------------------------------------------------   yhat_test = forward(X_test) ypred_test = np.argmax(yhat_test, axis=1)  acc_test = accuracy_score(y_test, ypred_test) prec_test = precision_score(y_test, ypred_test, average='macro') rec_test = recall_score(y_test, ypred_test, average='macro') f1_test = f1_score(y_test, ypred_test, average='macro')  print(\"\\n\ud83d\udcca M\u00c9TRICAS TESTE\") print(f\"Acur\u00e1cia : {acc_test:.4f}\") print(f\"Precis\u00e3o : {prec_test:.4f}\") print(f\"Recall   : {rec_test:.4f}\") print(f\"F1-score : {f1_test:.4f}\")  # ------------------------------------------------------------------ # 6) Matriz de Confus\u00e3o # ------------------------------------------------------------------ cm = confusion_matrix(y_test, ypred_test) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes) disp.plot(cmap=\"Blues\") plt.title(\"Matriz de Confus\u00e3o (Teste)\") plt.show()  # ------------------------------------------------------------------ # 7) Decision Boundary (PCA 2D) \u2014 APENAS TESTE (mant\u00e9m o gr\u00e1fico desejado) # ------------------------------------------------------------------ from matplotlib.patches import Patch  # PCA \u00e9 ajustado no TRAIN e aplicado no TESTE pca = PCA(n_components=2, random_state=42) X_train_p = pca.fit_transform(X_train) X_test_p  = pca.transform(X_test)  # grade no espa\u00e7o PCA x_min, x_max = X_train_p[:, 0].min() - 0.2, X_train_p[:, 0].max() + 0.2 y_min, y_max = X_train_p[:, 1].min() - 0.2, X_train_p[:, 1].max() + 0.2 xx, yy = np.meshgrid(     np.linspace(x_min, x_max, 350),     np.linspace(y_min, y_max, 350), ) grid_pca  = np.c_[xx.ravel(), yy.ravel()] grid_full = pca.inverse_transform(grid_pca)  # predi\u00e7\u00e3o da malha e do teste y_grid     = forward(grid_full) y_grid_cls = np.argmax(y_grid, axis=1).reshape(xx.shape)  yhat_test   = forward(X_test) y_pred_test = np.argmax(yhat_test, axis=1) correct     = (y_pred_test == y_test) errors      = ~correct  plt.figure(figsize=(8, 6)) cmap = plt.cm.Set1 if num_classes &lt;= 9 else plt.cm.tab20  # fronteira plt.contourf(     xx, yy, y_grid_cls,     levels=np.arange(num_classes+1)-0.5, alpha=0.30, cmap=cmap, antialiased=True )  # pontos do TESTE: # - bolinha (o) para corretos # - X para errados plt.scatter(     X_test_p[correct, 0], X_test_p[correct, 1],     c=y_test[correct], cmap=cmap, edgecolors=\"k\",     s=35, marker=\"o\", label=\"Corretos\" ) plt.scatter(     X_test_p[errors, 0], X_test_p[errors, 1],     c=y_test[errors], cmap=cmap, edgecolors=\"k\",     s=60, marker=\"x\", label=\"Errados\" )  # legenda com classes + corretos/errados class_handles = [     Patch(color=cmap(i / max(3, num_classes)), label=f\"Classe {classes[i]}\")     for i in range(num_classes) ] plt.legend(handles=class_handles + [     plt.Line2D([0], [0], marker=\"o\", color=\"w\",                markerfacecolor=\"gray\", markeredgecolor=\"k\",                label=\"Corretos\", markersize=8),     plt.Line2D([0], [0], marker=\"x\", color=\"k\", label=\"Errados\", markersize=8) ], loc=\"upper right\")  plt.title(f\"Decision Boundary (PCA 2D)\\nAcur\u00e1cia teste: {accuracy_score(y_test, y_pred_test):.4f}\") plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\") plt.tight_layout() plt.show()  <pre>\u00c9poca   1 | TrainLoss 0.9766 | ValLoss 0.8791 | TrainAcc 0.654 | ValAcc 0.653\n\u00c9poca  10 | TrainLoss 0.5499 | ValLoss 0.5394 | TrainAcc 0.793 | ValAcc 0.791\n\u00c9poca  20 | TrainLoss 0.4953 | ValLoss 0.4857 | TrainAcc 0.813 | ValAcc 0.811\n\u00c9poca  30 | TrainLoss 0.4782 | ValLoss 0.4684 | TrainAcc 0.820 | ValAcc 0.817\n\u00c9poca  40 | TrainLoss 0.4727 | ValLoss 0.4629 | TrainAcc 0.822 | ValAcc 0.820\n\u00c9poca  50 | TrainLoss 0.4704 | ValLoss 0.4601 | TrainAcc 0.824 | ValAcc 0.821\n\u00c9poca  60 | TrainLoss 0.4691 | ValLoss 0.4588 | TrainAcc 0.824 | ValAcc 0.823\n\u00c9poca  70 | TrainLoss 0.4679 | ValLoss 0.4587 | TrainAcc 0.824 | ValAcc 0.823\n\u00c9poca  80 | TrainLoss 0.4671 | ValLoss 0.4566 | TrainAcc 0.825 | ValAcc 0.823\n\u00c9poca  90 | TrainLoss 0.4666 | ValLoss 0.4562 | TrainAcc 0.824 | ValAcc 0.823\n\u00c9poca 100 | TrainLoss 0.4659 | ValLoss 0.4561 | TrainAcc 0.825 | ValAcc 0.823\n\u00c9poca 110 | TrainLoss 0.4654 | ValLoss 0.4558 | TrainAcc 0.824 | ValAcc 0.823\n\u00c9poca 120 | TrainLoss 0.4651 | ValLoss 0.4548 | TrainAcc 0.825 | ValAcc 0.824\n\u00c9poca 130 | TrainLoss 0.4648 | ValLoss 0.4548 | TrainAcc 0.825 | ValAcc 0.824\n\u00c9poca 140 | TrainLoss 0.4644 | ValLoss 0.4544 | TrainAcc 0.825 | ValAcc 0.823\n\u00c9poca 150 | TrainLoss 0.4639 | ValLoss 0.4545 | TrainAcc 0.826 | ValAcc 0.823\n\u00c9poca 160 | TrainLoss 0.4633 | ValLoss 0.4566 | TrainAcc 0.825 | ValAcc 0.824\n\u00c9poca 170 | TrainLoss 0.4631 | ValLoss 0.4545 | TrainAcc 0.825 | ValAcc 0.823\n\u00c9poca 180 | TrainLoss 0.4629 | ValLoss 0.4549 | TrainAcc 0.826 | ValAcc 0.824\n\u00c9poca 190 | TrainLoss 0.4624 | ValLoss 0.4540 | TrainAcc 0.826 | ValAcc 0.824\n\u00c9poca 200 | TrainLoss 0.4619 | ValLoss 0.4555 | TrainAcc 0.826 | ValAcc 0.823\n\u00c9poca 210 | TrainLoss 0.4615 | ValLoss 0.4534 | TrainAcc 0.825 | ValAcc 0.823\n\u23f9\ufe0f Early stopping (\u00e9poca 219) melhor val_loss=0.4528\n</pre> <pre>\ud83d\udcca M\u00c9TRICAS TESTE\nAcur\u00e1cia : 0.8195\nPrecis\u00e3o : 0.7897\nRecall   : 0.7690\nF1-score : 0.7771\n</pre> <pre>C:\\Users\\henri\\AppData\\Local\\Temp\\ipykernel_27688\\1451932628.py:268: UserWarning: You passed a edgecolor/edgecolors ('k') for an unfilled marker ('x').  Matplotlib is ignoring the edgecolor in favor of the facecolor.  This behavior may change in the future.\n  plt.scatter(\n</pre> In\u00a0[24]: Copied! <pre>import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n\n# Classe mais frequente no conjunto de TESTE\nunique, counts = np.unique(y_test, return_counts=True)\nmajority_class = unique[np.argmax(counts)]\nprint(\"Classe majorit\u00e1ria:\", majority_class)\n</pre> import numpy as np from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay  # Classe mais frequente no conjunto de TESTE unique, counts = np.unique(y_test, return_counts=True) majority_class = unique[np.argmax(counts)] print(\"Classe majorit\u00e1ria:\", majority_class) <pre>Classe majorit\u00e1ria: 2\n</pre> In\u00a0[25]: Copied! <pre>y_pred_baseline = np.full_like(y_test, fill_value=majority_class)\n</pre> y_pred_baseline = np.full_like(y_test, fill_value=majority_class) In\u00a0[26]: Copied! <pre>acc_base = accuracy_score(y_test, y_pred_baseline)\nprec_base = precision_score(y_test, y_pred_baseline, average='macro', zero_division=0)\nrec_base = recall_score(y_test, y_pred_baseline, average='macro', zero_division=0)\nf1_base = f1_score(y_test, y_pred_baseline, average='macro', zero_division=0)\n\nprint(\"\\n\ud83d\udcca M\u00c9TRICAS BASELINE (Majority Class)\")\nprint(f\"Acur\u00e1cia : {acc_base:.4f}\")\nprint(f\"Precis\u00e3o : {prec_base:.4f}\")\nprint(f\"Recall   : {rec_base:.4f}\")\nprint(f\"F1-score : {f1_base:.4f}\")\n</pre> acc_base = accuracy_score(y_test, y_pred_baseline) prec_base = precision_score(y_test, y_pred_baseline, average='macro', zero_division=0) rec_base = recall_score(y_test, y_pred_baseline, average='macro', zero_division=0) f1_base = f1_score(y_test, y_pred_baseline, average='macro', zero_division=0)  print(\"\\n\ud83d\udcca M\u00c9TRICAS BASELINE (Majority Class)\") print(f\"Acur\u00e1cia : {acc_base:.4f}\") print(f\"Precis\u00e3o : {prec_base:.4f}\") print(f\"Recall   : {rec_base:.4f}\") print(f\"F1-score : {f1_base:.4f}\") <pre>\ud83d\udcca M\u00c9TRICAS BASELINE (Majority Class)\nAcur\u00e1cia : 0.4708\nPrecis\u00e3o : 0.1569\nRecall   : 0.3333\nF1-score : 0.2134\n</pre> In\u00a0[27]: Copied! <pre>print(\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O\")\nprint(f\"{'M\u00e9trica':&lt;12}{'Modelo':&gt;12}{'Baseline':&gt;12}\")\nprint(f\"{'Acur\u00e1cia':&lt;12}{acc_test:&gt;12.4f}{acc_base:&gt;12.4f}\")\nprint(f\"{'Precis\u00e3o':&lt;12}{prec_test:&gt;12.4f}{prec_base:&gt;12.4f}\")\nprint(f\"{'Recall':&lt;12}{rec_test:&gt;12.4f}{rec_base:&gt;12.4f}\")\nprint(f\"{'F1-score':&lt;12}{f1_test:&gt;12.4f}{f1_base:&gt;12.4f}\")\n</pre> print(\"\\n\ud83d\udcca COMPARA\u00c7\u00c3O\") print(f\"{'M\u00e9trica':&lt;12}{'Modelo':&gt;12}{'Baseline':&gt;12}\") print(f\"{'Acur\u00e1cia':&lt;12}{acc_test:&gt;12.4f}{acc_base:&gt;12.4f}\") print(f\"{'Precis\u00e3o':&lt;12}{prec_test:&gt;12.4f}{prec_base:&gt;12.4f}\") print(f\"{'Recall':&lt;12}{rec_test:&gt;12.4f}{rec_base:&gt;12.4f}\") print(f\"{'F1-score':&lt;12}{f1_test:&gt;12.4f}{f1_base:&gt;12.4f}\") <pre>\ud83d\udcca COMPARA\u00c7\u00c3O\nM\u00e9trica           Modelo    Baseline\nAcur\u00e1cia          0.8195      0.4708\nPrecis\u00e3o          0.7897      0.1569\nRecall            0.7690      0.3333\nF1-score          0.7771      0.2134\n</pre> In\u00a0[\u00a0]: Copied! <pre> \n</pre>"},{"location":"projetos/mlp-classification/main/#comparando-com-o-meu-baseline","title":"Comparando com o meu Baseline:\u00b6","text":""}]}